{"pages":[],"posts":[{"title":"How to create a Hexo Blog","text":"Simple Hexo Tutorial입니다. Hexo는 Github pages를 사용해서 만드는 블로그입니다. 그러니, Github 계정을 가지고 계셔야 사용하실 수 있습니다. Github pages는 username.github.io라는 이쁜 고유 도메인(무려 https)도 주는 아주 고마운 블로그입니다. 지금 보고 계신 이 블로그도, Hexo로 만들어졌습니다. 도메인만 제가 사서 바꿨을 뿐이구요. 이 블로그를 설치하고 관리하고 사용하려면, HTML, Markdown, git에 대한 기본적인 이해는 있어야 합니다. 사실, 언급한 기술들을 잘은 몰라도, 대충 한 번씩만 해보셨으면, 하시다보면 익숙해져서 할만 하실 것도 같습니다. :) 1. Make a repository at GithubGithub에 가셔서 새로 Repository를 만드시는데, 그 repository의 이름은 꼭, username.github.io로 하셔야 합니다. 아래 보이는 사진처럼요. 2. Install Hexonode.js & gitHexo를 설치하고, 사용하려면 node.js와 git이 설치되어 있어야 합니다. 이 두가지 툴이 잘 설치가 되어있다면, 아래와 같은 명령어 한 줄로 간단하게 Hexo를 설치할 수 있습니다. $ npm install -g hexo-cli local directory본인이 사용 하고자 하는 폴더를 만드시고 들어가셔서, 명령어로 $ hexo init $ npm install 를 순차적으로 입력합니다. 그리고 나서 설치가 끝나면, Hexo blog를 시작할 수 있는 파일들이 설치 된 것을 확인 할 수 있습니다. 저는 temp_blog라는 폴더를 만들어서 설치해 보았습니다. 3. Start Hexo Blog믿기지는 않지만, 이렇게 위의 3개 명령어로 당신의 Hexo Blog가 생겼습니다. 생성된 Blog를 확인해 보도록 합시다. $ hexo server localhost:4000으로 접속을 하라고 하니, 웹브라우저 창을 통해 http://localhost:4000/ 을 입력하거나, 주소를 클릭해서 확인해 봅시다. Ta da!! 블로그가 생겼다는 것을 확인할 수 있습니다! :D 가장 기본 테마인 landscape로 설정되어있고, 글은 Hello World 밖에 없지만, 이제 여러분의 취향대로 바꿔나가기만 하면 됩니다! 자, 그럼 차근차근히 어떻게 블로그의 설정을 바꿀 수 있는지, 글은 어떻게 쓰면 되는지, 실제로 나에게 부여된 도메인에 배포를 할 수 있는지 알아봅시다. 4. Hexo _config.yml우리가 설치한 폴더를 보면, _config.yml파일을 확인 할 수 있습니다. 대략적인 모습은 아래와 같습니다. 간단하게, title, timezone, url, 그리고 deploy부분만 본인에게 맞는 정보로 고쳐보기로 합니다. 1234567891011121314151617181920212223242526272829303132# Hexo Configuration## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# Sitetitle: Stand firm Petersubtitle:description:author: John Doelanguage: entimezone: Asia/Seoul# URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'url: http://petercha90.github.ioroot: /permalink: :year/:month/:day/:title/permalink_defaults:...# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: landscape# Deployment## Docs: https://hexo.io/docs/deployment.html## 배포방식은 git으로, type은 본인의 repository 주소!deploy: type: git repo: https://github.com/PeterCha90/petercha90.github.io.git Deployment부분에서 deploy 방식(git)과, 우리 블로그와 실제 부여받은 도메인과의 연결을 할 수 있게 주소를 잘 적어줘야, 이후에 나오는 hexo deploy명령어를 통해 실제로 블로그를 배포할 수 있게 됩니다.hexo server로 켰던 로컬 서버를 Ctrl + c로 종료한 다음, hexo server로 서버를 구동시킨 뒤, localhost:4000으로 접속해서 확인해 보면, title이 잘 바뀌어 있습니다 :D 5. Create a new post그럼, posting은 어떻게 하면 될까요?! 이역시, 매우 쉽습니다. $ hexo new “sample_posting” 이 명령어를 치면, 자동으로 /source/_posts/ 밑에, sample_posting.md이라는 파일이 뙇.하고 생겼다는 메세지를 볼 수 있습니다. 이 파일을 열어보시면 뭐가 적힌게 별로 없습니다. 12345---title: sample_postingdate: 2018-01-13 22:56:29tags:--- 그래서 제가 몇 자 적어보았습니다. 123456789101112---title: sample_postingdate: 2018-01-13 22:56:29tags:---Markdown > 몇자 적어 보았습니다. ## 두 자 적어 봤습니다. **세 자 입니다.** 그럼 이 sample_posting.md파일을 저장하시고, 아래의 명령어를 수행해 봅니다. $ hexo generate 뭔가 많이 생성됐다는 메세지가 나왔습니다. 그리고 다시 hexo server를 사용해서 서버를 여시고, 로컬 4000번 포트로 접속해서 확인해 주세요 :D 짜잔~ 이렇게 우리의 첫 번째 Posting이 잘 등록 됐습니다! 🎉🎉 위의 제가 적은 내용과 결과를 보시면 알 수 있듯이, Hexo는 기본적으로 Markdown 언어를 지원합니다. 6. Deploy to remote sites자, 그럼 이번에는 마지막으로 이렇게 작성한 블로그와 포스팅을 우리의 실제 원격 저장소(Github repository)로 배포해보도록 합니다. 먼저, 아래와 같이 hexo-deployer-git을 먼저 설치해주시고, 1$ npm install --save hexo-deployer-git 아래의 명령어를 입력하시면, 배포가 끝났습니다!😆 1$ hexo deploy 2, 3분 뒤에 username.github.io로 접속하셔서 확인해 보세요 :) 7. Summaryhexo server Blog contents가 올바르게 보이는지 확인할 수 있는 로컬 서버를 구동합니다. hexo new \"title\" 새로운 “title”이라는 이름의 posting을 작성합니다. hexo generate 수정된 사항으로 deploy할 수 있도록 contents를 생성합니다. hexo deploy 원격 저장소(github repository)에 실제로 배포합니다. Tips : hexo generate = hexo g hexo deploy = hexo d hexo clean: 게시글에 html문법이 rendering되지 않고 깨져서 나올 때, 가끔씩 실행Contents 생성 후 바로 배포하기 hexo generate deploy = hexo g -d 8. Themes기본 테마도 훌륭하지만, 좀 더 다른 느낌의 테마를 원하신다면, 여기로 들어가셔서, 테마를 고르시고 본인의 _config.yml파일을 수정하면 됩니다!구체적인 테마 적용 방식은 보통 선택한 테마에서 설명을 해주기 때문에, 세부설정이 제각각 다릅니다. _config.yml 파일에서, theme이라는 key 값을 본인이 선택한 테마의 이름으로 바꾼다는 점은 공통이겠지만요 :D 지금 이 튜토리얼을 그대로 따라하셨다면 지금 여러분들의 상태는 아래와 같을 겁니다. theme: landscape document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2018/01/06/Start/"},{"title":"Logarithms","text":"Deep Learning 논문에서 자주 등장하는 Log. Machine learning 및 통계에서 자주 쓰는 이유! Log를 왜 쓰는 걸까 많은 딥러닝 논문들을 읽다보면, 하나같이 수식으로 설명하기 시작하는데, 그 때 꼭 나오는 친구가 이 Log다. 이미 아시는 분들에겐 시시한 이야기였겠지만, 대충 어렴풋이 필자와 같이 ‘숫자 크기 작게 해주려고 하는 거겠지..’정도로만 생각하고 있었다면(맞긴 맞다만…), 정확한 의미를 알아보자. 아래는 상용로그인 $\\text{log}x$와, $\\text{ln}x$로 많이 쓰는 자연상수 $e$를 밑으로 하는 $\\text{log}_ex$의 그래프다. 이 그래프를 보면서 할 수 있는 생각은, ‘$x$값이 엄청나게 커진다고 해도, $y$ 증가율은 격하게 변하지 않구나.’ $\\to$ ‘$x$수치가 낮을 때는 민감하게 반응하고, 높을 때는 둔감하구나.’ $\\to$ ‘그럼 수치가 대부분 낮을 때는 비교하기 좋을 거 같고, 이상치와 같은 비정상적인 예외 케이스들도 같이 고려할 수 있겠다.’ 정도가 될 것이다. 그럼 이걸 활용하면 아래와 같은 상황에 유용하게 쓸 수 있다. $log$는 어디에 사용되나요 대표적으로 두 가지의 경우에 많이 사용되는 것 같다. 데이터들 간의 수치적인 간극이 너무 커, 주어진 수치를 그대로 사용하면 회기분석시에 결과가 왜곡될 수도 있어서. 비선형적인 데이터의 분포를 선형적으로 쉽게 보기 위해서 1. 아래의 사진은 동물들의 체중별 뇌의 크기를 나타낸 scatter plot이다. 파충류같은 작은 동물들 부터, 코끼리, 고래, 공룡까지 다 들어 있다고 생각해보면 그 비교대상들이 서로 가지는 수치적인 차이는 실제로 왼쪽 그림과 같다. 오른쪽은 같은 데이터에 상용로그를 취한 것이고 실제로 그 경향성을 더 파악하기 쉬운 형태가 되었다. 아래 사진도 그런 방식으로 $\\text{log}$를 취해서 파충류부터 고래와 코끼리, 공룡까지 비교한 plot인데, x축과 y축의 수치를 자세히 보면 $\\text{log}$를 취한 그래프임을 알 수 있다. 2. 아래는 $y=2^x$ 그래프와, 이 그래프에 자연로그를 취한 $\\text{ln}2^x$의 그래프다. 비선형 그래프인 $y=2^x$를 선형으로 만들어 주고 있다. 이처럼, 로그는 복잡한 비선형인 수식들을 간소화 시켜주는 역할을 한다. 곱셈과 나눗셈이 log연산이 되면서 +, -로 바뀌기 때문에 아무래도, Model에서 computation을 많이 해야하는 부담도 좀더 줄어 들지 않을까?! 이런 이점 때문에도 많이 쓰는 것 같다. 이상, $\\text{log}$를 왜 쓰는지 한 번 알아 보았다. Reference blog1, blog2 Desmos(그래프 그려주는 사이트) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2018/09/12/Logarithms/"},{"title":"Pandas Cheat Sheet","text":"Pandas의 Dataframe 관련 기능정리. Cheat Sheet for Pandas Dataframe. 0. Start Import pandas1import pandas as pd 1. Creating a dataframe Dictionary Style:12345df = pd.DataFrame( {\"a\" : [4, 5, 6], \"b\" : [7, 8, 9], \"c\" : [10, 11, 12]}, index = [1, 2, 3]) Array Style:123456df = pd.DataFrame( [[4, 7, 10], [5, 8, 11], [6, 9, 12]], index=[1, 2, 3], columns=['a', 'b', 'c']) 2. Select an row or column There are 5 ways to access the value that is at index 0, in column ‘a’.loc selects the value by label, not index.iloc accesses to the value using row and column index. 12df.loc[0]['a']df.iloc[0][0] at selects the value by label, not index.iat accesses to the value using row and column index. 12df.at[0, 'a']df.iat[0,0] ix can use both label and index. 1df.ix[0, 'a'] The difference between loc and at is the return type. loc can return more than one row, it means loc can return a scalar, a Serise or a Dataframe. On the other hand, at only can access to a cell of certain position, it means it returns a scalar only. Actually it returns a scalar faster than loc, so if you have to deal with great amount of data, it would be more suitable. 123456789101112df.at[2, 'b'] # A scalar.df.iat[2, 2]df.loc[2]['b']df.iloc[2][2]df.ix[2, 'b'] df.loc[2][:'b'] # A Series.df.iloc[2][:2]df.ix[2, :'b'] df.iloc[:2][:2] # A dataframe.df.ix[:2, :'b'] If you want to retrieve the value(s) from a series, 12sr = df.bsr.values() Select multiple rows or columns 1234567# rowsdf.iloc[[0, 2]] df.loc[[1, 3]] # row names # columnsdf.iloc[:, [0, 2]]df[[\"a\", \"c\"]] # column names 3. Selecting rows with conditions Condition in the brackets.1234df[df.a > 4]df[(df.a > 4) & (df.b < 9)]# You have to wrap all conditions with parentheses.df[((df.a > 4) | (df.b < 9)) & (df.c > 10)] Condition using query.123df.query('a > 4')df.query('a > 4 and b < 9')df.query('(a > 4 or b < 9) and c > 10') 4. Adding rows or columns Adding new rows12df.loc[3] = 0 # fill with 0df.loc[3] = [5, 7, 1] # Add a row Creating a new column12345df[\"d\"] = 0 # fill with 0 df[\"d\"] = df.a # copy column 'a'df[\"d\"] = pd.Series([13, 14, 15], index = df.index) # fill with new valuesdf.loc[ : , \"d\"] = pd.Series([13, 14, 15], index = df.index) 5. Delete Indices, Rows or Columns Delete rows1234567df.drop([0, 2], axis=0) # Delete the rows with labels 0, 2df.set_index(\"a\", inplace=True) # Delete all rows with label 4df.drop(4, axis=0, inplace=True)df.reset_index(inplace=True)df = df.iloc[2:, ] # Delete the first two rows using iloc selector Delete columns.12df.drop(\"a\", axis=1, inplace=True) # Delete a columndf.drop([\"a\", \"c\"], axis=1, inplace=True) # Delete multiple columns 6. Combine Dataframes pd.concat()1234567891011121314151617df2 = pd.DataFrame( # Make two more dataframes {\"a\" : [1, 4, 7], \"b\" : [2, 5, 8], \"c\" : [3, 6, 9]}, index = [1, 2, 3])df3 = pd.DataFrame( {\"d\" : [1, 7], \"e\" : [2, 8]}, index = [1, 3])# concat rows df4 = pd.concat([df, df2]) # indices can be duplicateddf4.reset_index(drop=True, inplace=True) # Reset index# concat columnsdf5 = pd.concat([df, df3], axis=1) pd.merge()12345678910df6 = pd.DataFrame( # Make another dataframes {\"a\" : [4, 5, 7], \"f\" : [20, 50, 80], \"g\" : [30, 60, 90]}, index = [1, 2, 3])df7 = pd.merge(df, df6) # Inner join df8 = pd.merge(df, df6, how='outer') # Outer joindf9 = pd.merge(df, df6, how='left') # left joindf10 = pd.merge(df, df6, how='right') # right join 7. Iterate over a dataframe iterrows()12for index, row in df.iterrows() : print(row['a'], row['b'], row['c']) 8. Reading and Writing a dataframe Reading a csv file:1234pd.read_csv('name.csv')# if the file has a date column,pd.read_csv(\"name.csv\", parse_dates=[\"column_name\"])# Then, pandas transform it as a numpy.datetime64 column. Writing as a csv file:123df.to_csv(\"name.csv\", index=False)# optionsdf.to_csv(\"name.csv\", sep='\\t', encoding='utf-8') Writing as an Excel file:123writer = pd.ExcelWriter('name.xlsx')df.to_excel(writer, 'DataFrame')writer.save() 9. Other Useful functions Following commands are used often. 123456789101112df.head(n) # Select first n rows df.tail(n) # Select last n rowsdf.nlargest(n, 'a') # Select and order top n entries.df.nsmallest(n, 'c') # Select and order bottom n entriesdf.sample(frac=0.5) # Randomly select fraction of rows.df.sample(n=2) # Randomly select n rowsdf.isna() # Check whether each element has NA or not df.dropna() # Drop rows with any column having NA/null datadf.fillna(value) # Replace all NA/null data with valuedf.rename(columns={\"a\": \"k\"}, inplace=True) # Renaming \"a\" column as \"k\"df.describe() # To see basic statisticsdf.reset_index(drop=True, inplace=True) # Reset index References DataCamp pandas.pydata.org Shane Lynn Data Science School document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2019/02/16/pandas-101/"},{"title":"Running a pretrained model on Android with TPU (3)","text":"Google TPU 사용해서 Transfer Learning하여 모바일에 심기 - (3) Annotation Labelling하기, tfrecord 파일 만들기 본 Tutorial은 2020년 6월 기준으로 작성되었으며, Mac OS 기준으로 작성되어 Ubuntu 및 다른 Linux기반 OS에서는 검증되지 않았습니다. 잘못 기재되어 있거나 수정이 필요한 부분들은 알려주시면 감사하겠습니다 :) 이번 포스팅에는, 1) 우리가 학습시키고 싶은 Annotation 데이터 생성하기2) 데이터를 tfrecord로 만들기 두 가지 과정을 함께 진행해보겠습니다. 1. Make your own annotation data 이번 Tutorial에서 하고자 하는 task는 Object Detection이다. 따라서, 모델을 학습하기 위해서는 training 이미지 속 객체들에게 해당하는 label을 모두 부여해야 한다. 어노테이션을 만들 수 있는 프로그램은 참 많은데, 필자는 LabelImg 를 추천한다. OS별로 설치도 쉽고, 사용하기도 편하다. 이 프로그램이 가지는 가장 큰 장점은 학습 이미지의 basedata로 가져가는 VOC dataset이나, COCO dataset과 같은 format으로 생성되는 label정보를 저장해준다는 것이다! 🙏 이 장점은 기존 dataset에 숟가락만 얹어, 우리가 감지하기 원하는 이미지만 추가하면 쉽게 학습을 시작할 수 있게 해준다. 이번 튜토리얼에서는 LabelImg와 결이 맞도록, VOC 2012 object detection dataset을 다운받고, 추가로 학습하고 싶은 다른 Class 하나를 추가한 21가지 이미지를 Detection할 수 있는 모델을 만들어 보도록 한다. 순서는 아래와 같다. LabelImg를 사용해서 추가로 학습시키고 싶은 클래스에 해당하는 모든 이미지의 Annotation을 생성한다. (a.k.a 노가다) 생성된 .xml Annotation 파일들을 다운받은 VOC 2012 데이터의 Annotations에 추가한다. 이미지들은 JPEGImages 폴더에 넣어준다. train, trainval, val 용도에 따른 이미지 목록을 잘 정리해서 ImageSets > Main 안에 해당 .txt(ex: train.txt, trainval.txt, val.txt) 파일마다 잘 추가해준다. 2. Make data into tfrecord 이전 포스팅에서 설치한 TensorFlow Object Detection API는 학습할 데이터가 TFRecord 포멧이어야만 한다. 지금부터 살펴볼 과정에서 모든 데이터를 하나의 .tfrecord파일로 생성하게 되는데, 그 과정이 조금 고통스러울 수 있지만 한 번 만들기만 한다면 사용하기에 매우 편리하다. 필자의 추천대로 LabelImg를 사용해서 annotatio을 생성했다면, 바로 models/research/object_detection/dataset_tools/에 위치하고 있는 create_pascal_tf_record.py를 경우에 따라 조금 수정해서 사용하면 된다. 수정이 필요한 부분은 아래와 같다. create_pascal_tf_record.py 1234567891011121314...# at line 91# 딱히 수정을 안해도 되긴 하지만, tfrecord는 PNG도 지원하기 때문에 # 필요에 따라 수정해서 사용하면 될듯 if image.format != 'JPEG': raise ValueError('Image format not JPEG')...# at line 163for year in years:logging.info('Reading from PASCAL %s dataset.', year)# aeroplane만 만들게 되어 있다. examples_path = os.path.join(data_dir, year, 'ImageSets', 'Main', 'aeroplane_' + FLAGS.set + '.txt')... In other case.. 만약 VOC format이 아니라면, 각 상황에 맞는 TFRecord 변환 스크립트를 작성해야 한다. 각자가 가지고 있는 파일(json, yaml, 등등)에 따라 코드를 읽어와서 생성을 해 주어야 한다. 하나의 예시로 Berkeley Deep Drive는 json 파일로 작성된 Object Detection Box label 데이터이다. 이를 tfrecord 파일로 만드는 예시는 여기에서 다운받아 참고할 수 있다. Finish ! tfrecord 생성에 성공했다는 증거는, 전체 이미지 용량보다 약간 더 용량이 많다는 것! tfrecord의 용량이 대상 전체 이미지를 합한 용량보다 조금 더 많다는 것을 확인할 수 있다면, 성공이다. 수고하셨습니다. :) 다음 시간에는, Google Cloud Platform에서 이번 편에서 생성한 tfrecord를 가지고 본격적인 Transfer Learning을 실행해보도록 하겠습니다! 이 글은 DataCrew 에서도 보실 수 있습니다. References Training and serving a realtime mobile object detector in 30 minutes with Cloud TPUs - Link GCP Project Setting - Link Step by Step TensorFlow Object Detection API Tutorial - Part 1 ~ 5 - Link document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/06/09/tensorflow_object_detection_with_tpu_3/"},{"title":"tf.identity()","text":"Tensorflow에서 자주 등장하는 tf.identity()에 대해 알아봅니다. tf.control_dependencies()와 함께 모델을 구현하다보면 거의 제일 마지막인 fully connected인 Linear에 이르러 맨 마지막 logits을 뽑아내기 전에 꼭 한 번쯤은 보게 되는 친구가 이 tf.identity(). 습관적으로 그냥 쓰는건가보다 하다가 문득 더 깊이 이해하고 넘어가야하겠다 싶어서 이렇게 포스팅으로 남긴다. tf.identity()를 이해하기 위해서는 tf.control_dependencies()를 먼저 이해해야 한다. Tensorflow의 공식문서에 따르면 그 설명이 아래와 같다. tf.control_dependencies() tf.control_dependencies(control_inputs) control_inputs: A list of Operation or Tensor objects which must be executed or computed before running the operations defined in the context. Can also be None to clear the control dependencies. If eager execution is enabled, any callable object in the control_inputs list will be called. Context 안에서 정의된 Operation이 Running되기 “전“에 “먼저“ 실행시켜줄 친구들을 control inputs으로 넣어주면 먼저 실행해준다는 의미다. 그럼 이제 tf.identity()를 살펴보자. tf.identity() tf.identity(input, name=None) Return a tensor with the same shape and contents as input. input: A Tensor. name: A name for the operation (optional). 그냥 입력된 Tensor랑 똑같은 Shape, 똑같은 contents를 돌려준다. 그럼 왜 쓰는 거지?? 1234567891011x = tf.Variable(0.0)x_plus_1 = tf.assign_add(x, 1)with tf.control_dependencies([x_plus_1]): y = tf.identity(x)init = tf.initialize_all_variables()with tf.Session() as session: init.run() for i in range(5): print(y.eval()) 여기에서 가져온 예시를 보면, 이유를 좀 알 수 있는데, 만약에 y = tf.identity(x) 부분이 그냥 y = x라면 아무 일도 일어나지 않음을 알 수 있다. print의 결과가 죄다. 0 0 0 0 0 이다. 왜냐하면, tf.control_dependencies()는 “Operation”이 실행되기 전에 inputs으로 들어온 부분을 처리해준다고 했는데, y = x는 아무런 operation이 없기 때문에 x_plus_1를 실행하지 않는다. 결론 Tensorflow로 모델을 만들 때, 주로 맨 마지막에 Fully connected Layer에서 Output에 해당하는 Logits을 뽑을 때 이 tf.identity()를 많이 쓰는 이유는 tf.control_dependencies()를 실행시켜주기 위한 건덕지(?)를 만들기 위해서다. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2018/09/07/tf-identity/"},{"title":"Colab tutorial :)","text":"Cloud Jupyter Notebook powered by Google Colab Introduction 2020.01.14 - Update Colab이란? 😶 Jupyter Notebook를 구글 드라이브에서 사용할 수 있는 것인데, 여태 있었던 구글 문서나 구글 스프레드시트처럼, 실시간 협업이 가능한 버전이라고 생각하면 된다고 합니다. 원래 구글 내부에서 직원들이 사용하던 것이라고 하네요! 그래서 error가 생기면 STACK OVERFLOW 버튼이 ..:+1: 그렇기 때문에, Jupyter Notebook나 다른 IDE에서 흔히 제공하는 Variable다음 .을 찍고, Tab키를 치면 하위요소(함수 or 변수)의 일부만 쳐도 나타나게 하는 친숙한 기능들도, 조금 반응이 느리긴 하지만, 0.5초 정도 뒤에 나타납니다. :D GPU, TPU 사용 설정을 하시려거든, 맨처음 Notebook을 생성하실 때, 선택하실 수도 있고, 수정 -> 노트 설정을 누르시면 GPU, TPU 사용 설정을 할 수 있습니다. 😄 시작하기 구글 아이디를 가지고 계신다면, 이 링크를 클릭하시거나 간단하게 Colab이라고 검색을 하시면 쉽게 Colab을 시작하실 수 있습니다. 그러면, 아래와 같은 첫 화면을 보실 수 있습니다. 그럼 새 PYTHON 3 노트 버튼을 누르시고, Jupyter notebook을 사용하듯 사용하시면 됩니다. :) Linux 명령어 사용하기 😲 Colab에서는 느낌표로 쓴 뒤에 명령어를 치면 terminal에서 작동하는 것으로 처리해줍니다. 로컬에서 Bash창을 켜서 설치하고 다시 Jupyter로 돌아와야했던 귀찮은 과정이 매우 심플해졌습니다! 하지만, 1$ cat sample.txt 같은 native Linux에서나 가능한 몇몇 문법들은 작동하지 않습니다. 그래서, 100% Linux native 명령어를 쓸 수 있는 것은 아니며, 어디까지나 그때 그때, 필요한 library, package 설치 편의를 위한 수준이라고 볼 수 있을 것 같습니다. 리눅스 명령어에서 익숙한 분들은 당연히 경로 이동을 할때 cd를 쓸 텐데, !cd는 먹히지 않고, os를 import하신 뒤에, os.chdir()로 이동해야 합니다. 빈 폴더를 하나 만들고 그 안에 짧은 txt파일을 만들어 확인해 봅시다. Cell. 1 123456import os!mkdir testos.chdir('test')!pwd# 명령어는 못찾는다고 나오지만 생성은 합니다 :)!'Sample text!' > sample.txt Output: 123/content/test /bin/bash: Sample text!: command not found sample.txt Cell. 21234# 목록에는 파일이 확인되지만, !ls# cat은 작동하지 않습니다. !cat sample.txt Output: 1sample.txt Colab.에서 쓸 수 있는 GPU 😉 먼저 수정 - 노트 설정 에 들어가셔서, 하드웨어 가속기 를 None에서 GPU로 설정합니다. 일단 필요한 Util을 다운을 받고, 1234!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi!pip install gputil!pip install psutil!pip install humanize Import 해줍시다. 1234import psutilimport humanizeimport osimport GPUtil as GPU GPU는 하나만 허락해 줍니다만… 갓글님의 그 하나는 에이스입니다. (2020.01.13 update) 현재. 무려 Tesla P100-PCIE-16GB. 😭 개인 실험용으로는 충분한 것 같습니다. 123456GPUs = GPU.getGPUs()gpu = GPUs[0]print(gpu.__dict__[\"name\"])print(\"The number of GPUs: {}\".format(len(GPUs)))print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil* 100, gpu.memoryTotal)) Output: 123Tesla P100-PCIE-16GB The number of GPUs: 1GPU RAM Free: 16280MB | Used: 0MB | Util 0% | Total 16280MB 참고사항 안타깝지만, Colab은 Docker기반의 Container로 실행되기 때문에 한 번에 12시간까지만 사용 할 수 있습니다. 그래서, 작업 하시던 Colab을 껏다가 잠시 후 다시 시작하면 새로운 docker container를 받기 때문에 새로운 인스턴스 위에서 시작할 때마다 설치가 필요한 패키지들을 매번 다시 설치해야 합니다. 그 다음 부터는 Jupyter notebook을 사용하듯 사용하면 됩니다. Local file Upload! 😀 Local 파일을 업로드하는 것은 아래와 같이 좌측의 화살표 버튼을 눌러 탭을 여신 뒤에, 파일 탭을 누르시면 됩니다. :) 그 다음 업로드 버튼을 누르시고 올리실 파일을 선택하시면 업로드가 잘 됐다는 것을 파일 목록에서 확인하실 수 있습니다. 이미지를 업로드 하시면 알림창 하나를 보실 수 있으실텐데, 로컬에서 가져온 업로드한 데이터들은 instance가 바뀌면(= 새로 시작하면) 없어진다는 말이니, 참고하세요. 아래의 코드를 실행해보면, 업로드한 그림을 확인해 볼 수 있습니다. 12345678import ioimport matplotlib.pyplot as pltfrom PIL import Imageos.chdir('..')image = Image.open('./person-walking.png')plt.imshow(image) 사실 이렇게 이미지를 올리면, upload 하고자 하는 데이터가 많을 수록 업로드 속도도 느릴 뿐만 아니라 브라우저를 다시 시작할 때마다, 또 업로드를 해야합니다. 잠깐 확인하는 용도로는 상관이 없겠지만요 :) 일괄적으로 Colab을 이용하되, 같은 데이터를 매번 사용해야하는 경우에는 개인 Google Drive에 업로드 한 다음, 그 directory 안에서 작업을 하면 더 편합니다. Google Drive를 Mount 하는 방법을 알아보도록 하겠습니다. :) Mount your Google Drive Google Drive를 마운트하는 것도, 이전에 비해서 더 직관적으로 편하게 되었습니다. 아래와 같이, 드라이브 마운트 버튼을 눌리면 갑자기 코드가 하나 생기고, 그 셀을 실행하게되면 나오는 URL을 클릭하시고 연결하고 싶은 Google Drive를 가지고 있는 구글 아이디로 인증을 거칩니다. 인증을 마치면 아래와 같은 키를 하나 생성해줍니다. 그 키를 복사에서 Colab으로 돌아가, Enter your authorization code: 부분에 입력해줍니다. 그러면 아래와 같이 본인의 Google Drive가 Mount된 것을 확인하실 수 있습니다. Google Cloud Storage 개인 실험은 위와 같이 로컬데이터 업로드나 구글 드라이브를 사용하겠지만, 단체의 프로젝트라고 하면, Google Storage와 같은 데이터 저장소를 공유해야합니다. 이번에는 그 방법입니다. Google Cloud Storage(이하 GCS)와 함께 Colaboratory를 사용하려면 Google Cloud 프로젝트를 만들거나 기존 프로젝트를 사용해야 합니다. 저는 아래와 같이 colab-test이라는 이름의 프로젝트를 만들었고 프로젝트 ID는 colab-test-265006이네요. 만들기 버튼을 눌러서 프로젝트를 만들어줍니다. 아래의 코드를 실행해서 인증을 통해, GCS에 접근합니다. 12from google.colab import authauth.authenticate_user() 그러면 Google Drive를 Mount할 때와 똑같은 인증 과정을 거치게 됩니다. 그 뒤로는 Command-line Utility인 gsutil 혹은 Python API 를 통해 GCS에 접근할 수 있습니다. 먼저 GCS에 파일을 업로드 하는 방법부터 알아보겠습니다. 데이터를 넣어놔야, Colab에서 접근해서 그 데이터를 사용할 수 있을테니까요. Upload data into the GCS Bucket gsutil이나 PythonAPI를 사용해서 GCS에 Bucket을 만들고 데이터를 업로드 할 수 있습니다. 하지만, 이 포스팅에서는 브라우저를 사용해서 간단하게 Bucket을 생성하고, Drag & Drop으로 데이터를 업로드 하도록 하겠습니다. Creat a bucket 생성한 프로젝트를 선택하시고, 좌상단에 위치한 탐색 메뉴를 클릭하셔서 아래와 같이 많은 제품들 중 저장소 카테고리 아래에 있는 Storage를 선택합니다. 그 다음은 버켓을 만들어 봅니다. 버킷 생성 , 혹은 버킷 만들기를 눌러 버킷을 생성해봅니다. 버킷의 이름, 데이터 저장 위치, 기본 스토리지, 액세스 제어 방법을 선택하는 옵션이 나오는데 필자의 경우, 이름은 colab-sample, 데이터 저장 위치는 Multi-region유형으로 asia, 클래스는 Standard, 객체 접근 방식은 세분화된 엑세스 제어로 하였고, 고급설정은 암호화 Google 관리 키인 상태에서 나머지는 아무것도 건드리지 않았습니다. 그렇게 아래와 같이 Bucket이 생성되었습니다. Upload data 그럼 데이터를 Drag & Drop이나 버튼을 눌러서 Upload 합니다. Upload가 잘 된 것을 확인한 후, 창을 닫아도 좋습니다. Download using gsutil 먼저 gcloud를 사용해서 생성한 프로젝트를 설정해줍니다. 1!gcloud config set project colab-test-265006 Output: 1Updated property [core/project]. 그 다음, 해당 버킷에 접근해서 업로드했던 파일을 다운로드 해봅니다.!gsutil cp gs://다음에는 사용하는 bucket의 이름부터 다운로드하고자 하는(정확하게는 cp 복사) 파일에 대한 절대경로를 명시하시고, 그 다음 다운로드 받을 위치를 명시해주면 됩니다. 저는 똑같은 파일을 위의 Local Upload에서 사용해서, 이름을 person-walking2.png로 하였습니다. 1!gsutil cp gs://colab-sample/person-walking.png ./person-walking2.png Output: 123Copying gs://colab-sample/person-walking.png.../ [1 files][ 2.1 KiB/ 2.1 KiB] Operation completed over 1 objects/2.1 KiB. 다운로드가 잘 됐는지 이미지를 열어서 확인해봅니다. 12image = Image.open('./person-walking2.png')plt.imshow(image) Download using Python API 먼저 서비스 클라이언트를 만듭니다. 12 from googleapiclient.discovery import buildgcs_service = build('storage', 'v1') 파일을 다운로드 합니다. gcs_service.objects().get_media()함수에 매개변수로, bucket 부분에 접근하고자 하는 Bucket의 이름은, object에는 해당 파일의 절대경로를 적어 주시면 됩니다. 이번에는 같은 파일을 person-walking3.png로 받도록 하겠습니다. 12345678910111213from apiclient.http import MediaIoBaseDownloadwith open('./person-walking3.png', 'wb') as f: request = gcs_service.objects().get_media(bucket='colab-sample', object='person-walking.png') media = MediaIoBaseDownload(f, request) done = False while not done: # _ is a placeholder for a progress object that we ignore. # (Our file is small, so we skip reporting progress.) _, done = media.next_chunk()print('Download complete') Output: 1Download complete 다운이 잘 됐는지 확인해봅니다. 12image = Image.open('./person-walking3.png')plt.imshow(image) References Jaeyeon Baek님의 블로그, 최건호님의 Google Colaboratory 사용법 Colab:로컬 파일, 드라이브, 스프레드시트, Cloud Storage document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2019/06/12/Colab_tutorial/"},{"title":"Basic Deep learning 02","text":"Deep Learning 개념 및 용어들을 알아봅니다. Batch, Epoch, CNN Peter Cha Deep Learning을 이해하고, 직접 Deep Learning을 구현하고자 했을 때 필요한 기본 개념들을 정리해 보았습니다. 이번 포스팅에서는 Epoch, Batch라는 단어들의 의미와, 기본적인 CNN - Convolutional Neural Network에 대한 키워드들을 다룹니다. 유명한 MNIST 데이터를 학습하는 모델을 만들고 싶다고 했을 때, 언급한 키워드들이 어떤 의미로 사용되는지 예시로 함께 보려합니다. In this post, you will learn the Concepts needed when you need to understand the process of training AI or implement the AI by yourself. We are going to talk about Epoch, Batch, and basic CNN knowledges.Supposed we want to make a model which classifies MNIST data, let’s check how the keywords above can be used. MNIST MNIST data는 아래에서 보시는 것처럼 0 ~ 9까지의 숫자가 적혀 있는, 손글씨 data입니다. 따라서 총 10가지의 class가 있습니다. 이 데이터를 이용해서 우리가 학습시키고 싶은 모델은 따라서 새로운 손글씨 data를 보더라도 0 ~ 9중에 어떤 숫자인지 잘 맞추는 AI가 될 것입니다. Images from tensorflow.gitbooks.io. As you can see above, MNIST is a dataset of handwritten digits, 0 to 9. Therefore, MNIST dataset has 10 classes to distinguish. Using this data, Our model to be trained will be able to distinguish 0 ~ 9 handwritten digits. Epoch, Batch MNIST는 Training data로 총 6만 장의 수기로 된 숫자를 제공하고, Test용으로 1만 장을 제공하는 Dataset입니다. 자, 그럼 우리는 6만장을 한꺼번에 모델에게 주고 학습해!라고 하면 될까요? 할 수는 있더라도 꽤 여유로운 메모리를 가진 local machine이 아니고서는 좀 힘들겠죠? MNIST가 아닌 더 큰 용량의 데이터일 수록 더 그럴 것입니다. 그래서, 우리는 이 데이터들을 특정한 양으로 나눠서 조금씩 학습을 할 수 있게 넣어주는데요, 그 작은 단위를 Batch라고 부르고, 그 Batch의 크기가 어떠한지를 일컫는 말로, Batch size라고 말합니다. 우리가 Batch size를 100으로 정했다고 하면, 총 몇 번의 반복을 해야 총 60000장의 Training data를 다 한 번씩 모델이 학습할 수 있게 될까요? 600번 일 것입니다. 그럼 실제로, 우리 모델은 100장의 데이터를 가져와서 한 번 돌고(학습하고), 앞에서 배운 Back propagation을 통해, Weight를 Update하게 되면, 그 다음 100장을 가져와서 또 학습을 똑같이 반복하는 이 행위를 총 600번을 하게 됩니다. 그렇게 600번을 다 돌았을 때, 우리는 '1 epoch을 돌았다'라고 말합니다. 참고로, 이 600번을 Step size라고 일컫습니다. CNN CNN은 Convolution Neural Network의 약어로, Convolution 계산이 어떻게 Neural Network 2D 이미지 계산과 관련이 있는지는 여기를 참고해주세요. 이 글에서는, CNN에서 자주 언급되는, Filter, Kernel, Stride, Pooling, 그리고 Padding에 대해서 알아봅니다. Feature(= Channel or Activation map) 잠시 MNIST대신에 고양이가 어떻게 생겼는지를 학습하는 Model을 만들고 있다고 생각해 봅시다. 그러면 RGB color로 된 사진을 넣어주게 되고, 우리는 모델에게 이 고양이에 대한 특징(Feature)을 추출해서 학습을 하라고 할 것입니다. Images from ireneli.eu. 자, 그러면 우리 모델이 맨 처음 보게될 이미지는 Width, Height, 그리고 Red, Green, Blue 3가지로 이루어진 이미지를 받게 되는 것이죠. 이 때, 우리는 Channel이라고 부르는 부분으로 이 RGB인 Depth를 지칭합니다. 그러면 원본 이미지는 'channel의 size가 3이다'라고 말 할 수 있게 됩니다. 우리 MNIST 이미지는 가로 28 pixel, 세로 28 pixel 짜리, 흑백 이미지입니다!(이미지에는 32라고 적혀있지만..) 그래서 MNIST 이미지의 channel의 Size는 1입니다. Images from parse.ele.tue.nl. 이번에는 왜 이 Channel의 또다른 이름이 Feature, Feature maps인지 알아봅시다. 우리 MNIST 데이터가 위의 그림과 같이 들어간다고 했을 때, $C_1$을 보시면 5x5 크기의 Conv를 통과한 뒤, 이미지가 4겹(?)이 됐습니다. 이 때 우리는 Feature map의 size가 4가 됐다고 말할 수 있습니다. 그리고 $C_2$를 보시면 feature map이 12개가 됐죠. 이런 행위를 해석을 하자면, 들어온 이미지에 대해서 특징을 추출했는데, $C_1$에서는 특징을 4개를 추출하고, $C_2$에서는 특징을 12개를 추출한 뒤 결과물이라고 생각하시면 됩니다. '특징(Feature)을 추출한다'라는 말이 무슨 말인지 이해를 돕기 위해서, 아래 사진을 준비했습니다. Images from deliveryimages. 위 사진은, 사람 얼굴을 학습하는 CNN 모델의 Layer별로 추출한 특징들을 시각화 한 것입니다. 맨처음엔 Pixel로 구성돼있는 원본 이미지에서 맨처음에는 취운 특징인 가로, 세로, 대각선, 원, 곡선 등등의 비교적 단순한 edge들만 특징으로 추출하고 있습니다. 하지만 모델의 구조가 더 깊이 들어갈 수록, 그 단순한 Feature들을 조합해서 조금더 복잡한 눈, 코, 입 등을 그릴 수 있게 되고, 그 자체를 Feature로 삼을 수 있게 됩니다. 그러다 보면 사람의 전체전인 얼굴이라는 object를 Detect할 수 있는 모델이 되는 것이죠. 그래서 제 개인적으로는 들어올 때는 Input Image의 Color라는 의미의 Channel로 부르는 것이 더 와닿다가, Layer를 통과할 수록 더욱더 정교한 특징들을 이미지에서 뽑기 때문에, Feature map이라는 말이 더 와닿으니 서로 혼용해서 같은 녀석을 부르는 것 같다는 느낌이 있습니다. Kernel(= Filter) & Stride 바로 위에서 설명한 Feature Map은 이제 설명할 Kernel, 혹은 Filter라는 녀석을 통과한 뒤 나온 결과물 입니다. 이미지가 들어왔을 때, kernel이라는 Window를 정하고, 그 Window를 움직이면서 그 Kernel이 가지고 있는 weight값들과 Input image와의 연산을 통해 새로운 값을 가진 Image를 생성하게 되는 것이죠. 아래 예시를 통해 더 자세히 알아 봅시다. Images from blog.bkbklim.com. 위 Animation에서 Input이미지는 5x5 Size이고, 3x3 Size의 Kernel, 혹은 Filter가 한 칸씩 움직이면서 Image와 자신이 가지고 있는 Feature map을 계산하여 결과값을 내놓고 있습니다. Kernel은 X자 모양의 Filter네요. element-wise 곱, 즉, dot product를 계산하여 자신의 필터에 부합하는 위치면 곱한 값이, 아니면 0이 곱해져서 의미없는 0이 나오게 됩니다. 그렇게 나온 결과들을 다 더한 값 하나만 결과로 내놓습니다. 여기서 등장하는 Stride! 자연스럽게 이 애니메이션에서는 한 칸씩 움직이고 있습니다만, 어디까지나, Stride가 1x1일 때의 움직임입니다. Stride는 어느 정도의 간격을 가지고 Kernel 계산을 진행할 것인지를 나타내는 척도입니다. Stride가 2x2였다면, 두칸씩 움직일테고, 이 Input Size와는 맞지 않기 때문에 에러를 일으킵니다. 아래는 Filter를 설명하는 또 다른 이미지 인데요, 우리가 언급한 저 Filter에 보이는 $w$가 심상치 않습니다. Backpropagation을 하면서 Update되는 Weight들은 다 저, Filter의 Weight들입니다! 더 명확한 특징들을 추출하기 위한 세련된 Filter가 되기 위해 그 weight들을 맞춰나가는 것이죠. 그럼 이렇게 만들어진 Filter들을 통과하는 Feature map들은 어떻게 형성되는지, 더 명확한 이해를 위해 아래 사진을 보실까요. Images from deliveryimages. 위와 같은 Filter가 하나 있습니다. 곡선을 찾는 Filter네요. 그럼 이 Filter가 들어온 쥐 이미지를 Stride에 맞게 돌아 다니게 됩니다. 그러면서, 이 곡선에 해당하는 위치가 있는지 찾습니다. 이 Filter가 한바퀴 다 돌고 완성된 Feature map은, 아래와 같이 해당 Filter에 반응하는 부분에 높은 숫자를, 아닌 부분에서는 0에 가까운 수를 가진 Feature map이 되는 것이죠. Images from deliveryimages.작아지는 Image Size 위에 나온 Animation에서 주목할 점은, 5x5 이었던 이미지 사이즈가, Kernel 계산을 마친 후, 3x3가 됐다는 것입니다! 이런 계산이 저 위에 사진에서 보셨듯이 32x32 size였던 MNIST 이미지가, $C_1$에서 28x28이되고, $C_2$에서 10x10 size로 줄게 되었는지 설명할 수 있게 됩니다. Padding 이미지가 계속 Convolution layer를 통과하면서 작아지면 나중에는 1x1까지 가다못해 없어지지 않을까요?! 그래서 이미지의 Size가 너무 줄어들지 않게 이미지 주변에 값을 넣어주는 기법을 padding이라고 합니다. 가장 많이 쓰이는 Zero padding으로 예시를 보이자면 아래와 같습니다. 저렇게 Kernel Size가 2x2이고, Stride가 1x1인 경우, 원본이 3x3 Size였으면 2x2 kernel로 계산을 해도 총 9번을 계산하여 원본 사이즈를 그대로 유지할 수 있게 됩니다. Pooling Pooling은 Filter 계산이 아닌, kernel size에 해당하는 영역에 있는 값들을 일괄적으로 처리하고 싶을 때 사용합니다. 대표적인 Pooling으로 많이 쓰이는 Max Pooling의 개념은 아래와 같습니다. Images from deliveryimages. 말 그대로 2x2 Max pooling을 하겠다 하면, 해당 사이즈에서 가장 큰 값만 취하는 것이죠. 가장 중요한 정보만 취하겠다는 의도가 있습니다. 그 외에도 Average Pooling을 비롯한 다양한 Pooling이 있습니다. 이 Pooling의 장점은, 무엇보다 계산이 단순해서 계산량, Computation Cost가 작다는 것 정도가 되겠습니다. 위에서 배운 Padding을 응용해보면, Padding을 적당히 준 이미지에 Pooling을 하면 이미지 사이즈는 줄지 않지만 빠른 계산은 가능한 구조도 가능하겠죠? 여기까지 기본적인 딥러닝 용어인, Batch, Epoch, 그리고 CNN의 기본 용어들을 살펴 보았습니다! 다음 포스팅에서는, Activation Function, MLP와 CNN의 차이점에 대해 이야기 해 보도록 하겠습니다. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2018/04/06/Basic-Deep-Learning-02/"},{"title":"Running a pretrained model on Android with TPU (1)","text":"Google TPU 사용해서 Transfer Learning하여 모바일에 심기 - (1) Google Cloud Platform 환경설정편 본 Tutorial은 2020년 6월 기준으로 작성되었으며, Mac OS 기준으로 작성되어 Ubuntu 및 다른 Linux기반 OS에서는 검증되지 않았습니다. 잘못 기재되어 있거나 수정이 필요한 부분들은 알려주시면 감사하겠습니다 😃 Result of this tutorial 😎 이 튜토리얼을 성공적으로 마치면 Transfer Leraning을 통해, 아래와 같은 Android Object Detection Application을 직접 사용해 볼 수 있습니다. 무료계정 생성하기 TPU는 아래에서도 살펴보겠지만, 무료로 한 달간 사용할 수 있는 방법이 있다. 하지만, TPU 이외의 Compute Engine과 저장소 역할을 하는 Google Storage, 그리고 TPU 및 여러 API를 사용하는 비용까지 추가로 청구가 될 수 있다. AWS와 같이 GCP도 무료로 일정량 이상을 서비스를 사용할 수 있도록 해주는데, GCP의 경우 AWS와 같이 1년 이라는 Free tier를 부여해주는 것이 아니라, 실제 비용 $300에 해당하는 만큼의 서비스를 사용할 수 있게 해준다. 한 계정(e-mail)당 한 번씩 쓸 수 있다고 하니,공부 목적의 학습용으로는 사용하다가 $300를 다 쓴다고 해도, 또 다른 계정을 생성하고 다시 시도하는 식으로 사용해도 좋겠다. Set the environment 🍀 1. Google Cloud Setup GCP 프로젝트 설정 의 안내에 따라, 다음과 같이 환경을 만들어주자. 1.1. 프로젝트 생성 Google Cloud Platform 콘솔에서 아래와 같이 프로젝트를 생성한 뒤, 선택해준다. 필자는 TPU를 사용해서 MobileNet-SSD 모델을 학습할 것이기 때문에, 프로젝트 이름을 object-detection이라고 지었다. 1.2. 결제 사용 설정 그 다음에는 결제 설정을 해줘야 한다. 물론 무료크레딧이 다 차감이 되어야 결제가 진행되지만, 구글 입장에서는 혹시나 사용자가 무료크레딧을 초과해서 발생시킬 수 있는 비용에 대해 청구를 해야하니 설정을 해 줘야 프로젝트가 진행된다. 아래와 같이, 좌상단에 메뉴 버튼을 누르면 나오는 선택지에서 결제 - 결제 수단을 차례로 눌러서 결제 사용 설정을 마친다. 1.3. API 사용 설정 다음은, API 설정을 해줘야 한다. 우리는 Local에서 구글 Compute Engine과 TPU에 명령을 내리게 될 것임으로 아래와 같은 API를 사용할 수 있도록 세팅해 주어야한다. Compute Engine API, Cloud TPU API, AI Platform Training & Prediction API 총 3가지 API 사용 설정을 한다. 좌상단 메뉴 버튼을 누르면 나오는 API 및 서비스 를 눌러서 나오는 화면에서 + API 및 서비스 사용 설정 를 눌러주자. API 검색창에 Compute Engine API를 차례대로 입력하고 사용 설정을 해준다. 1.4. Google Cloud SDK 설치 Google Cloud SDK를 설치를 하면, 우리는 로컬에서 GCP로 TPU학습 명령을 내리고, 학습과정을 log로 모니터링 할 수 있게 된다! 📺 여기에 들어가서 해당 OS에 해당하는 설명대로(Windows는 없다) 따라하면 Google Cloud SDK 를 설치할 수 있다. 설치시 소소한 팁 google-cloud-sdk/install.sh 실행 한 뒤에는 (설치 경로에 따라 필요한 경우 sudo로 해야할 수 있음) 터미널 껏다가 켜보면, google-cloud-sdk가 설치 완료 된 것을 확인할 수 있다. Google Cloud SDK 설정시, Compute Region 과 Zone은 나중에라도 gcloud config set compute/zone NAME, gcloud config set compute/region NAME명령어를 써서 바꿀 수 있어서 Default로 설정하고 넘어가도 된다. 하지만, 아래에서 언급되겠지만 TFRC에서 허락해주는 무료 TPU의 Region/Zone과 맞추기 위해서 필자는 us-central1-f로 세팅했다. terminal 창을 껏다가 다시 실행하면 gcloud config list를 입력해서 본인의 계정과 프로젝트가 잘 설정됐는지 확인해 볼 수 있다. 아래와 같은 결과를 확인할 수 있다면 끝. 12345$ gcloud config list[core]account = petercha0990@gmail.comdisable_usage_reporting = Falseproject = object-detection-265202 여기까지 설정이 잘 끝났다면, 이제 TPU와 관련된 설정만 마무리 하면 된다! 사실이제 시작임… 2. Allowing Cloud TPU Access 2.1. Get free TPUs “왜 TPU 안써?!” \"…??! 😦없는데 어떻게 써요!\"라고 대답할 필자와 같은 사람들을 위해서 갓글님은 TFRC 프로그램을 진행하고 계신다. Google is Love. 돈 없고 가난하지만 TPU를 써서 연구해보고 싶은 전 세계의 연구자들을 위해서 한 달간 TPU를 무료로 사용할 수 있게 해주는 프로젝트다. 여기 에 들어가서 지금 신청하기 버튼을 눌러서, 잽싸게 신청해 놓자. 신청 후에 TFRC로 부터 답변을 허가 메일이 당장 오진 않는다. 하지만 조금만 (하루정도) 기다리면 아래와같은 자비로운 답변 메일을 받을 수 있다 🙏 답변을 받았다면 메일 안에 아래와 같이 3. Send us your project number via this short form. 부분을 클릭해서(1번과 2번 과정인 프로젝트를 만들고 Cloud TPU API를 사용설정하는 것은 위에서 다 했기 때문에), TPU를 사용할 프로젝트의 정보와 더물어, 진정으로 TPU를 사용하겠다고 신청을 하면 된다. 신청이 완료되었으면, 필자의 경우 약 3시간 뒤 다시 답변 메일이 왔는데, 그 때부터 30일이라는 무료로 사용할 수 있는 기간이 카운트다운 되기 시작한다. 두 번째로 받은 메일에는 실제로 프로젝트가 TPU를 무료로 쓸 수 있는 Zone과 Region이 명시가 되어있으니, 이 점을 잘 참고해서 설정을 해야한다. 그렇지 않으면, 어마어마한 TPU사용료가 청구되기 때문에 조심! 2.2 Set a TPU account 아래와 같이 projects.getConfig 를 호출해서 tpuServiceAccount 를 알아낸다. 12$ curl -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\ https://ml.googleapis.com/v1/projects/:getConfig Output: 123456{ \"serviceAccount\": \"service-630609466618@cloud-ml.google.com.iam.gserviceaccount.com\", \"serviceAccountProject\": \"601122858872\", \"config\": { \"tpuServiceAccount\": \"service-blahblah@cloud-tpu.iam.gserviceaccount.com\"} 그리고 아래 같이 설정해준다. Google Cloud Platform Console에 로그인하고 TPU를 사용 중인 프로젝트를 선택합니다. IAM 및 관리자 ▶️ IAM을 선택합니다. 추가 버튼을 클릭하여 프로젝트에 멤버를 추가합니다. 멤버 입력란에 TPU 서비스 계정(service-blahblah@cloud-tpu.iam.gserviceaccount.com) 을 입력합니다. 역할 드롭다운 목록을 클릭합니다. Cloud TPU API 서비스 에이전트 타이핑 검색해서 설정 3. Cloud Storage Bucket 💾 모델 학습에 사용할 데이터 및 학습 결과를 저장할 Cloud Storage 버킷이 필요하다. 만든 버킷은 가상 머신(VM)과 Cloud TPU 기기 또는 Cloud TPU 슬라이스(여러 TPU 기기)와 동일한 리전에 있어야 한다. 아래 명령어를 치면 간단하게 Bucket을 생성할 수 있다. 그 누구도 사용하지 않은 유니크한 이름이어야 한다. 1$ gsutil mb gs://YOUR_UNIQUE_BUCKET_NAME 필자의 경우, object_detection_peter이라는 이름으로 버킷을 만들었다. 아래와 같이, GCP Console 메뉴에서, 저장소 카테고리의 Storage 를 확인해보면 생성된 Bucket을 확인해 볼 수 있다. 4. ctpu 이제 내 프로젝트와 Bucket을 연동시켜보자. ctpu 를 사용해서 Cloud TPU 프로젝트 리소스를 만들고 관리하도록 하자. 먼저, ctpu를 설치하자. 맥 OS의 경우 아래의 명령어로 ctpu를 설치할 수 있다. 12# Mac$ curl -O https://dl.google.com/cloud_tpu/ctpu/latest/darwin/ctpu && chmod a+x ctpu 설치가 끝났으면, 내가 사용하고자 하는 region을 명시하여서 ctpu up 을 호출하자. 아래와 같이 명시를 하지 않으면 기본값인 us-central1-b등으로 임의로 할당하는 경우가 있다. 1$ ctpu up --zone=us-central1-f 이 ctpu up이라는 명령어를 사용하면, 알아서 Compute Engine VM과, TPU node를 생성해줄 뿐만 아니라, IAM 계정도 만들어서 등록해주고, 만들어진 Compute Engine VM에 로그인까지 할 수 있게된다. 1234567891011121314151617$ ctpu up --zone=us-central1-fctpu will use the following configuration: Name: petercha0990 Zone: us-central1-f GCP Project: object-detection-265202 TensorFlow Version: 2.1 VM: Machine Type: n1-standard-2 Disk Size: 250 GB Preemptible: false Cloud TPU: Size: v2-8 Preemptible: false Reserved: falseOK to create your Cloud TPU resources with the above configuration? [Yn]: y 성공적으로 위에서 말한 과정이 끝나게 되면, 아래처럼 쉘 프롬프트 username@tpuname로 변경된다. 하지만 우리는 이 안에서 작업을 할 것이 아니라, 단순히 이 VM에 딸린 TPU를 사용하고 싶을 뿐임으로, 바로 exit를 입력하고 나가자. 123456789101112131415161718192021222324252627OK to create your Cloud TPU resources with the above configuration? [Yn]: y2020/01/19 15:32:30 Creating TPU petercha0990 (this may take a few minutes)...2020/01/19 15:32:30 Creating Compute Engine VM petercha0990 (this may take a minute)...2020/01/19 15:32:37 TPU operation still running...2020/01/19 15:32:38 Created Compute Engine VM petercha0990!2020/01/19 15:32:59 TPU operation still running...2020/01/19 15:33:21 TPU operation still running...2020/01/19 15:33:42 TPU operation still running...2020/01/19 15:33:53 Created TPU petercha0990!2020/01/19 15:33:55 Updating the project's IAM policy to add the TPU service account ('service-630609466618@cloud-tpu.iam.gserviceaccount.com') to roles: roles/logging.logWriter and roles/storage.admin.About to ssh (with port forwarding enabled -- see docs for details)...Updating project ssh metadata...⠶Updated [https://www.googleapis.com/compute/v1/projects/object-detection-265202]. Updating project ssh metadata...done. Waiting for SSH key to propagate.Warning: Permanently added 'compute.8979413283803908368' (ECDSA) to the list of known hosts.Enter passphrase for key '/Users/chayesol/.ssh/google_compute_engine': Enter passphrase for key '/Users/chayesol/.ssh/google_compute_engine': bind [127.0.0.1]:8888: Address already in usechannel_setup_fwd_listener_tcpip: cannot listen to port: 8888Linux petercha0990 4.9.0-11-amd64 #1 SMP Debian 4.9.189-3+deb9u2 (2019-11-11) x86_64The programs included with the Debian GNU/Linux system are free software;the exact distribution terms for each program are described in theindividual files in /usr/share/doc/*/copyright.Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extentpermitted by applicable law.chayesol@petercha0990:~$ (← VM으로 접속한 상태) 5. Last Configuration 긴 과정을 거쳐왔다… 드디어 환경설정 마지막이다. 로컬머신 .bash_profile에 환경변수 3개만 더 추가해주면 된다. 각자의 bash_profile이 있는 경로로 이동한 후에, 1$ sudo vi .bash_profile 를 입력해서, 아래와 같이 환경변수 3가지를 추가해주자. 12345...export PROJECT=object-detection-265202export YOUR_GCS_BUCKET=object_detection_peterexport TPU_ACCOUNT=service-blahblah@cloud-tpu.iam.gserviceaccount.com... 다 끝났으면, $ source .bash_profile도 잊지 말고 실행시켜주자! 고생하셨습니다. 다음편에는 실제 Tensorflow Transfer Learning을 각자의 필요에 맞게 원하는 Class 개수에 맞게 학습을 진행하는 방법을 살펴 보겠습니다! 👏👏 이 글은 DataCrew 에서도 읽으실 수 있습니다. References Training and serving a realtime mobile object detector in 30 minutes with Cloud TPUs - Link GCP Project Setting - Link Step by Step TensorFlow Object Detection API Tutorial - Part 1 ~ 5 - Link document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/01/19/HowtoUse_Google_CloudTPU_1/"},{"title":"Docker tutorial (3)","text":"Docker Container에 SSH 접속 현재 내 컨테이너를 이미지로 저장하기 Docker Tutorial, 세 번째⭐입니다! 이번 포스팅에서는 이전 글에서 노출시킨 22 port 를 사용, 컨테이너 환경으로 외부에서 SSH 접속을 하는 방법에 대해 다루고, SSH Setting이 끝난 컨테이너 자체를 이미지로 만드는 방법을 알아보도록 하겠습니다. 최근들어 많은 회사나 단체, 개인들이 AWS나 Google Cloud Platform 등을 사용하면서 GPU 및 TPU Server를 사용하고 있죠. 그래서 당연히 AWS나 Google Cloud에서도 Docker를 사용할 수 있게 됐고, 자연스럽게 언제 어디서든 무거운 계산이나 학습은 Cloud나 원격 서버에서 처리하고 실제 사용자는 자유롭게 접속하고 해당 머신을 사용할 수 있게 되었습니다. Digital Nomad🐑가 가까워지고 있습니다. 혹은 Digital Slave.. 이런 현실에 맞춰, SSH로 내 Deocker Container에 접속하는 법 정도는 아는 것이 좋겠죠? 물론 키와 패스워드 관리 및 보안상의 이유로 일각에서는 SSH로 컨테이너에 접속할 수 있게 하는 것을 우려하지만, 개인적인 학습이나 가벼운 실험 용도로만 사용한다고 했을 때는 효율적이라고 생각해 포스팅을 하기로 했습니다. SSH Server in Container1. Container 생성 (--cap-add) 먼저 SSH을 사용할 수 있게 이미지로 부터 컨테이너를 생성할 때, 아래와 같은 옵션을 함께 줍니다. 이미지는 Hi, Docker! :) (2)에서 생성한 docker101이미지를 사용합니다. 1$ docker run -d --name ssh_container --cap-add=NET_ADMIN --cap-add=NET_RAW -p 8888:8888 -p 22022:22 docker101 --cap-add=NET_ADMIN에서 --cap-add 옵션은, 컨테이너에게 특정한 cgroups 을 사용하게 해주는 것으로, 여기서는 admin의 network를 사용하겠다는 의미로 NET_ADMIN 을, admin의 iptables를 그대로 사용하겠다는 의미 NET_RAW 를 변수로 줍니다. cgroups: control groups의 약자 - 프로세스들의 자원의 사용(CPU, 메모리, 디스크 입출력, 네트워크 등)을 제한하고 격리시키는 리눅스 커널 기능 (feat. wikipedia) 저는 -p 22022:22 로, 컨테이너가 SSH 접속을 허용할 22번 포트를 Host의 22022와 연결시켜 주겠습니다. 2. SSH, ufw 설치 이렇게 생성한 컨테이너에 이제 SSH 접속을 할 수 있도록 하려면, 손봐줄 것이 많기때문에…👊 백그라운드에서 돌아가고 있는 ssh_container에 exec로 접속합니다. 1$ docker exec -it ssh_container /bin/bash 그리고 SSH와, 리눅스에서 방화벽을 관리해주는 ufw를 설치합니다. 1$ apt-get install ssh ufw -y 아래와 같이 sshd의 위치가 잘 나온다면 설치가 잘 된 것입니다! 12[work] # which sshd/usr/sbin/sshd 3. 비밀번호 설정 (sshd_config, passwd root) SSH 접속을 할 때 물어볼 비밀번호를 설정하려면, 먼저 /etc/ssh/ 아래에 있는 sshd_config파일을 좀 수정해야합니다. 1$ vim /etc/ssh/sshd_config 를 보시면, 123456789# $OpenBSD: sshd_config,v 1.101 2017/03/14 07:19:07 djm Exp $# This is the sshd server system-wide configuration file. See# sshd_config(5) for more information.# 어쩌구저쩌구......#LoginGraceTime 2m#PermitRootLogin prohibit-password#StrictModes yes... 라고 뜰 텐데요, 여기서 아래와 같이 #PermitRootLogin prohibit-password 부분의 주석을 해제하고, prohibit-password를 yes로 바꿔줍니다. 12345...#LoginGraceTime 2mPermitRootLogin yes#StrictModes yes... 그렇게 저장을 하신 뒤에, 다시 bash로 돌아와 passwd root 을 입력해서 비밀번호를 설정해주면 끝입니다! 1234[work] # passwd rootEnter new UNIX password:Retype new UNIX password: passwd: password updated successfully 4. ufw 세팅 etc/ufw/아래에 있는 ufw.conf파일에서 ENABLED=no를, 1$ vim /etc/ufw/ufw.conf 1234# /etc/ufw/ufw.conf...ENABLED=no... ENABLED=yes로 👇 바꿔줍니다. 123...ENABLED=yes... 그 뒤에, bash로 돌아와서 ufw enable 로 활성화를 해주고, 12[work] # ufw enableFirewall is active and enabled on system startup 이제 실제로 22번 port를 방화벽에서 허락할 수 있게 ufw allow 를 사용합니다. 123[work] # ufw allow 22/tcpRule addedRule added (v6) 5. SSH server 시작 수고하셨습니다. 이제 귀찮은 일은 다 끝났습니다.service ssh start로 ssh를 시작하고, 12[work] # service ssh start * Starting OpenBSD Secure Shell server sshd [ OK ] service ssh status로 동작을 확인하면 끝! 12[work] # service ssh status * sshd is running SSH Client 자, 그럼 열심히 세팅한 컨테이너로 이제 접속을 해봐야겠죠?😎 당연하지만 기본 개념은 Port forwarding입니다. Host의 22022번 port가 ssh_container의 22번 port와 연결되어있기 때문에, 기본적으로 외부에서 Docker 컨테이너로 접속을 하려면 Docker가 실행되고 있는 Host 머신을 통해서 연결을 해야합니다. 먼저 도커가 실행되고 있는 제 PC는 맥북인 관계로 ifconfig를 사용해서 제 컴퓨터의 IP가 172.16.8.236라는 것을 알아냈습니다. (linux: ifconfig - window: ipconfig) 따라서, ssh 명령어도 아래와 같습니다. 1$ ssh -p 22022 root@172.16.8.236 그렇다면 이 명령을 내리는 의미는 아래의 그림과 같습니다. 아래와 같이 접속을 허용하겠냐는 물음에 답하면 비밀번호를 물어보고, 12345The authenticity of host '[172.16.8.236]:22022 ([172.16.8.236]:22022)' can't be established.ECDSA key fingerprint is SHA256:iMDl7X79mKNZEA5oadtMr2zQdJhJ4n2toAeJ58o9Tsg.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added '[172.16.8.236]:22022' (ECDSA) to the list of known hosts.root@172.16.8.236's password: 비밀번호를 입력하면 드디어 ssh_container 안으로 입성(?)하게 됩니다! 123456789101112131415161718Welcome to Ubuntu 18.04.2 LTS (GNU/Linux 4.9.125-linuxkit x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantageThis system has been minimized by removing packages and content that arenot required on a system that users do not log into.To restore this content, you can run the 'unminimize' command.The programs included with the Ubuntu system are free software;the exact distribution terms for each program are described in theindividual files in /usr/share/doc/*/copyright.Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted byapplicable law. [~] # ssh를 사용해서 컨테이너 안으로 들어왔습니다! work directory안에 반가운 Data폴더도 그대로 잘 있네요. 이제 곧 Digital Nomad🐑가 될 것같은 기분 은 희망사항 입니다. 1234[~] # cd work[work] # ls -a. .. .empty Data[work] # 이번 포스팅에서 예시로 들었던 IP 172.16.8.236은 내부망(같은 공유기를 쓰는, 혹은 같은 사내망)입니다. 따라서, 때로는 192.xx.xxx.xx가 될 수도 있죠. 같은 내부망뿐만 아니라, 진정으로 인터넷을 통해 어디서나 접근을 가능하게 하고 싶다면 저 IP는 Host가 부여받은 Public IP인 external IP이어야 합니다. AWS나 GCP에서 Docker 컨테이너를 사용하고 계신다면, 사용하고 계신 Cloud Machine에 접근할 수 있는 IP를 알면 되는 것이고, 자체 사설 서버를 운영하신다고 해도 마찬가지 입니다. 해당 Host 머신이 가지고 있는 Static IP, 고정IP가 되겠죠. 다만 각각의 환경에 따라 ssh 접속을 허용하기 위한 SSH Server측의 방화벽 및 구체적인 세팅을 다 설명하기에는 이번 포스팅의 논지를 흐릴 것 같아 사실 제가 힘들어서😞, 내부망 접속이라는 예시를 통해 ssh container 접속의 개념만 살펴보았습니다. 외부망을 통해 Docker Container에 접속을 하시려면, 필요에 따라 가지고 계신 환경에 따른 더 많은 정보가 필요합니다. Save My Container as an Image.(commit) 더 게으르고 싶은 개발자가 좋은 개발자기 때문에, 우리는 이 귀찮은 작업들을, 매번 컨테이너를 생성할 때마다 해줄 수는 없습니다! 이렇게 힘들게(?) 세팅한, SSH 접속이 되는 ssh_container를 image로 만들어서 Docker Hub에도 올리고, 편하게 사용하고 싶습니다. 이것을 가능하게 해주는 명령어가 commit 입니다. 말이 나온 김에 지금 당장 사용하도록 하겠습니다. ssh_container로 ssh_machine_learning이라는 이름으로 Image를 만듭니다. 12$ docker commit ssh_container ssh_machine_learningsha256:2e134384b1c846ee76a069db2aad0b2664610c195b9d8c1b03d79b9e3de74a0e 이미지가 잘 생성이 됐는지 확인을 해보니, 잘 생성이 되었네요! 참 쉽죠?😇🤘 1234$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEssh_machine_learning latest 4d9c11b16e4c 4 seconds ago 4.75GBdocker101 latest 71522855bd07 4 days ago 4.72GB Test 그럼 정말로 의도대로 잘 생성된 이미지인지 확인하기 위해, 기존의 ssh_container와, docker101 이미지를 삭제합니다. 123456$ docker stop ssh_containerssh_container$ docker rm ssh_containerssh_container$ docker rmi docker101Untagged: docker101:latest 그리고 생성한 ssh_machine_learning 이미지로 컨테이너를 생성하고 확인합니다. happy_cori라는 이름의 컨테이너로 생성이 되어있습니다. 12345$ docker run -d --cap-add=NET_ADMIN --cap-add=NET_RAW -p 8888:8888 -p 22022:22 ssh_machine_learninge3f1a95dc804632ed1802c17c90b44046f13301a261d06ebd48dcd7cb8f57447$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES292970ce1cff ssh_machine_learning \"/tini -- /bin/sh -c…\" 3 seconds ago Up 2 seconds 0.0.0.0:8888->8888/tcp, 0.0.0.0:22022->22/tcp happy_cori happy_cori로 exec 접속을 해서 SSH Server를 시작시킵니다. 123$ docker exec -it happy_cori /bin/bash[work] # service ssh start * Starting OpenBSD Secure Shell server sshd [ OK ] 위에 나왔던 같은 방식으로, SSH 접속을 해봅니다.팝콘준비🍟 Dealing with an Error 아, 지금 저처럼 같은 PC로 접속을 하신다면 아마도.. 123456789 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!어쩌구저쵸구.....Add correct host key in /Users/chayesol/.ssh/known_hosts to get rid of this message.... 이런 멋진 친구를 먼저 만나게 될 것입니다. 그도 그럴만한 것이, 접속을 하려는 PC 입장에서는 똑같은 IP로 접속을 하려고 하는데, 전혀 다른 Server인데??라고 하면서 '뭐냐이거'라고 정색을 하는 것이죠. 이럴 경우, PC가 가지고 있떤 ssh-key를 아예 초기화시켜주는, 1$ ssh-keygen -R YOUR.IP.ADDR.ESS 를 실행한 뒤에 다시 하면 된다는 분들이 계시고, 저같은 경우 이 방법이 안먹혀서 에러 메세지 하단쯤에 친절히 적혀 있는 /Users/chayesol/.ssh/known_hosts을 파일을 열고 해당 IP를 지워버렸습니다. 그리고나서 다시, 도즈언🏊 하니 잘 되네요! 123456789101112131415161718192021222324$ ssh -p 22022 root@172.16.8.236The authenticity of host '[172.16.8.236]:22022 ([172.16.8.236]:22022)' can't be established.ECDSA key fingerprint is SHA256:XFFyjB4g0y4cJzd08AV2nDfxGxzcm8qBoS5n7VRc9fo.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added '[172.16.8.236]:22022' (ECDSA) to the list of known hosts.root@172.16.8.236's password: Welcome to Ubuntu 18.04.2 LTS (GNU/Linux 4.9.125-linuxkit x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantageThis system has been minimized by removing packages and content that arenot required on a system that users do not log into.To restore this content, you can run the 'unminimize' command.The programs included with the Ubuntu system are free software;the exact distribution terms for each program are described in theindividual files in /usr/share/doc/*/copyright.Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted byapplicable law.[~] # 읽어주셔서 감사합니다. 🙇 References ruo91 - GitHub Gist docker의 ubuntu container에 ssh로 접속하기 HOW TO CREATE A DOCKER IMAGE FROM A CONTAINER document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2019/05/25/docker-103/"},{"title":"L1 & L2","text":"L1, L2 loss, regularization, and norm. Machine Learning을 공부하기 시작하면, 꼭 마주치는 L1, L2. L1, L2 loss라고도 하고 L1, L2 Regularization이라고도 하는데, 명확히 그 각각의 개념과 그 차이를 짚고 넘어가려, Loss로써 쓰일 때와 Regularization으로써 쓰일 때를 정리해 보았다. As an Error Function 모델의 Loss, 즉 Cost를 구하는 방법으로 사용하겠다 하면 L1, L2 loss function은 아래와 같은 식을 사용한다. L1 lossL1 loss부터 살펴보면, 식에서 보는 것과 같이 실제 값($y_i$)과, 예측값($f(x_i)$)의 그 차이값에 절대값을 취해, 그 오차 합을 최소화하는 방향으로 loss를 구한다.Least Absolute Deviations라고 하고 줄여서, LAD라고 한다. $$L = \\sum\\limits_{i=1}^{n}|y_i - f(x_i)|$$ L2 lossL2 loss는 MSE (Mean Sqaured Error)를 안다면 아주 익숙한 개념으로, target value인 실제값($y_i$)과 예측값($f(x_i)$) 사이의 오차를 제곱한 값들을 다 합해서 Loss로 산정한다.Least squares error라고 하고, 줄여서 LSE라고 한다. $$L = \\sum\\limits_{i=1}^n(y_i - f(x_i))^2$$ L1 loss와 L2 loss 비교 L1 loss와 L2 loss는 아래와 같은 차이점을 가지고 있다. 1. Robustness: $$L1 > L2$$ 여기서 말하는 Robustness는 outlier, 즉 이상치가 등장했을 때, loss function 얼마나 영향을 받는지를 뜻하는 용어다. L2 loss는 outlier의 정도가 심하면 심할 수록, 직관적으로 제곱을 하기 때문에 그 계산 값이 L1보다는 더 큰 수치로 작용을 할 수 밖에 없기 때문에 Roubustness에서 L1보다 더 그 성질이 작다고 말할 수 있다. 그렇기 때문에, outliers가 효과적으로 적당히 무시되길 원한다면, 비교적 이상치의 영향력을 작게 받는 L1 loss를, 반대로, 이상치의 등장에 주의 깊게 주목을 해야할 필요가 있는 경우라면 L2 loss를 취하는 선택을 할 수 있겠다. 2. Stability: $$L1 < L2$$ Stability는 모델이 비슷한 데이터에 대해서 얼마나 일관적인 예측을 할 수 있는가 정도라고 생각하면 된다. 이해를 돕기 위해, 아래 animation을 참고하자. 위 애니메이션 그래프는 실제 데이터(검은 점)와, Outlier point인 주황색 점이 움직임에 따라서 어떻게 그 예측 모델이 달라지는지 실제로 실험을 해 본 결과다. Outlier point가 검은 점들과 비교적 비슷한 위치에 위치해서 덜(?) 이상치 일때, L1 loss 그래프는 변화가 있고 움직이지만, L2 loss 그래프에는 없다는 것을 관찰 할 수 있다. 이런 성질을 보고 L1이 L2보다는 Unstable하다고 표현한다. 이 애니메이션에서 위에서 살펴본 Robustness도 살짝 관찰할 수 있는데, Outlier point가 검은 점들이 구성하는 보이지 않는 선을 기준으로 밖에서 안으로 들어올 때, 확실히 L2 error line이 먼저 반응하는 것도 관찰할 수 있다. As Regularization Machine learning에서 Regularization은 Overfitting을 방지하는 중요한 기법이다. 그래서 수식적으로 L1, L2 Regularization을 말하자면, 모델을 구성하는 계수(coefficients)들이 학습 데이터에 너무 완벽하게 Overfit되지 않도록, 그저 정규화 요소(regularization term)를 더해주는 것이다. L1 regularization$$cost(W, b) = \\frac{1}{m}\\sum\\limits_{i}^mL(\\hat y_i, y_i) + \\lambda\\frac{1}{2}|w|$$ L2 regularization$$cost(W, b) = \\frac{1}{m}\\sum\\limits_{i}^mL(\\hat y_i, y_i) + \\lambda\\frac{1}{2}|w|^2$$ 위와 같이, 더해주는 정규화 요소로 L1 error때 봤던 절대값을 취하는 기법을 쓰냐, L2 error에서처럼 제곱을 취하는 값을 주냐에 따라 L1 정규화이냐 L2 정규화로 나뉜다. 아래는 딥러닝에서 쓰는 Loss function에 각각의 정규화를 취한 식이다. $\\lambda$는 얼마나 비중을 줄 것인지 정하는 상수다. 0에 가까울 수록 정규화의 효과는 없어진다. 우리는 적절한 $\\lambda$값을 k-fold cross validation과 같은 방법으로 찾을 수 있다. L1, L2 Regularization 차이 비교 두 정규화 방식에는 어떤 차이점이 있을까? 우선 그 차이를 확실히 알기 위해 이제 언급될 Norm이라는 개념에 대해 잠깐 언급하고 넘어가도록 하겠다. Norm Norm은 벡터의 길이 혹은 크기를 측정하는 방법(함수)이다. $$L_p = \\big(\\sum\\limits_i^n|x_i|^p\\big)^{\\frac{1}{p}}$$ $p$는 Norm의 차수를 의미한다. 따라서, $p$가 1이면 L1 norm이고, $p$가 2이면 L2 norm이다. $n$은 대상 벡터의 요소의 수다. 보통 Norm은 $||x||_1$ 혹은 $||x||_2$와 같이 L1 Norm이냐, L2 Norm이냐를 구별하는데, 아무런 표시가 없는 $||x||$와 같이 차수가 생략이 되었다면, L2 Norm을 의미한다. Norm 계산의 결과로 나오는 수치는 원점에서 벡터 좌표까지의 거리고, Magnitude라고 부른다. L1, L2 정규화는 이같은 L1, L2 Norm을 사용한 값을 더해주는 것이다. 그래서 실제로 너무 Overfitting이 발생할 수 있는 가능을 가진 수치에 Penalty를 부여한다고 생각하면 된다. 1. Solution uniqueness & Computational efficiency. $$L2$$ 다시 본론으로 돌아와서 L1, L2 정규화의 차이점을 알아보자면, L1, L2 정규화는 L1, L2 Norm을 계산함에 있어서 아래와 같은 특징을 지니게 된다. 초록색이 L2 Norm인데, Square연산에 의해 유일한 Shortest path를 가지는 반면, L1 Norm을 의미하는 빨강, 파랑, 노랑색 path들은 다 같은 길이를 가지지만 제각각 다른 모양을 하고 있다. 이런 특징 때문에 Computational efficiency에서는 L2 Norm이 효율적인 계산량을 제공한다고 볼 수 있다. 2. Sparsity & Feature Selection $$L1$$ $L1$ 정규화의 Sparsity를 설명하기 위해, 다음과 같은 두 Vector가 있다고 생각해보자. a = (0.25, 0.25, 0.25, 0.25)b = (-0.5, 0.5, 0.0, 0.0) 이 두 벡터의 $L1 \\text{ norm}$을 구하면, $||a||_1 = |0.25| + |0.25| + |0.25| + |0.25| = 1$ $||b||_1 = |-0.5| + |0.5| + |0.0| + |0.0| = 1$ 과 같이 같은 1이라는 숫자가 나오지만, $L2 \\text{ norm}$을 구하면, $||a||_2 = \\sqrt{0.25^2 + 0.25^2 + 0.25^2 + 0.25^2} = 0.5$ $||b||_2 = \\sqrt{(-0.5)^2 + (0.5)^2 + 0^2 + 0^2} = 0.707$ 과 같이 다른 수가 나온다. 이런 L1과 L2의 차이점은 위에서 살펴본 L2의 Solution uniqueness의 성질과 맞물려 생각할 수 있는데, L2는 이처럼 각각의 Vector에 대해 유니크한 값을 추출하는 반면, L1은 경우에 따라 특정 Feature(Vector의 요소)없이도 같은 값을 낼 수 있다는 말이 된다. 이런 특징으로 L1 norm은 Feature Selection을 하는 데 사용할 수 있고, 특정 Feature들을 0으로 처리해버리는 것이 가능하기 때문에 결과적으로 그 Coefficient들이 Sparse한 형태를 가질 수 있다. 만약 $\\beta = [\\beta_0, \\beta_1]$이라는 벡터가 있을 때, 그 L1, L2 Norm의 값이 똑같이 1이라고 했을 때, L1과 L2에서 가능한 영역을 표시를 하자면 아래 그림과 같다. $$\\text{L1: }||\\beta||_1 = |\\beta_0| + |\\beta_1| = 1$$ $$\\text{L2: }||\\beta||_2 = \\sqrt{(\\beta_0)^2 + (\\beta_1)^2} = 1$$ 위의 그림- L2의 이런 원을 Unit Circle이라고 한다 - 위의 예시에서 본 것과 같이 ‘특정 요소가 0일 수 있는 경우의 수 / 전체 경우의 수’ 로 L1 Norm과 L2 Norm을 비교한다고 생각하면 ‘L1 norm의 경우 좀더 β의 요소 중 0이 들어갈 수 있는 가능성이 더 높다’고 말할 수 있다. 이해를 돕기위해 본 포스팅의 댓글로 C B님이 알려주신 것처럼, 마름모의 둘레는 한 변의 길이가 $\\sqrt{2}$ 이니 대략 5.6568… 이 되고, 반지름이 1이니 원의 둘레는 6.28…정도 된다는 점을 생각해보면 된다. 이런 특징이 L1의 Sparsity, 혹은 Feature Selection이라는 개념을 가질 수 있게 해준다고 생각할 수 있다. Feature가 너무 많은 데이터셋을 다룰 때 유용하게 쓸 수 있다. 이러한 Feature Selection의 특징 때문에 L1 norm은 convex optimisation을 할 때 유용하게 쓰인다고 한다. 참고 L1 regularization을 쓰는 Regression model을 Lasso(Least Absolute Shrinkage and Selection Operator) Regression. L2 regularization을 쓰는 Regression model을 Ridge Regression 이라고 부른다. Reference Quora Garbled Notes Anuja Nagpal TAEWAN.KIM 블로그 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2018/09/24/l1l2/"},{"title":"Docker tutorial (1)","text":"Introduction to Docker. Docker 개념, 설치, 유용한 명령어 사용해보기. Docker? 😶 귀여운 고래 아이콘🐳으로 많은 사랑을 받고 있는 Docker에 대해 간단히 알아보도록 하겠습니다. Docker는 기존의 Virtual Machine들과 같이 Host OS 위에 Guest OS를 올리는 것이 아니라, 별도의 OS를 만들지 않고 단순히 프로세스만 격리시켜서 동작하는 방식입니다. 그래서 훨씬 더 빠르게 가상환경을 즐길 수 있게 해줍니다! 그래서 CPU나 메모리는 프로세스가 필요한 만큼만 사용하기 때문에 성능도 실제 Host에서 돌아가는 다른 Process들과 비교해서 큰 차이가 없답니다. 😉 Container 안에 다 있어요! Docker가 핫한 이유는 이게 다가 아니겠죠. 머신러닝을 위한 환경구축을 해보신 분들은 다들 공감하실 수 빡쳐 보신 적👿있으실 겁니다. 내가 그렇게 수많은 에러들과 StackOverflow를 헤매가며 설정한 그 환경 말이죠. (Anarconda + Tensorflow + Pytorch + R + R Studio + cuDNN + 온갖 Python Packages + 등등..) 이제 Docker를 쓰면, 그렇게 만든 환경을 Docker계의 Github인 Docker Hub에 Image화 한 뒤 올리면, 어디서나 받아서 쓸 수 있습니다. 또한 남들이 열심히 만들어서 공유해 준, 인성 거의 산타🎅 환경을 받아 편하게 쓸 수도 있습니다. 너무 좋죠? 그렇게 공유하는 (Github의 Private Repository처럼 Docker Hub에도 Private계정을 제공합니다.) 파일을 Image라고 하고, 그 Image를 받아서 생성하게 되는 하나하나의 Process 가상환경을 우리는 Container라고 부릅니다. Docker라는 이름에 걸맞게 항만에서 수많은 컨테이너들이 공유되는 환경이 Docker Hub이라고 생각하시면 되겠네요. 그럼 이제 Docker를 설치해 보실까요! InstallationLinux Ubuntu 환경만 설명하자면, 아래와 같은 명령어로 간편하게 설치 하실 수 있습니다. 명령어 위에 주석처리된 부분은 2019년 5월 현재 Ubuntu 환경의 Prerequisites이니 참고하세요 :) 12345678# OS requirements# To install Docker CE, you need the 64-bit version of one of these Ubuntu versions:# Cosmic 18.10# Bionic 18.04 (LTS)# Xenial 16.04 (LTS)$ sudo apt-get update$ sudo apt-get install docker-ce docker-ce-cli containerd.io 다른 Linux 환경은 여기를 참조하세요. Mac & Windows Mac과 Windows 환경은 다음과 같은 기준에 따라 설치 방법이 두 가지로 나누어 집니다. Windows: Windows 10 Pro 이상 모델 (10 Home 안됨) → Docker for WindowsMac: OS가 Sierra 10.12 혹은 그 이상 → Docker for Mac Windows & Mac: 1번 조건을 만족하지 못하는 경우 → Docker ToolBox 참고 사항 기본적으로 Docker에 회원가입을 하시고 ID를 생성하셔야 설치 파일을 다운을 받을 수 있습니다. Windows에서는 [작업 관리자 > 성능] 에 들어가 \"가상화 : 사용\" 을 확인을 하시고 안되어있다면, 활성화를 해줘야 합니다. 개인적인 경험상, Windows 환경에서 1번의 환경이 충족되어 Docker for Windows를 설치하였는데도, Linux Container로 Switch를 못한다 던가 하는 에러가 발생되어 사용이 힘들 때$\\to$ 그냥 2번으로 Docker ToolBox를 설치하고, Docker Quickstart Terminal을 사용, Docker를 실행하기도 했습니다. 되기만하면 장땡이니까요 설치 확인 docker version 이라고 Terminal에 쳤을 때, 아래와 같이 version 정보가 잘 나온다면 설치가 잘 된 것입니다. :) 123456789101112131415161718Client: Docker Engine - Community Version: 18.09.2 API version: 1.39 Go version: go1.10.8 Git commit: 6247962 Built: Sun Feb 10 04:12:39 2019 OS/Arch: darwin/amd64 Experimental: falseServer: Docker Engine - Community Engine: Version: 18.09.2 API version: 1.39 (minimum version 1.12) Go version: go1.10.6 Git commit: 6247962 Built: Sun Feb 10 04:13:06 2019 OS/Arch: linux/amd64 Experimental: true Docker version을 확인해보니, Client-Server로 나뉘어져 있는 것을 확인할 수 있습니다. Docker가 실제로 어떻게 동작하는지 볼 수 있는 부분입니다. 😉 사용자가 Docker 명령을 내리면 Client에서 기본적으로 Docker Server를 바라보고 있기 때문에, 사용자는 바로 명령만 내린 것 같지만, 실제로는 Server가 Client로 부터 전송을 받아, 처리한 결과를 다시 Client에게 돌려주고 있는 것이죠. 자, 이제 설치를 잘 마쳤으면 본격적으로 Docker를 사용해 볼까요? Practice 아래와 같은 순서로 실습을 하면서 필요한 명령어들을 정리해 보겠습니다 😉 1231. Anaconda가 설치되어 있는 Ubuntu Image를 다운받기. 2. 그 이미지로 생성한 컨테이너에서 Jupyter notebook을 띄워놓기.3. Host Browser로 접근해서 사용. 1. 이미지 다운로드 하기 (pull, images) Docker Hub에 접속해서 jupyter-python3로 검색을 하니, 천만 다운로드에 빛나는 이미지가 나오네요. 아래의 명령어를 입력하면, Anaconda3를 품은 Ubuntu 18.04 Image를 다운로드 받게 됩니다. 다운로드에 약간의 시간이 소요됩니다. 1$ docker pull civisanalytics/civis-jupyter-python3 보시다시피, 기본적으로 Docker 명령어는 docker로 시작하고, Git을 쓰신 분들은 익숙한 단어이실 pull이라는 명령어로 원하는 이미지를 가져 올 수 있습니다. 가져온 이미지는 명령어 1$ docker images 를 통해 확인할 수 있습니다. 2. 컨테이너 목록 조회 & 삭제 (ps, rm, stop) 컨테이너 실습에 앞서서, 깔끔한 진행을 위해 현재 Docker 설치시에 Default로 가지고 있는 컨테이너들을 한 번 싹 비우고 시작하도록 하겠습니다😏. 아래 명령어를 입력하시면 현재 멈춰있는 컨테이너까지 포함한 목록들을 확인할 수 있습니다. 1$ docker ps -a docker ps는 현재 동작하고 있는 컨테이너를 모든 컨테이너들을 조회하는 명령어 입니다. -a 옵션은 멈춘 컨테이너까지 모두 조회합니다. 아직 컨테이너 생성은 하지도 않는데 목록에 멈춰있는 다른 컨테이너들이 보이실 것입니다. 다 지워보도록 하겠습니다.😈 1$ docker rm $(docker ps -a -q) docker rm는 멈춰있는 컨테이너를 삭제하는 명령이고, -q 옵션은, PORT, STATUS, NAME등의 정보는 제외하고 컨테이너 ID만 확인하게 해주는 옵션입니다. 따라서 바로 위에서 배웠던 docker ps -a -q의 결과로 나오는 ID에 해당하는 모든 컨테이너들을 삭제해라는 명령이 됩니다. 컨테이너가 아니라 이미지를 삭제하고 싶으면 docker rmi로 i만 추가해주세요! 혹시 컨테이너가 정지 상태가 아닌데 docker rm container_ID를 실행하시면 해당 컨테이너는 삭제되지 않습니다. 먼저 docker stop container_ID로 삭제하고자 하는 컨테이너의 동작을 멈춘 뒤에 실행해야합니다. 3. 컨테이너 생성 & 포트 설정 & 이름지어주기 (run -p, –name) 이제 우리가 사용할 우분투 컨테이너를 생성하고 접속해보도록 하겠습니다. 123$ docker run -it --name docker101 \\ -p 8888:8888 civisanalytics/civis-jupyter-python3 \\ /bin/bash run명령어로, 가지고 이미지를 실행하라고 하면서 -it 옵션을 주어서(-ti도 같습니다) 터미널(t)에 입력(i) 을 받을 수 있게 해줍니다. -i와 -t옵션을 함께 쓴 거죠. 제일 마지막에 터미널의 경로를 /bin/bash 로 전달해줍니다. --name 옵션을 줘서 원하는 이름을 부여할 수도 있습니다. 이름을 따로 주지 않으면 유명한 과학자의 이름에 수식어를 붙여서 랜덤생성합니다.(장영실도 포함되어있다고 하네요!) -p 옵션은 포워딩 해줄 port번호를 의미하는데요, 앞에 8888은 호스트 port, 뒤의 8888은 컨테이너 port를 의미합니다. 접속이 잘 되었다면 아래와 같이 sudo mode로 root권한을 가진 상태의 bash를 쓸 수 있게 됐습니다! 간단한 ls -a이나 pwd같은 명령어들로 가상환경 Ubuntu를 느껴보세요.😎. apt update로 우분투를 업데이트 해줍니다. 1$ [work] # apt update 사실 run명령어는 실행하라고 한 이미지가 로컬에 없을 경우, 가장 최신 버전으로 다운을 받기 때문에, 이미지를 다운받으면서 동시에 컨테이너를 만드는 명령어이기도 합니다. 하지만, 한 번 다운 받고 난 뒤에는 해당 이미지가 업데이트가 되어도 가지고 있는 이미지만 사용하기 때문에, 가장 최신 버전을 다운 받게 해주는 pull도 메리트가 있는 것이죠. 다시 본론으로 돌아와, 접속한 컨테이너 우분투 환경에서 python을 입력해보면, Acaconda3-python 3.7이 설치 되어있음을 확인할 수 있습니다. 이로써 Anaconda를 설치한 적도 없지만, 사용할 수 있는 환경을 Get하게 됐습니다.😊 12345[work] # pythonPython 3.7.1 | packaged by conda-forge | (default, Feb 18 2019, 01:42:00) [GCC 7.3.0] :: Anaconda, Inc. on linuxType \"help\", \"copyright\", \"credits\" or \"license\" for more information.>>> 이 상태에서 jupyter notebook을 바로 실행을 시킬 수도 있습니다. 하지만 다른 명령어들을 더 익히기 위해 exit로 일단 나오도록 합니다. 4. 컨테이너 시작, 명령어 실행시키기 (start, exec -d) Jupyter notebook을 실행시키도록 해보겠습니다! 먼저 docker ps -a을 입력해서, 방금 나왔던 컨테이너의 이름을 확인해봅니다. exit명령어로 환경을 나오면, 기본적으로 컨테이너는 Stopped(Exited)인 상태입니다. 123chayesol-ui-MacBook-Pro:Blog chayesol$ docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES57ca32782e7e civisanalytics/civis-jupyter-python3 \"/tini -- /bin/bash\" 9 minutes ago Exited (0) 6 minutes ago docker101 제가 생성할 때 이름으로 주었던 docker101이 이름으로 잘 보이네요! 이 컨테이너는 지금 Stopped 상태(Exited)이기 때문에 새로 동작을 시켜 줘야합니다. docker start명령어로 다시 작동시킨 뒤, Jupyter notebook을 실행합니다. 123$ docker start docker101$ docker exec docker101 jupyter notebook \\ --ip=0.0.0.0 --port=8888 --allow-root exec 명령어는 해당 컨테이너 이름(or ID) 뒤에 나열된 명령어들을 실행하게 해줍니다. ip는 0.0.0.0으로 로컬호스트를 지칭하고, 포트는 우리가 컨테이너 생성할 때 작성한 8888로 주었습니다. 실행 후, 나오는 token값을 복사한 뒤, 호스트의 브라우저 창에 localhost:8888을 입력하시면 token을 적게 되어 있습니다. 그 결과, 아래와 같이 우리는 컨테이너에 있는 Jupyter notebook을 도커를 통해 로컬호스트에서 사용할 수 있게 되었습니다!😆 exec -d? exec명령을 줄 때, -d옵션을 추가하면, detached mode, 즉 백그라운드에서 컨테이너가 실행되도록 할 수 있습니다. (-d옵션은 run에도 쓸 수 있습니다.) 우리 예시에서는 보안상 Jupyter notebook의 token을 출력받고 사용해야하기 때문에 -d옵션을 사용하지 않았습니다. 보안상 권장사항은 아니지만, exec -d 옵션을 사용해서 편하고 빠르게, ‘귀찮은 token입력 없이 jupyter notebook만 내가 띄우고 싶다!’ 하시면 아래와 같은 명령어를 사용하실 수 있습니다. 1234$ docker exec -d docker101 \\ jupyter notebook --NotebookApp.token='' \\ --ip=0.0.0.0 --port=8888 --allow-root$ 실행 후에는 아무 일도 없다는 듯이 그 다음 line을 출력하지만, localhost:8888로 접속하시면 똑같이 Jupyter notebook을 사용하실 수 있습니다. 5. 컨테이너 재접속 & 재시작 (attach, restart) Jupyter notebook은 성공적으로 띄웠으나, 컨테이너에 다시 접속해서 패키지를 더 설치하거나 환경을 세팅해줘야 할 경우, 재접속 하는 방법은 exec 를 사용하는 방법과 attach 를 사용하는 방법, 두 가지가 있습니다. 또 다른 Terminal을 띄우신 뒤에, 123$ docker exec -it container_name /bin/bashor$ docker attach container_name 둘 중 하나를 입력하시면 됩니다! 컨테이너를 재시작 해주고 싶은 경우는 docker restart 를 사용합니다. 1$ docker restart contatiner_name 이상, Docker의 개념과 설치, 그리고 간단한 기본 명령어들에 대해 알아보았습니다. 다음 Hi, Docker!:) (2)에서는 컨테이너와 로컬저장소 연결하기 와 내 컨테이너 이미지로 업로드하기 라는 두가지 주제를 다뤄보도록 하겠습니다. 감사합니다 😌 References 이 글은 아래 두 글을 기반으로 작성되었습니다😄. 더 자세한 정보는 ↓ 초보를 위한 도커 안내서 - 도커란 무엇인가? 초보를 위한 도커 안내서 - 설치하고 컨테이너 실행하기 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2019/04/24/docker-intro/"},{"title":"Keras tutorial (1)","text":"A Keras Usage with fashion MNIST Keras example using Colab Pytorch와 Tensorflow의 Wrapper인 Tensorpack만 써봤던 저는, 올해 처음으로 Keras를 사용해보게 되었습니다. 블록과 함께 하는 파이썬 딥러닝 케라스 라는 책으로 공부하면서 그 간결함에 놀랐고, 대충대충 이해하고 넘어갔던 개념들이 좋은 예시로 설명되어 있어, 그 내용들을 정리해보고자 포스팅을 하게 되었습니다. 이번 포스팅은 책의 Part 1, 케라스 시작하기와 Part 2. 딥러닝 개념잡기에 나오는 Keras 예제들을 Fashion MNIST 데이터로 재구성해 본 것입니다. 😬 책 내용 이외에 추가된 부분은, Colab의 Notebook으로 이번 포스팅이 구성이 되었다는 것이고, 여기에서 Colab Notebook으도 동일한 내용 확인하실 수 있습니다. 또, matplotlib의 plot 대신에, Tensorboardcolab을 사용하여서 plot 시각화를 하도록 방식을 바꿔보았습니다. 흔쾌히 블로그 포스팅을 허락해주신 저자 김태영님께 다시 한 번 감사를 표합니다. ☺ Import packages 자, 시작해볼까요! 😎1234567891011121314import osimport kerasimport tensorflow as tfimport keras.utils as utils import matplotlib.pyplot as pltfrom tqdm import tqdm_notebookfrom keras.datasets import fashion_mnistfrom keras.layers import Dense, Activationfrom keras.models import Sequential, load_model# remove error message from tensorflowtf.logging.set_verbosity(tf.logging.ERROR)%matplotlib inline Output: 1Using TensorFlow backend. keras.utils: 자주 사용되는 유용한 기능들 모음. 대표적으로 One-hot encoding을 해주는 to_categorical(), l1-l2 normalize를 가능하게 해주는 normalize() 등이 있다. keras.datasets: MNIST, Fashion MNIST, CIFAR10, CIFAR100, IMDB Movie reviews 긍정-부정 판별셋, Reuters 뉴스토픽 분류셋, Boston 부동산가격 dataset을 불러올 수 있다. keras.layers: Dense부터 CNN, Pooling, Padding, RNN 등등.. 익숙한 딥러닝 layer들의 집합소. keras.models: keras 모델을 만드는 방법은 크게, Sequential과 Model - functional API를 사용하는 방법 2가지로 나뉜다. Pytorch랑 비슷하다. Sequential은 사용할 모델의 부품을 다 설정한 뒤 Input을 넣으면 한번에 레이어와 레이어 사이 설정을 세팅한 output에 맞게 맞춰주고, functional API인 Model을 사용하면 한땀한땀(?) 그 흐름을 구체적으로 설정할 수 있는 자유도를 가진다. 본 포스팅에서는 Sequential을 사용할 것이니, Model을 사용하는 예시는 여기에서 확인. Loading Data Fashion MNIST 1(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data() Output: 12345678Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz32768/29515 [=================================] - 0s 9us/stepDownloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz26427392/26421880 [==============================] - 5s 0us/stepDownloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz8192/5148 [===============================================] - 0s 0us/stepDownloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz4423680/4422102 [==============================] - 2s 1us/step Peeking the data 10개의 Fashion MNIST 데이터들을 시각화! 😏123456789101112class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']plt.figure(figsize=(8, 4))for i in range(10): plt.subplot(2, 5, i+1) plt.xticks([]) plt.yticks([]) plt.imshow(x_train[i], cmap=plt.cm.binary) plt.xlabel(class_names[y_train[i]]) plt.grid(False)plt.show() Preprocessing 50000 for Training, 10000 for Validation, 10000 for Test 123456789x_val = x_train[50000:]y_val = y_train[50000:]x_train = x_train[:50000]y_train = y_train[:50000]# preprocessing x_train = x_train.reshape(50000, 784).astype('float32') / 255.0x_val = x_val.reshape(10000, 784).astype('float32') / 255.0x_test = x_test.reshape(10000, 784).astype('float32') / 255.0 Label one-hot encoding (utils.to_categorical):keras.utils.to_categorical API 1234# label one-hot encoding.y_train = utils.to_categorical(y_train)y_val = utils.to_categorical(y_val)y_test = utils.to_categorical(y_test) Callbacks Tensorboard 띄우기 Early Stopping / 5번을 기다려도 성능이 나아지지 않을 경우 학습 중단 Model Checkpoint / val_loss가 가장 낮을 때만 저장 123456789101112131415161718from tensorboardcolab import *from keras.callbacks import EarlyStopping, ModelCheckpoint# 3. Tensorboard 세팅tbc=TensorBoardColab()# 4. Early Stopping early_stopping = EarlyStopping(patience=5) # 5. Model Checkpointpath = './model/'if not os.path.exists(path): os.mkdir(path) model_path = path + '{epoch:02d}-{val_loss:.4f}.h5'checkpoint = ModelCheckpoint(filepath = model_path, monitor = 'val_loss', verbose = 0, save_best_only = True) Output:123Wait for 8 seconds...TensorBoard link:https://57207182.ngrok.io Create a model / Train / Test Sequential() - 모델 생성 add() - 모델 블럭끼우기 compile() - 모델 학습에 쓸 도구 세팅하기 fit() - 학습시키기 evaluate() - 평가하기 123456789101112131415161718192021222324252627282930313233343536373839# GPU 사용# with tf.device('/device:GPU:0'):# 1. 모델 구성model = Sequential()model.add(Dense(units=64, input_dim=28*28, activation='relu'))model.add(Dense(units=10, activation='softmax'))# 2. 모델 학습과정 설정하기 model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])# 3. 모델 학습시키기for epoch in tqdm_notebook(range(50)): hist = model.fit(x_train, y_train, epochs=1, batch_size=32, verbose=0, validation_data=(x_test, y_test), callbacks=[TensorBoardColabCallback(tbc), early_stopping, checkpoint]) # tensorboard lines tbc.save_value(\"fasion mnist\", \"train_acc\", epoch, hist.history['acc'][0]) tbc.save_value(\"fasion mnist\", \"val_acc\", epoch, hist.history['val_acc'][0]) tbc.save_value(\"fasion mnist\", \"train_loss\", epoch, hist.history['loss'][0]) tbc.save_value(\"fasion mnist\", \"val_loss\", epoch, hist.history['val_loss'][0]) tbc.flush_line(\"train_acc\") tbc.flush_line(\"val_acc\") tbc.flush_line(\"train_loss\") tbc.flush_line(\"val_loss\") if (epoch+1)%10 == 0: print('-----'*5) print(\"Epoch: {} | Loss: {:0.3f} | Acc: {:0.3f}\".format( epoch+1, hist.history['loss'][0], hist.history['acc'][0]))loss_and_metrics = model.evaluate(x_test, y_test, batch_size=32)tbc.close()print('-----'*10)print('\\nLoss and metrics: ' + str(loss_and_metrics)) Output:12345678910111213141516HBox(children=(IntProgress(value=0, max=50), HTML(value='')))-------------------------Epoch: 10 | Loss: 0.393 | Acc: 0.864-------------------------Epoch: 20 | Loss: 0.342 | Acc: 0.879-------------------------Epoch: 30 | Loss: 0.312 | Acc: 0.890-------------------------Epoch: 40 | Loss: 0.288 | Acc: 0.898-------------------------Epoch: 50 | Loss: 0.270 | Acc: 0.90510000/10000 [==============================] - 1s 60us/step--------------------------------------------------Loss and metrics: [0.3537946595430374, 0.8761] Tensorboard Plot: Model Structure Visualization Model 구조 시각화하기: model.summary() & SVG() 😃 12345from IPython.display import SVGfrom keras.utils.vis_utils import model_to_dotprint(model.summary())SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg')) Output: 123456789101112_________________________________________________________________Layer (type) Output Shape Param # =================================================================dense_1 (Dense) (None, 64) 50240 _________________________________________________________________dense_2 (Dense) (None, 10) 650 =================================================================Total params: 50,890Trainable params: 50,890Non-trainable params: 0_________________________________________________________________None Save & Load Model Model 저장 & 불러오기 12 model.save('fashion_mnist_model.h5')model = load_model('fashion_mnist_model.h5') document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2019/06/12/keras_101/"},{"title":"MobileNet V1","text":"2017년 4월 17일. 대놓고 청량감을 주는 이름으로, 현재 많은 모바일, 자율주행 등의 Local 장비에서의 Object Detection으로 사랑받고 있는 Google의 MobileNet.📱 어느새 2019년 6월 12일을 기점으로 Version 3까지 나와버렸습니다. 😎 그 기념으로(?) MobileNet은 과연 어떠한 진화를 거듭해 왔는지 함께 살펴 보실까요? MobileNet Version 1부터 시작합니다! 논문 설명부터는 편의상 평어체를 사용하겠습니다.😶 2017.04.17 MobileNet은 논문에서, 자신들의 $3 \\times 3$ Depthwise Separable Convolutions 을 쓰면, Standard Convolution 계산량의 8배에서 9배 가량 작은 계산을 할 수 있게 된다고 한다. 이것을 가능하게 하는 MobileNet의 핵심 아이디어는 아래와 같다. ‘굳이 모든 Input Feature Map 들에 대해 모든 Filter 가 학습해야하나?’ 😯 1. Standard Convolution 전형적인 Conv layer의 계산량은 아래와 같다. $D_K$: Dimension of the Kernel$M$: Input Channel Depth.$N$: Output channel Depth. Stride가 1이고, Padding이 있다는 전제하에, 이 Conv layer가 $G$를 생산하기 위한 계산량은, 아래와 같이 Input 필터 사이즈인 $M$만큼, 또 Output 필터 사이즈인 $N$만큼 계산하게 된다. $$G_{k,l,n} = \\sum_{i, j, m}K_{i,j,m,n}\\cdot F_{k+i-1, l+j-1,m}$$ $F$: Input Feature map$G$: Output Feature map$K$: Kernel 그래서 총 Cost는, Kernel Size($D_K$)와 Input channel 수($M$), Output channel 수($N$), 그리고 Output feature map size($D_F$)에 의해 결정된다. $$D_K \\cdot D_K \\cdot M \\cdot N \\cdot D_F \\cdot D_F$$ 2. Depthwise Separable Convolutions 이번엔 MobileNet이 사용하는 Depthwise Separable Conv layer의 경우를 살펴보자. 이름에 Separable은, MobileNet이 쓰는 conv layer가 depthwise convolution 과 pointwise convolution 로 나누어져 쌍으로 동작하는 것을 말한다. 2.1. Depthwise Convolution 위에 보이는 바와 같이, ‘하나의 Feature map(=input channel)에 대해, 하나의 filter’를 사용한다. 기존의 $M$만큼 필터를 사용했던 Standard Conv의 필터들보다 훨씬 더 가볍다. 그렇기 때문에 Input channel의 사이즈인 M개만큼의 Kernel(=filter)가 생길 뿐이고, 이는 학습해야할 parameter를 현저하게 줄여주는 역할을 한다. 그래서 이 depthwise conv를 통과하고 생성되는 feature map은 아래와 같이 n이 없어진 계산량을 가지게 된다. $\\hat{K}$: Depthwise Convolution Kernel of size $D_K \\times D_K \\times M$$\\hat{G}$: $m_{th}$ channel of the filtered output feature map 2.2. Pointwise Convolution 이렇게 depthwise conv layer를 통과한 Depth 1짜리 output들은, 결과적으로 새로운 feature가 되기위해서 합쳐져야 한다. 그 역할을 $1 \\times 1$ pointwise convolution 이 해준다. 원하는 $N$만큼의 Depth를 가지는 Feature map 을 만들기 위해, 이렇게 Depthwise Conv를 통과한 $M$개의 output들을 $N$개의 $1 \\times 1$ convolution들이 차원을 맞춰준다. 이 때 비로소 Depthwise separable convolution이라는 칭호(?)를 얻으며, Cost는 아래와 같이 된다. $+$를 기준으로, 앞항은 Depthwise Conv, 뒷 항은 Pointwise Conv. $$D_K \\cdot D_K \\cdot M \\cdot D_F \\cdot D_F + M\\cdot N \\cdot D_F \\cdot D_F$$ 2.3. Depthwise Separable Convolutions 이 과정을 정리하면 아래와 같다. Standard Convolution에 비해 얼마나 계산량이 줄어드는지, 식으로 간단히 계산해보면 아래와 같다. $$\\frac{D_K \\cdot D_K \\cdot M \\cdot D_F \\cdot D_F + M\\cdot N \\cdot D_F \\cdot D_F}{D_K \\cdot D_K \\cdot M \\cdot N \\cdot D_F \\cdot D_F}$$ $$=\\frac{1}{N} + \\frac{1}{D^2_K}$$ 이는 Output으로 낼 feature map의 depth인 $N$과, 사용할 Kernel 사이즈에 따라 기존 Convolution에 비해 계산량이 얼마나 주는지 정해진다는 말이 된다. 모바일넷은 $3 \\times 3$ depthwise conv를 쓰기 때문에, 이는 약, 8배에서 9배 가량 Standard Convolutions 보다 적은 계산량으로, 비슷한 정확도를 내는 모델을 만들 수 있다는 말이 된다. 3. Network Structure and Training 좌측은 일반 Conv. 우측은 Depthwise Separable Conv. Depthwise Separable Conv는 위와같이 Depthwise Conv와 Pointwise Conv뒤에 BN-ReLU를 적용한다. 이렇게 Depthwise conv, Pointwise conv를 각각 하나의 Layer로 센다면, MobileNet은 아래와 같이 총 28 layers로 구성된 모델이 된다. 첫번째 layer는 Standard Conv. Channel이 3으로 계산량이 많은 편이 아니라서 그런듯. Down sampling은 depthwise conv의 첫 번째 layer에서 stride를 2를 주는 방식으로 진행. 마지막에 Average pooling을 사용 $1 \\times 1$으로 축소. Optimizer는 RMSprop with asynchronous gradient descent. Table 4에서는, ImageNet 데이터에 대해서, 같은 Mobilenet Structure에 Standard Conv를 쓴 모델과 Depthwise Separable Conv를 사용했을 때, 계산량과 학습 parameters는 현격히 줄어든 것에 비해 정확도는 1% 정도밖에 낮아지지 않았음을 보여주고 있다. MobileNet에는 두 가지 Hyper-parameter가 있다. Width Multiplier와, Resolution Multiplier다. 4. Width Multiplier: Thinner Models Width Multiplier라 불리는 $\\alpha$의 역할은 각각의 layer를 균일하게 얇게 만들어 줘서 전체 네트워크를 좀 더 가볍게 만들어 주는 것이다. 이는, Input Channels 수인 $M$과 Output Channels 수인 $N$에 width multiplier인 $\\alpha$를 단순히 곱해주는 것으로, $\\alpha$를 사용한 모델의 Depthwise Separable Conv는 아래와 같은 계산량을 가진다. $$D_K \\cdot D_K \\cdot \\alpha M \\cdot D_F \\cdot D_F + \\alpha M\\cdot \\alpha N \\cdot D_F \\cdot D_F$$ $\\alpha \\in (0, 1]$의 값으로서, 보통 1, 0.75, 0.5, 0.25로 세팅이 된다. $\\alpha = 1$이면 baseline MobileNet이 되는 것이고, $\\alpha < 1$이 더 계산량이 줄어든 MobilneNets을 의미한다. $\\alpha$는 계산량, 그리고 학습할 총 Parameters을 대략 $\\alpha^2$ 정도로 줄여주는 효과를 가지고 있다. Width multiplier는 accuracy와 latency, 그리고 모델 size의 합리적인 trade off를 조절하게 해주며, 밑바닥부터 학습을 해로 할 필요가 있는 reduced model을 정의할 때에 사용된다. Table 6. ImageNet 데이터에 대해 $\\alpha$를 사용할 때, 정확도가 Mult-Adds(Multiplications and Additions)와 학습할 Parameters를 비교한 표는 아래와 같다. 5. Resolution Multiplier: Reduced Representation Resolution multiplier $\\rho$(‘Rho’)는 해상도, 즉 layer들이 주고 받는 input image 및 feature map들의 크기를 조절하는 hyper-parameter다. 따라서, Width Multiplier $\\alpha$와 Resolution Multiplier인 $\\rho$를 사용하는 경우의 Depthwise Separable Conv의 계산량은 아래와 같다. $$D_K \\cdot D_K \\cdot \\alpha M \\cdot \\rho D_F \\cdot \\rho D_F + \\alpha M\\cdot \\alpha N \\cdot \\rho D_F \\cdot \\rho D_F$$ $\\rho$역시, $\\rho \\in (0, 1]$의 값을 가지고, 주로 Network의 Input resolution이 224, 192, 160, 혹은 128 사이즈가 되도록 맞춰준다. Table 7. ImageNet 데이터에 대해 $\\rho$를 사용할 때, 정확도가 Mult-Adds(Multiplications and Additions)와 학습할 Parameters를 비교한 표는 아래와 같다. 논문의 Table 3는 $14 \\times 14 \\times 512$ Size Feature map과 $3 \\times 3 \\times 512 \\times 512$인 kernel $K$가 있을 때, Full Conv layer와 비교해서, 위의 $\\alpha$와 $\\rho$라는 Shrinking methods를 썻을 때의 Mult-Adds 계산량과 학습해야할 parameters를 비교한 표다. 6. Performance Table 13. COCO dataset로 Object detection 결과를 다른 모델들과 비교한 표. MobileNet-SSD의 경우 상당한 Mult-Adds와 Parameters 감소를 고려했을 때, mAP(Mean Average Precision)와의 trade-off가 상당히 Reasonable하다. 여기까지, MobileNet V1 리뷰를 마치도록 하겠습니다. 다음 포스팅에서는 MobileNet V2 리뷰로 돌아오도록 하겠습니다. 읽어주셔서 감사합니다. 🙇 References Review: [MobileNetV1] MobileNet [Paper] document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2019/08/11/mobilenet/"},{"title":"Tensorpack tutorial","text":"Tensorflow Wrapper인 Tensorpack을 소개합니다. Tensorflow, Tensorpack ModelDesc와 Trainer를 중심으로Peter ChaTensorpack을 공부하면, 우선 모르는 것들 투성이다. 알고나면 너무 쓰기 편하지만, 처음 접할 때는 너무 많이 추상화 된 API에 ‘이게 tensorflow는 맞는지..’할 정도니까. 우선 이 튜토리얼을 보기 전, 필자의 tensorpack_tutorial.ipynb를 실행해 보길 바란다. 대략적인 dataflow는 데이터를 불러오는 부분으로 이해를 마쳤다고 생각을 하고, dataflow부분은 생략하고 설명을 진행하도록 하겠다. 이번에는 Model의 선언하게 될 때 상속받은 ModelDesc class와, 학습을 실행하는 Trainer들의 모태가 되는 TowerTrainer 에 대해 알아보고자 한다.tensorpack_tutorial.ipynb에서 설명에 해당하는 부분을 함께 찾아보면 이해에 도움이 더 될 것 같다. 이 Tutorial은 Tensorpack documentation을 참고해서 만들었다. 1. Class ModelDescBase Base class for a model description이다. ModelDesc는 ModelDescBase를 기반으로 만들어졌기 때문에, ModelDescBase를 먼저 설명한다. 1.1. build_graph(*args) 모든 symbolic graph (Model의 형태)를 Build한다. 이 함수가 뒤에서 설명할 TowerTrainer에서 tower function의 일부분이다. 그 다음 설명할 inputs()에서 정의된 input list에 맞는 tf.Tensor를 parameter로 받는다. 아무것도 리턴하지 않는다. 1.2. inputs() Model에서 input으로 받을 텐서들의 placeholder들을 정의하는 함수다. 후에 InputDesc로 변환될, tf.placeholder들을 return 한다. 1.3. get_inputs_desc 이름에서 알 수 있듯이, inputs()에서 정의된 모양대로 생긴 InputDesc를 list로 반환하는 함수다. 2. Class ModelDesc 주의사항: build_graph()를 꼭 cost를 return하도록 코딩해야 한다. 앞에서 설명한 ModelDescBase를 상속받은 터라, 위의 3가지는 함수는 내장하고 있다. 2.1. optimizer() tf.train.Optimizer를 여기에 선언해주고 Return하게끔 함수를 짜준다. 2.2. get_optimizer() optimizer()를 호출하면, 계속 새로 optimizer를 만들어서 생성하는데, 이 함수를 쓰면 이미 optimizer()를 통해 생긴 optimizer를 기록해 놓았다가 반환시켜준다. 3. Class TowerTrainerTensorpack에서는, 우리가 흔히 말하는 Model을 계속 Tower라고 지칭한다.(왜 그런지 모르겠다.😶) 그래서 아래에서 나오는 TowerTrainer는 만든 모델을 학습을 시키는 Trainer고, 그 트레이너가 어떤 특징들을 가진 함수들을 들고 다니는지 이해하면 이해가 쉽다. 기본적으로 Tensorpack에 나오는 모든 Trainer들은 TowerTrainer의 subclass다. 이 개념이 그래서 궁극적으로는 모든 neural-network training을 가능하게 해준다. 3.1. get_predictor(input_names, output_names, device=0) Returns a callable predictor built under TowerContext(is_training=False). 이 함수가 호출되면, 가지고 있는 TowerContext(모델)가 training mode가 아닌 상태(is_training=False)로 돌려준다. 그러니까 Test data로 시험할 때만 부르는 함수. 그래서 이름도 predictor. Parameters: input_names (list), output_names (list), device (int) – build the predictor on device ‘/gpu:{device}’ or use -1 for ‘/cpu:0’. 파라미터로 들어가는 input, output이름은 모델 안에서 선언된 이름이 아니면 안 돌아가니까 조심. 3.2. inputs_desc Returns – list[InputDesc]: metainfo about the inputs to the tower. 말 그대로, 모델에 들어갈 Input의 크기와 같은 정보가 들어있는 list를 반환해준다. 3.3. tower_func Build Model. 이 친구가 실제 모델을 정의하고, Build할 수 있는 함수를 세팅하는 부분! ModelDesc interface로 정의된 model을 trainer로 돌려야 하는 상황이 자주 발생할 수 있는데, 이 때, ModelDesc에서 선언된 build_graph 함수가 이 역할을 대신해 줄 수 있다. 3.4. towers Returns – a TowerTensorHandles object, to access the tower handles by either indices or names. 모델 및 Train 전반에 걸쳐 관련된 변수들에 접근하고 싶을 때 사용한다! 그래서 이 함수는 Transfer learning을 할 때 유용할 거 같다. 이미 모델 그래프가 Set up이 끝난 뒤에만 이 함수는 호출될 수 있다. 각각의 layer와 attributes에 이 towers함수를 호출하면 접근할 수 있게 된다! 아래는 예시. 12# Access the conv1/output tensor in the first training towertrainer.towers.training()[0].get_tensor('conv1/output') 4. Class Trainer Base class for a trainer. 분명히 위에서 금방, “기본적으로 Tensorpack에 나오는 모든 Trainer들은 TowerTrainer의 subclass다 “ 라고 했는데, 이 TowerTrainer가 상속을 받는 class가 있었으니, 이름하여 TowerTrainer보다 더 단순한 Trainer 다. 다른 TowerTrainer를 상속 받은 Trainer들을 사용할 때, 종종 TowerTrainer에서 본 적 없는 친구들이 나타나는데, 그 친구들이 Trainer의 것인 경우가 있다. 하지만, Trainer 고유 함수나 요소에 직접적으로 접근할 일이 별로 없어서 아래의 3가지 정도만 알고 있으면 될 것 같다. 나머지는 문서를 참고하자. 아래 1, 2번의 max_epoch과, steps_per_epoch은 TrainConfig에서 자주 만나는 키워드들인데, 이 친구들이 Trainer의 요소였다. 4.1. max_epoch Epoch은 몇 번 돌릴 것인지 4.2. steps_per_epoch 한 에폭당 steps은 총 몇 번인지. 4.3. register_callback(cb) Register callbacks to the trainer. It can only be called before Trainer.train(). Trainer가 모델을 돌릴 때마다(epoch이 진행 됨에 따라), 수행하게 될 부가적인 기능들을 Tensorpack에서는 callback이라고 부르고, 대표적인 callback으로 ModelSaver() 가 있다. 이 Callback을 명시적으로 전달하여 Trainer Object에 세팅할 수 있는 기능이다. 주로 모델을 튜닝할 때, 설정하면서 종종 쓰는 것을 코드 상에서 확인할 수 있다. 5. TowerContext TowerContext 는 Training과 Validation 혹은 Test시에 동작이 달라야 하는 BatchNorm이나, Dropout을 제어하기 위해서 만들어진 function이다. tensorpack_tutorial.ipynb에서는 이 친구를 찾아볼 수 없는데, SimpleTrainer 소스코드를 보니, 자체적으로 안에서 train/test time에 맞춰서 TrainTowerContext라는 것으로 조절하고 있기 때문이었다. 사용법은 간단하다. 1234567# trainingwith TowerContext('', is_training=True): # call any tensorpack layer# testwith TowerContext('name or empty', is_training=False): # build the graph again 그래서, 내가 세운 모델을 외부에서 사용하고 싶을 때, 즉, 나만의 Trainer를 새로 정의해서 train/test time때, 다르게 동작을 해야하는 상황이라면, TowerContext를 적절히 써서 분기시켜줘야 한다. 아래는 Tensorpack Github에서 제공하는 GANTrainer에서 실제로 TowerContext를 어떻게 설정해주는지 보여주는 예시다. 1234567891011class GANTrainer(TowerTrainer): def __init__(self, input, model): .. ... # Build the graph self.tower_func = TowerFuncWrapper(model.build_graph, inputs_desc) with TowerContext('', is_training=True): self.tower_func(*input.get_input_tensors()) opt = model.get_optimizer() ... Thank you :) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2018/08/18/Tensorpack-tutorial/"},{"title":"Pseudo Labelling","text":"준지도학습인 Pseudo Labeling에 대해 알아봅니다. Semi-Supervised Learning, Pseudo Labeling Pseudo-Label: The Simple and Efficient Semi-Supervised Learning Method for Deep Neural NetworksPeter Cha이 포스팅은 SHUBHAM JAIN의 글을 레퍼런스로 사용하였습니다. 이번 포스팅에서 살펴 볼, Pseudo Label은 Semi-supervised Learning의 여러 방법 중 한 가지 입니다. 먼저, Semi Supervised Learning이 무엇인지 살펴보기로 하겠습니다. 1. Semi-Supervised Learning란? 우리는 보통 labelled data (supervised learning)와 unlabelled data(unsupervised learning) 양쪽 모두를 사용하는 방식의 학습을 Semi-Supervised Learning(이하 SSL)라고 정의합니다. 그러면, 어떠한 상황에 SSL이 필요할까요? 보통 다음과 같은 두 가지 상황이 있습니다. 만들고자 하는 Model에 쓸, Training data가 절대적으로 부족할 때. Large dataset이 될 수록 새로 생성되는 data들에 대한 Human annotation이 힘들고, 비쌀 때. 그래서, 보통은 ‘데이터가 부족할 때 쓰는 방법‘으로만 알고 있지만, SSL을 아래와 같은 이유로 사용할 수도 있습니다. Labeled data만으로는 도달 할 수 있는 성능에 한계가 있을 때, Unlabelled data를 사용하여 전반적인 성능을 더 높이기 위해. 논문에서 말하고 있는 Semi Supervised Learning의 목적과, 이 글 후반부에 나올 필자의 Pseudo Label실험도 이 세 번째 이유에 대한 것입니다. Image from Dataiku hadoop summit. 2. Unlabeled data는 어떻게 도움이 될까? 그렇다면 Unlabeled data를 쓴다는 것은 어떤 이점이 있을까요? Labelled data 는 비싸고 얻기 힘들지만 unlabelled는 그 양이 풍부하고 값이 싸기 때문에 데이터 획득이 용이하다. Unlabeled data는 Model의 Decision boundary를 더 정확하게 잡아주는 역할을 해줌으로써, 모델의 Robustness를 향상시킨다. 두 번째 장점을 처음 읽으면 조금 추상적으로 다가올 수 있습니다. 그림으로 조금 더 설명하자면 아래와 같습니다. 단순히 2가지의 Class를 구별해야하는 모델의 경우, Labeled data만 가지고 그 Decision Boundary를 결정하게 되면 선형으로 그 Decision Boundary가 그어진다고 생각해 봅시다. 가지고 있는 Labeled Data에서 경계라고 판단할 만한 정보가 저것밖에 없기 때문에 가지고 있는 Data로 학습시킬 수 있는 모델의 한계라고도 생각할 수 있습니다. 다른 Unlabeled Data를 사용하면서 학습을 하면, 경계를 그을 때, 더 많은 Case들을 고려하면서 정교하게 경계를 긋기 시작합니다. 이는 자연스럽게, 나중에 모델이 Test set을 만났을 때, 혹은 처음보는 다른 Data를 만났을 때도, ‘당황하지 않고 대응할 수 있는’ 힘을 가지게 해준다고 이해할 수도 있습니다. 그래서 두 번째 장점에서 말하고 있는 모델의 Robustness(견고함)는 이를 뜻합니다. 우리가 잘 알고 있는 Overfitting도 이 Robustness의 정도가 낮아서 발생하는 것이라고 볼 수 있습니다. Images from A Tutorial on Graph based Semi-Supervised Learning Algorithms for NLP. 3. Pseudo Labeling을 소개합니다 :) Pseudo Labelling의 개념은 아주 간단합니다. Labeled Data처럼 일일히 label을 하기보다, 이미 가지고 있는 Labeled data에 기반하여 대략적인 Label을 주는 것을 Pseudo Labelling이라고 합니다. 그래서 Pseudo Labeling의 순서는 다음과 같습니다. Labeled Data로 Model을 먼저 학습시킨다. 그렇게 학습된 모델을 사용하여, Unlabeled Data를 예측하고 그 결과를 Label로 사용하는 Pseudo-labeled data를 만든다. Pseudo-labeled data와 Labeled data를 모두 사용하여 다시 그 모델을 학습시킨다. Images from Data, what now?3.1. Pseudo Labeled data Pseudo Label은 아래와 같은 식으로, 각각의 sample에 대해, 예측된 확률이 가장 높은 것으로 정합니다. 예측된 확률이 가장 높은 것을 Label로 선정한다고 했을 때, 제대로 학습을 마치지 못한 모델로 이 작업을 하였을 경우에는 상식적으로 더 학습을 저해하는 데이터를 만들 뿐입니다. 그래서 Pseudo Label은 학습을 Labeled data로 일정 수준까지 마친 뒤의, fine-tuning phase에 시행합니다. 3.2. Loss Function for Pseudo Labelling 논문에서 Pseudo Label로 학습을 할 때는, 다음과 같은 Loss function을 사용합니다. 적절한 수치의 $\\alpha(t)$가 네트워크 성능에 있어서 매우 중요한 요소입니다. $\\alpha(t)$가 너무 높으면 labeled data의 training을 방해할 것이고, 반대로 너무 그 수치가 작으면 unlabeled data의 유익을 취할 수 없게 되겠죠. 그래서 아래와 같이 점진적으로 그 비율을 늘려주는 식으로 $\\alpha(t)$를 조절하여서 local minima에 빠지는 문제를 점차 피할 수 있도록하여, Process를 최적화시킵니다. 4. Pseudo-Label로 성능향상이 왜 가능한거죠? 논문에서는 이 Pseudo Label이 왜 잘 동작하는지에 대해 아래와 같이 설명합니다. 4.1. Low-Density Separation between Classes. Cluster Assumption (Chapelle et al., 2005)에 의하면, Model의 전반적인 성능을 높이기 위해서는 Model의 Decision boundary는 Low-densidy regions에 위치해야한다고 말하고 있습니다. 즉, 앞에서 살펴본 Decision Boundary를 결정할 때, 그 경계를 구분하는 지점의 데이터가 몰려있는 밀도가 낮으면 낮을수록, 더 미세한 차이점도 구별한다고 생각할 수 있기 때문에, 전체적인 성능을 높일 수 있다고 말하고 있습니다. 그런 점에서, Pseudo Label이 아닌 다른 SSL들인, Semi-Supervised Embedding (Weston et al., 2008)이나, Manifold Tangent Classifier (Rifai et al., 2011b)도 같은 목적을 달성한 방식이라고 소개하고 있습니다. Pseudo Label도 마찬가지로 Low-Density Separation 효과를 가져오는 방법이라는 것이죠. 4.2. Entropy Regularization Entropy Regularization (Grandvalet, Yoshua Bengio, 2006)은 Baysian에서 말하는 Maximum posteriori estimation(or Maximum a posteriori, MAP) 관점에서 Unlabelled data의 이점을 얻는 수단입니다. 이 방식은 Unlabeled data가 가지는 class별 확률에 대한 Entropy를 최소화시킴으로써, 위에서 언급한 Class들 간의 Low-Density separation을 추구합니다. 아래에 나오는 MAP식들과 함께 더 자세히 설명하도록 하겠습니다. 위와 같은 MAP식에서 $\\sum\\limits_{m = 1}^{n}\\text{log}P(y^{m}|x^{m};\\theta)$를 첫번째 항, $-\\lambda H(y|x^{‘};\\theta)$를 두 번째 항이라고 지칭할 때, 첫 번째 항인 labeled data의 log-likelihood을 최대화시키면서, 두 번째 항인 unlabeled data의 entropy를 최소화시키기 때문에, 우리는 좀 더 좋은 성능을 얻을 수 있게 됩니다. 두번째 항에서 최소화 시킨다는 Entropy는 Class간의 그 경계치가 Overlap이 되는 정도를 뜻하는데요, Class Overlap이 작아질수록, data들의 밀집된 부분이 더 낮은 decision boundary를 가지게 됩니다. 이것이 위에서 설명한, class별 확률에 대한 Entropy를 최소화시킴으로써, 위에서 언급한 Class들 간의 Low-Density separation을 추구합니다. 의 의미입니다. 따라서, 위에서 설명 하였던 Pseudo-Label의 Loss function에서 나오는 $\\alpha$는 Entropy Regularization의 $\\lambda$와 같은 역할을 하고 있다는 것을 관찰하실 수 있습니다. 결론적으로 이 논문에서 말하고 있는 바는, 우리가 취한 Loss function은 Entropy Regularzation과 동일하다! 그래서, Pseudo-Label을 Entropy Regularization으로 Training하는 것이 효과가 있다. 로 정리할 수 있습니다. 4.3. 논문 실험 결과 논문의 저자는 MNIST test data로 t-SNE (Van der Maaten et al., 2008) 2-D embedding results를 첨부하여 그 성능을 보여주고 있습니다. train data는 600개 밖에 쓰지 않았고, 60000개의 unlabeled data를 사용했다고 하네요. Pseudo-Label (이하 PL)을 쓰지 않은 DropNN 모델과, 거기에 PL을 쓴 +PL 모델의 Conditional Entropy를 비교해 보면, Train에서는 +PL이 확실히 그 Entropy가 더 높게 나타나지만, Unlabeled data나, Test set에서 나오는 Entropy는 월등히 낮음을 볼 수 있었습니다. 시각적으로도 그 구분하는 경계 즉, Decision boundary가 어떤 모델이 더 섬세하게 작용하고 있는지(=더 확실히 구별하고 있는지) 확인할 수 있습니다. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2018/08/22/pseudo-label/"},{"title":"Running a pretrained model on Android with TPU (2)","text":"Google TPU 사용해서 Transfer Learning하여 모바일에 심기 - (2) Tensorflow object detection API 설치하기 Pre-trained Model 선택하기 본 Tutorial은 2020년 6월 기준으로 작성되었으며, Mac OS 기준으로 작성되어 Ubuntu 및 다른 Linux기반 OS에서는 검증되지 않았습니다. 잘못 기재되어 있거나 수정이 필요한 부분들은 알려주시면 감사하겠습니다 :) 본격적인 Transfer Learning 시작 이전 글 에서 TPU를 사용하기 위한 처절한 환경세팅이라쓰고 몸부림을 끝냈습니다. 이번 포스팅에서는,1) Tensorflow 1.14 Bazel로 install.2) Tensorflow Oject Detection API를 설치.3) Transfer Learning을 할 모델을 선택하기 까지 진행해보겠습니다. 1. Install Tensorflow with Bazel Tensorflow 공식 홈페이지에서 제공하는 튜토리얼의 내용을 한글로만 옮겼다. 공식 홈페이지 내용은 여기에서 확인할 수 있다. 공식홈페이지 ‘소스에서 빌드’ 튜토리얼 가장 아래를 확인하면, 가장 중요한! 우리가 설치를 진행할 OS별로, 테스트 결과 작동이 확인된 Tensorflow 버전과 Bazel 버전 쌍을 아래와 같은 표로 확인할 수 있다. Tensorflow를 2.0 버전 이상으로 할 경우 bazel로 학습이 끝난 모델을 경량화를 하는데 아직은 충돌되는 코드가 꽤 있는 것 같아, 이번 튜토리얼에서는 Tensorflow 1.14.0에 맞춰 Bazel 0.24.1로 설치하여 진행한다. 참고로, 공식 홈페이지에서는 Docker Container를 사용해서 설치하는 방법도 볼 수 있다. 1.1. Xcode install Mac OS의 경우, 먼저는 Xcode가 설치되어 있어야 한다. Xcode가 설치되어 있지 않다면, 먼저 여기에서 설치를 진행하도록 하자. 1.2. Bazel install 그 다음 아래와 같이 Terminal에서 명령어를 순차적으로 실행시켜 Bazel을 설치한다. 1234export BAZEL_VERSION=\"0.24.1\"curl -LO https://github.com/bazelbuild/bazel/releases/download/{$BAZEL_VERSION}/bazel-{$BAZEL_VERSION}-installer-darwin-x86_64.shchmod +x bazel-$BAZEL_VERSION-installer-darwin-x86_64.sh./bazel-$BAZEL_VERSION-installer-darwin-x86_64.sh --user 설치가 잘 끝났다면, $bazel version을 터미널에 입력하면, Build label: 0.24.1이라는 output을 확인할 수 있다. 1.3. Tensorflow install Tensorflow 1.14.0으로 아래와 같이 설치하자. 참고로, google-cloud-sdk 폴더 안에 Tensorflow를 설치하면 안된다. google-cloud-sdk를 벗어난 다른 장소에서 Tensorflow를 설치하도록 하자. 1234git clone https://github.com/tensorflow/tensorflow.gitcd tensorflowgit checkout v1.14.0./configure ./configure을 실행하면, 파이썬의 위치와 파이썬 패키지 위치 등을 각자의 환경에 맞게 설정하는 것과 더불어 여러가지 옵션을 설정할 수 있는데, 필자의 경우 XLA JIT support에만 y를 하고 나머지 다른 옵션들에는 모두 n을 하였다. 1.4. pip package build 이제 아래와 같이 Tensorflow를 bazel을 사용해서 tensorflow를 pip package build를 해주자. 이 부분에서 상당한 시간이 걸린다. (필자의 경우 5시간 이상) 123bazel build //tensorflow/tools/pip_package:build_pip_package# Package Build./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg 끝이 났다면, root에 위치한 .bash_profile에 export PATH=\"$PATH:$HOME/bin\"를 추가 해주자. 1.5. pip package install 공들여 build한 pip package로 이제 tensorflow를 설치해보자. 1pip install /tmp/tensorflow_pkg/tensorflow-1.14.0-cp37-cp37m-macosx_10_15_x86_64.whl 설치가 잘 되었는지 Tensorflow를 import해보고 version을 확인해보자. 1python -c \"import tensorflow as tf; print(tf.__version__)\" 2. Tensorflow Object Detection API 설치하기2.1. insatll Tensorflow Object Detection API 1편에서 설치한 google-cloud-sdk를 설치한 경로와 같은 곳에, TensorFlow의 Models를 Clone한다. 우리는 Tensorflow 2를 사용하지 않기 때문에, 브랜치도 Tensorflow 1.14.0과 호환될 수 있는 r1.13.0로 옮겨주자. 123git clone https://github.com/tensorflow/models.gitcd modelsgit checkout remotes/origin/r1.13.0 그 뒤에는 models/research/object_detection/g3doc/으로 이동해서 installation.md 을 열어본다. 위에서 tensorflow는 bazel을 통해서 설치했기 때문에, pip install tensorflow부분은 건너 뛰고, installation.md 에서 설명하고 있는대로 Dependencies, COCO API, Protobuf, Protobuf Compilation, PYTHONPATH 등록까지 순서대로 설치 및 실행한다. 절대설명하기귀찮아서가아닙..읍읍👀 이 때, 주의할 점은 COCO API를 설치할 때, coco api build, install 명령어가 누락되어 있으니 꼭 신경써서 설치를 해야 한다는 점이다. 아마 실수로 빼먹은 듯하다. 123456git clone https://github.com/cocodataset/cocoapi.gitcd cocoapi/PythonAPImakepython setup.py build # this part is ommited.python setup.py install # this one, too.cp -r pycocotools /models/research/ 잘 설치가 됐는지, 아래 명령어를 models/research로 이동해서 실행해보자. 1python object_detection/builders/model_builder_test.py Future warnings를 뒤로하고, 아래와 같은 결과를 볼 수 있다면 Tensorflow Object Detection API를 성공적으로 설치완료.👏👏 12345678910111213141516171819202122232425262728293031323334353637383940...Running tests under Python 3.7.4: /Users/chayesol/opt/anaconda3/bin/python3[ RUN ] ModelBuilderTest.test_create_experimental_model[ OK ] ModelBuilderTest.test_create_experimental_model[ RUN ] ModelBuilderTest.test_create_faster_rcnn_model_from_config_with_example_miner[ OK ] ModelBuilderTest.test_create_faster_rcnn_model_from_config_with_example_miner[ RUN ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul[ OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul[ RUN ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul[ OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul[ RUN ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul[ OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul[ RUN ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul[ OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul[ RUN ] ModelBuilderTest.test_create_rfcn_model_from_config[ OK ] ModelBuilderTest.test_create_rfcn_model_from_config[ RUN ] ModelBuilderTest.test_create_ssd_fpn_model_from_config[ OK ] ModelBuilderTest.test_create_ssd_fpn_model_from_config[ RUN ] ModelBuilderTest.test_create_ssd_models_from_config[ OK ] ModelBuilderTest.test_create_ssd_models_from_config[ RUN ] ModelBuilderTest.test_invalid_faster_rcnn_batchnorm_update[ OK ] ModelBuilderTest.test_invalid_faster_rcnn_batchnorm_update[ RUN ] ModelBuilderTest.test_invalid_first_stage_nms_iou_threshold[ OK ] ModelBuilderTest.test_invalid_first_stage_nms_iou_threshold[ RUN ] ModelBuilderTest.test_invalid_model_config_proto[ OK ] ModelBuilderTest.test_invalid_model_config_proto[ RUN ] ModelBuilderTest.test_invalid_second_stage_batch_size[ OK ] ModelBuilderTest.test_invalid_second_stage_batch_size[ RUN ] ModelBuilderTest.test_session[ SKIPPED ] ModelBuilderTest.test_session[ RUN ] ModelBuilderTest.test_unknown_faster_rcnn_feature_extractor[ OK ] ModelBuilderTest.test_unknown_faster_rcnn_feature_extractor[ RUN ] ModelBuilderTest.test_unknown_meta_architecture[ OK ] ModelBuilderTest.test_unknown_meta_architecture[ RUN ] ModelBuilderTest.test_unknown_ssd_feature_extractor[ OK ] ModelBuilderTest.test_unknown_ssd_feature_extractor----------------------------------------------------------------------Ran 17 tests in 0.243sOK (skipped=1) 2.2. object_detection_tutorial.ipynb models/research/objection_detection/ 폴더 안에 보면, object_detection_tutorial.ipynb 라는 파일을 발견할 수 있을 것이다. 이 파일을 jupyter notebook으로 실행시키면, Tensorflow Object Detection API를 사용해 볼 수 있다. 쭉 실행하면, 아래와 같이 귀여운 강아지와 해변 Test Image들에 대한 Tensorflow object detection API 예시를 확인할 수 있다. ✔ Selecting a model models/research/object_detection/g3doc 폴더에 가면, detection_model_zoo.md 라는 파일을 우리는 확인할 수 있다. 이 파일에서 우리는 COCO dataset로 pre-train된 모델들의 목록을 아래와 같이 확인할 수 있다. ⭐표시가 있는 것들이 바로, TPU로 학습이 가능한 모델들이다. 각자가 학습하기 원하는 모델을 클릭하면, 해당 pre-trained model의 학습된 graph 및 config 파일을 다운받을 수 있다. 다운 받은 그래프 파일과 config 파일을 각자가 학습하고자 하는 데이터에 맞게 어떻게 사용할 수 있는지는 이후Posting에서 살펴보도록 하겠다. 필자는 맨처음 ssd_mobilenet_v1_fpn_coco를 선택했다가, 후에 경량화 과정에서 fpn은 quantization을 지원하지 않는다는..미리 말을 해주셔야 할 것 아닙니까.. 것을 알게 되었다. 독자 분들은, fpn을 피해서 모델을 선택하길! 다음 포스팅에서는 각자가 가지고 있는 Annotation File을 어떻게 모델이 학습할 수 있는 파일인 tfrecord파일로 만들 수 있는지 살펴 보도록 하겠습니다! 😀 이 글은 DataCrew 에서도 보실 수 있습니다. References Training and serving a realtime mobile object detector in 30 minutes with Cloud TPUs - Link GCP Project Setting - Link Step by Step TensorFlow Object Detection API Tutorial - Part 1 ~ 5 - Link document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/01/31/tensorflow_object_detection_with_tpu_2/"},{"title":"Basic Deep learning 01","text":"Deep Learning 개념 및 용어들을 알아봅니다. Optimizer, Loss function, Back propagation Peter ChaIntro Deep Learning을 사용해서 우리가 하고자 하는 일련의 과정은 결국, 우리가 만든 AI(model)가 특정 데이터를 얼마나 잘, 데이터를 구별(classification), 혹은 감지(detection)할 수 있게 할 것인가? 하는 것입니다. 물론, GAN과 같은 Unsupervised Learning에서는 말이 달라 질 수 있지만, 설명을 위해 편의상 그렇다고 생각해봅시다. AI를 학습시키고자 하는 데이터만 주면, model 스스로 ‘아, A는 이렇게 생겼구나, 이렇게 생기면 B라고 하구나’하고 그 데이터가 가지고 있는 특징(feature)을 스스로 깨우치기 원하는 거죠. 그렇게 잘 학습이 잘 되면, 한 번도 본 적은 없지만 여태껏 봐왔던 특징을 가지고 있는 새로운 이미지를 봤을 때, ‘아, 이건 A야.’혹은, ‘B야’하고 맞출 수 있게 되는 것이구요. 더 쉽게 이야기 해보죠. 우리는 강아지와 고양이가 어떻게 생겼는지 우리 model에게 알려주고, 처음보는 강아지나 고양이를 봐도 그 것이 강아지인지, 고양이인지 잘 구별할 수 있었으면 좋겠습니다. 아래 그림처럼 강아지와, 고양이 그림을 엄청나게 많이 주고 우리는 우리가 만든 Model에게 ‘강아지는 이렇게 생긴거야’, ‘고양이는 이렇게 생겼단다.’하고 알려줍니다. 우리는 이 과정을 학습, 혹은 training - learning이라고 부릅니다. 그렇게 잘 학습된 모델은, 훈련할 때 본 적은 없지만, 처음 본 강아지 사진(test)을 봐도 ‘얘는 강아지네요. 고양이는 아니에요’라고 말할 수 있게 됩니다. (Image from KDnuggets) The purpose of deep learning is all about the question, “How can we let our AI classify or detect the certain image or something?” Of course, there are exceptions like GAN, unspervised learning area, let’s simplify the concept for the explanation. What we want to do is to create the model knowing the characteristics or feature of a particular class of data, so it can answer which class the data in. Let’s talk more easily. We want to tell our model how puppies or cats look like and wish it can distinguish whether it is a puppy or a cat when it sees another puppy or another cat for the first time. As shown in the picture above, we give a lot of puppies and cats pictures to our model, and we teach the model, ‘puppy looks like this’, and ‘cat looks like this.’ This is called, learning or training. Then, as the process progresses, the model can distinguish the cat from the dog. That’s all what we are going to talk about. In this post, Deep Learning을 이해하고, 직접 Deep Learning을 구현하고자 했을 때 필요한 기본 개념들을 정리해 보았습니다. 학습이란 무엇을 의미하는지, Optimizer는 어떤 역할을 하는 것인지, Loss function은 무엇인지, 그리고 마지막으로 Back propagation은 어떻게 진행되는지, 간단한 예시를 통해 알아보도록 하겠습니다. :) In this post, I will talk about the basic concepts of deep learning. Let’s start to learn what Learning means, Optimizer does, Loss function is, and How the Back propagation works via a simple example. Learning 특정한 값을 예측을 하고 싶다고 하면, 우리는 먼저 실제로 그러한 예측을 할 수 있는 Model이 필요합니다. 그리고, 그 모델이 예측한 값(prediction)과 실제 값(grounth truth or answer)과의 값의 차이를 loss라고 말합니다. 예를 들어, 간단한 선형 모델인 y = wx을 우리가 모델로 가지고 있다고 합니다. y는 실제 정답이고 x는 input값 입니다. 이 때, w를 우리는 weight라고 부릅니다. 보통 맨 처음엔 이 w를 랜덤하게 고릅니다. w가 매우 적절하게 잘 정해져서 실제 정답인 y와 wx가 똑같은 값이 되었다면 $loss$는 0이 되겠죠! 그래서 Input인 x를 주면 정답인 y를 잘 맞추려면, 당연히 우리는 이 w를 잘 맞출 필요가 있습니다. 근데 위에서 말한 것 처럼 w를 랜덤하게 시작해서는 곤란하죠. 한 번 만에 잘 맞춘다는 건 힘듭니다. 그래서 우리는 학습을 진행을 함에 따라, 우리는 무엇이 됐을지 모르는 이 w값을 반복적으로 update를 시켜서, $loss$를 최소화 할 수 있게끔 만듭니다. 그래서 학습이라는 것은 loss를 최소화 시키는 w 찾기! 라고 할 수 있습니다. When we predict some values, firstly we need a model that can actually do predict, and we call the difference with a prediction and an actual value, ground truch, loss. For example, if we have a linear model y = wx, y is the answer, and x is input. Then, we call the w weight. If the weight is set very properly, then the loss will be 0! We often choose the inital value of w randomly. As the training proceeds, we repeatedly update this w so that we can find minimizes the loss. Therefore, Learning is finding w that minimizes the loss! Optimizer Weight update 자,우리는 $loss$를 최소화시키는 $w$를 알고 싶습니다. 그럴 때, $w$가 값에따라, 그림과 같이 $loss$와 $w$의 값으로 그래프를 그렸다고 했을 때, U자로 형성됐다고 생각하고, $loss$를 최소화하는 $w$의 값으로 $w$를 update하고 싶습니다. 시작점은 편의상, 랜덤하게 정해졌다고 합시다. w값을 update시키기 위해서, 우리는 다음과 같은 공식을 씁니다. 여기서 $\\alpha$가 뜻하는 것은 learning rate라고 하는 것인데요, 보통 0.001같은 아주 작은 값이고, 그래서 다음 학습할 때 쓸 w는 지금 w와 얼마만큼 떨어져있는지정도를 의미합니다. $w = w - \\alpha \\cdot \\frac {\\partial loss}{\\partial w}$ 이 w값은 미분을 하면 구할 수 있는데요, 미분의 의미는 결국 아주 작은 구간에서의 순간변화율이라고 우리가 알고 있는 만큼, 이는 미분은 곧, 기울기의 정도를 표현한다고 할 수 있죠. 이 $w$값을 구하기 위한 미분 방법은 아래에 나오는 Back Propagation을 소개하면서 다시 이야기 하도록 하겠습니다. 작은 예시로, Back Propagation이 ‘내가 지금 알고 있는 지식을 가지고 대학교 졸업 후의 나로 돌아 갈 수 있다면, 훨씬 더 좋은 선택과 결정을 하면서 살 수 있을 것이다.’ 같은 겁니다. 여기서, 근데 이 learning rate을 너무 크게 설정해줘서, 필요이상으로 군 입대 하루 전으로 돌아간다면 끔찍하겠죠? 그 과거로 ‘적절히’ 돌아가야 지금 가지고 있는 정보를 십분 활용할 수 있기 때문에, 이 learning rate라는 수치가 중요합니다. Let me think we want to know the value of weigh which minimizes the loss. If we draw a graph consists of loss and w, let us consider it looks like a bowl like the image above. Then, we want to update the weight to the point which becomes the value minimizing the loss. Of course, the starting point is randomly selected. To update the w value, we use follwing equation. alpha means learning rate which is usually very small number like 0.001, so it means that How far the next step w is from where now w is. $w = w - \\alpha \\cdot \\frac {\\partial loss}{\\partial w}$ As we know, the meaning of derivative is Instantaneous rate of change, inclination. Let me introduce the way how to calculate the derivative of w, later on the Back propagation part. SGD 미분을 이용하여, 만약 미분 결과값이 -인 경우, w는 좀더 양수쪽으로 가게 되고, 반대로는 음수로 가는 방식으로 우리는 w를 update할 수 있습니다. 이런 update 방법을 Stochastic Gradient Descent 최적화 - 한국어로는 경사하강법 -, 또는 줄여서 SGD라고 부릅니다. SGD 이외에 최적화 기법으로 Adam, Adamx 등등 다양한 기법들이 있습니다만, 클래식한 이런 경사하강법의 방식의 다른 방식이라고 이해해도 크게 틀리지 않습니다. By using derivative, We can update the w in this way: if the drivative value(=gradient) is minus, then w will be move toward the posivie side, and visa versa. This kind of update approach is called Stochastic Gradient Descent Optimization, or SGD for short. There are other more various different Optimizers like Adam, Adamax and so on, but you might think that those are different to classical stochastic gradient descent. Loss function 자, 그러면 위에서 설명한 loss라는 것을 계산하기 위해서는 어떤 방법을 사용할까요? Loss function을 설명하기 위해서, 쉬운 loss function 하나를 예시로 들어봅시다. MSE는 모델의 loss를 산출하는 방법 중 하나입니다. MSE는 Mean Squared Error의 준 말로, 문자 그대로 아래에 보이는 수식 - 예측한 값에서 실제 값을 빼고 그 차이를 제곱하여 평균을 내는 방식 - 으로 모델의 loss를 계산합니다. Here, what should we do to get the loss mentioned before? To explain loss function, let’s take an easy loss function as an example. MSE is an one of ways to measure the loss of a model. The Acronym for the Mean Square Error which is following equation. y hat is a prediction of our model, and y is a real value. So, it means simply the sum of differences between forecasts and actual values. Binary class에 대한 loss를 구해줘야 할 때는, MSE 보다는 BCEloss를 더 잘 씁니다. 이렇게 더 다양한 loss function들이 있습니다. There are various other loss functions like BCEloss for binary loss, and so on. Back Propagation 역전파라고도 하는, Back Progagation은 loss를 weight로 미분한 값을 계산하는 방법입니다. 예를 들어, 우리의 모델이 선형회기식인, Linear model이라고 하고, 우리의 loss function이 MSE라고 합시다. 그러면 우리 모델이 loss를 구할 때 거쳐가게 될 공식은, $loss = (\\hat y - y)^2$ 이기 때문에, 즉 $(x*w - y)^2$이 될테고, 이 식은 아래의 그림과 같이 도식화 할 수 있습니다. Back propagation is the way to calculate the derivate value of loss by w. For example, our model is a linear model, and we use MSE as a loss function. Then, the gates of our model will look like following. x = 1, y = 2, 그리고 w = 1이라고 합시다. 그러면 loss를 구하는 forward path는 명백합니다. Let’s assume that x = 1, y = 2, and w = 1. Then, the forward path is obvious. Derivate Computation Chain Rule에 의해서, 우리는 차근 차근 w의 미분값을 계산해 나갈 수 있습니다. $loss\\text{ } function$ 공식에서 사용하는 operator 하나를, 하나의 gate라고 생각할 때, 각 gate마다 input으로 들어오게 되는 그 값이 최종 $loss$ 가 산출되는데 얼마만큼이나 기여를 하나. 하는 정도가 곧 우리가 미분을 하는 이유입니다. 그래서 결국은 그렇게 w가 $loss\\text{ } function$에서 input으로 들어가게 될 때의 미분값을 구하면, 그 것은 즉, w가 loss를 구하는데 얼마나 영향을 미치는가(= 기울기)라는 의미가 됩니다. :D Back Propagation은 가장 우측의 gate와 함께 loss로부터 시작합니다. By using Chain Rule, we can calculate the derivative value of w, step by step. Let us consider the each operator in the loss function is a gate, then, we are going to calculate how much this input of each gate contributes to the loss. That’s the reason why we do the derivative calculation. So, at last, we can get the derivative of w as an input of a gate, it means the amount of contribution of w to the final loss value. :D The back propagation starts from the loss with rightmost local gate. $x^2$ gate $loss$인 1은 s인 -1을 제곱해서 나온 값이니까요, $loss = s^2$ 으로 생각할 수 있습니다. loss를 제곱 gate로 미분한 다는 의미의 $\\frac{\\partial loss} {\\partial s}$라는 식은 곧, $\\frac{\\partial s^2} {\\partial s}$라는 식과 같다고 생각할 수 있습니다. $\\frac{\\partial loss} {\\partial s}$ = $\\frac{\\partial s^2} {\\partial s}$ $s^2$을 $s$로 미분한거죠! 그러면 $\\frac{\\partial s^2} {\\partial s} = 2s$ 이기 때문에, 우리가 알고 있는 s = -1를 대입하면, $x^2$gate의 local gradient는 -2가 됩니다. $loss$ is 1, and s is -1, so, the local derative of square gate is -2. $\\frac{\\partial loss} {\\partial s}$ = $\\frac{\\partial s^2} {\\partial s} = 2s$ Again, s is -1. Therefore, the local gradient of - gate is -2. $-$ gate $x^2$gate에서 -2가 - gate에 $loss$로 들어왔습니다. 그리고 그 - gate의 계산결과는 s인, -1 이었구요. 자, 이제 Chain Rule을 사용해서 - gate의 local gradient를 구해볼까요? 우리는, Chain Rule에 의해서, - gate의 local gradient를 아래와 같은 식으로 표현할 수 있습니다. $\\frac{\\partial loss} {\\partial \\hat y} = \\frac{\\partial loss}{\\partial s}\\frac {\\partial s}{ \\partial \\hat y} \\Rightarrow -2\\cdot\\frac {\\partial \\hat y - \\partial y}{\\partial \\hat y} = -2\\cdot1 = -2$ $\\frac{\\partial loss}{\\partial s}$은 -2라는 것을 $x^2$ gate에서 이미 알고 있기때문에, 나머지, s를, $\\hat y$로 미분한 결과만 계산해서 곱하면 끝납니다. s는 $\\hat y - y$ 라는 식의 결과나 마찬가지었으니, 치환해서 생각하면 편하구요. 그래서 결과는 -gate에서도 여전히 local gradient는 -2가 되군요! -2 is passed to the - gate as loss. In the - gate, $y$ is a constant value and $\\hat y$ is 1, so the derivative is -2. We already know that $\\frac{\\partial loss}{\\partial s}$ = -2, so, the thing we need to do is to calculate the $\\frac {\\partial \\hat y - \\partial y}{\\partial \\hat y}$.$\\frac{\\partial loss} {\\partial \\hat y} = \\frac{\\partial loss}{\\partial s}\\frac {\\partial s}{ \\partial \\hat y} \\Rightarrow -2\\cdot\\frac {\\partial \\hat y - \\partial y}{\\partial \\hat y} = -2\\cdot1 = -2$ $*$ gate 자 이제 마지막으로, w가 input으로 들어간 * gate의 local gradient를 계산하고 w의 gradient를 계산하는 과정을 끝냅시다. -gate에서 -2가 $loss$로 넘어왔고, 또 다시, 우리는 위와 같은 방법으로 Chain Rule을 쓰면, $\\frac {\\partial loss}{\\partial w} = \\frac{\\partial loss}{\\partial \\hat y}\\frac{\\partial \\hat y}{\\partial w}$ 로, 표현할 수 있고, 또 우린 이미 $\\frac{\\partial loss}{\\partial \\hat y}$ = -2 라는 것을 알고 있습니다. 그래서 여기서 $\\hat y$를 의미하는 식인 $wx$를 $w$로 미분한 값만 알면되는데, 그 값은 $x$이므로 그냥 x였던, 1를 넣어주면, $loss$에 대한 w의 미분값이 -2라는 결과를 얻습니다. 이런 식으로 우리는 w의 미분값을 계산해 나가면서, $loss$가 최소가 되는 w를 찾습니다. Let’s finish this process with calculating the local gradient of the * gate. As we know, -2 is given from the - gate, so, as the same way, using the Chain Rule again, we can write the equation like following.$\\frac {\\partial loss}{\\partial w} = \\frac{\\partial loss}{\\partial \\hat y}\\frac{\\partial \\hat y}{\\partial w}$ We already know the value of $\\frac{\\partial loss}{\\partial \\hat y}$ is -2, so, we just put the value of derivative of $wx$ with $w$, 1. Then, at last, we can know the derivative value of w is -2. In this way, we can find the weight point where the loss becomes minimum. Update weight 그렇게 계산이 끝난 W의 loss를 어떻게 update해줄까요? 아래 보시는 것처럼 단순히 $w = w - learning{:} rate * w.loss$ 로 Update를 하게 됩니다. 아래 예시에서는 learning rate로 0.01을 줬네요. 이것으로 Back Propagation에 대한 설명을 마치도록 하겠습니다. Then, how to update calculated weight’s loss? Simply we can calculate simply like this. $w = w - learning{:} rate * w.loss$ . In the example above, it used 0.01 as a learning rate. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2018/02/06/Basic-Deep learning-01/"},{"title":"Docker tutorial (2)","text":"로컬 저장소 컨테이너로 마운트하기 내 컨테이너 환경 이미지로 Upload하기 안녕하세요 :) Docker Tutorial, 두 번째✌입니다! 컨테이너를 만들고 작업을 열심히 했는데, 그 컨테이너가 실수로 지워진다면, 그 컨테이너 안에 있던 모든 자료들이 삭제되었다는 것을 의미합니다 동공지진👀 그런 일을 미연에 방지하기 위해서, 저장소를 컨테이너 외부에 두는 것이 안전하죠! 그래서, 첫 번째로 생성한 Docker 컨테이너에 내 로컬 폴더 연동하기를 함께 알아보겠습니다. 또 내가 쓰던 개발환경을 떠나, 새로운 곳으로 갔을때이직 일일히 환경설정을 그 때 환경으로 다 할 필요 없이 우리는 도커를 쓸 수 있습니다. 두 번째로 내가 작업하고 있는 컨테이너의 설정을 Image로 Docker Hub or Registry에 업로드해서 다른 곳에서도 쓸 수 있도록 하는 방법을 알아보겠습니다! Local Directory Mount (run -v) 내가 가지고 있는 Host OS있는 폴더를 생성한 컨테이너와 공유하려면 run 명령어를 실행할 때 -v 옵션을 사용하면 됩니다! 포트 설정을 할 때와 같이 :을 기준으로 좌측에는 마운트하고자 하는 폴더경로를, 우측에는 마운트시킬 컨테이너 경로를 적어주면 됩니다.123$ docker run -it \\ -v /path/to/folder:/root/work \\ image_ID_or_name /bin/bash Example 제 컴퓨터 바탕화면에 Data라는 폴더에는 CNN 학습을 위한 이미지들이 담겨있는 train이라는 폴더가 있습니다. 이 폴더를 이전 포스팅에서 다운 받았던 이미지를 사용한 컨테이너와 연동시켜보도록 하겠습니다. 먼저 가지고 있는 images의 ID를 확인해보니, a8928a6d6eaa이네요. 12REPOSITORY TAG IMAGE ID CREATED SIZEcivisanalytics/civis-jupyter-python3 latest a8928a6d6eaa 8 days ago 3.37GB 해당 이미지로 컨테이너를 생성하면서 마운트를 해보면, 123$ docker run -it -p 8888:8888 \\ -v /Users/chayesol/Desktop/Data:/root/work \\ a8928a6d6eaa /bin/bash 실제로 1.jpg, 2.jpg가 들어있는 train 폴더가 /root/work/train로 잘 마운트 된 것을 확인할 수 있었습니다. 1234567[work] # ls -a. .. train[work] # cd train [train] # ls -a. .. 1.jpg 2.jpg[train] # pwd/root/work/train 이 폴더는 실제로 공유되고 있는 폴더이기 때문에, 이 폴더 안에 파일을 지우면 당연히 컨테이너 상에서도 지워집니다. (Vise Versa) Docker는 로컬 뿐만아니라 AWS S3 Bucket과 연동시킬 수도 있습니다. 시간나면..쿨럭..😷다시 포스팅하겠습니다.. 내 컨테이너 환경 Upload하기!Layer? 컨테이너는 이미지 기반으로 생성되기 때문에, 내 컨테이너 환경을 업로드한다는 것은, 내 컨테이너 환경의 이미지를 만든다고 생각할 수 있습니다. Docker는 이미지를 만들 때, 최종 이미지를 만들기 위한 명령어들을 한땀한땀 실행합니다. 이태리장인 즉, 최종 이미지를 만들기 시작한 맨처음 기반이 되어주는 이미지로 컨테이너를 만든 다음, 그 컨테이너에서 그 다음 부품(명령어)을 가져다가 설치해서 다시 이미지를 만들고, 또 그 다음 부품, 그 다음, 그 다음.. 식이라는 거죠. 이렇게 한 겹, 한 겹 싸이는 구조 때문에, 그 부품이 되는 명령어 하나하나를 layer라고 부릅니다. 업로드 할 이미지 만들기(Dockerfile) Docker는 이미지 파일을 Dockerfile 이라는 파일을 보고 생성합니다. Dockerfile을 작성하기 위해서는 Dockerfile 명령어들을 사용해서 내 환경에 맞게 잘 적어줘야합니다. 실습을 위해, 여태까지는 Tensorflow만 사용했지만, Pytorch도 사용하고 싶은 사용자가 있다고 가정합시다. 그렇다면 다음과 같은 시나리오로 이미지를 만들 수 있습니다. 1231. Jupyter notebook과 Tensorflow가 설치되어 있는 이미지를 Base Image로 사용2. Pytorch를 추가로 설치 (layer 추가)3. 컨테이너 생성시, Jupyter notebook 자동 실행하도록 세팅. 이 시나리오에 해당하는 Dockerfile을 먼저 보고, 하나씩 설명해보도록 하겠습니다. Dockerfile 우리가 만들 Image의 Dockerfile은 다음과 같습니다. 원하시는 경로에서 다음과 같은 Dockerfile을 만들어 주세요. (파일 이름을 “Dockerfile”로) 123456789101112131415161718# Base ImageFROM civisanalytics/civis-jupyter-python3LABEL maintainer=\"petercha90@gmail.com\"# Pytorch 설치RUN apt-get -y -qq update && \\ conda install -y pytorch-cpu torchvision-cpu -c pytorch# Data 폴더 복사COPY ./Data /root/work/Data/# 명령어를 실행할 디렉토리 설정WORKDIR /root/work# Jupyter notebook 가동EXPOSE 8888 22CMD jupyter notebook --NotebookApp.token='' \\ --ip=0.0.0.0 --port=8888 --allow-root FROM: 1FROM civisanalytics/civis-jupyter-python3:1.11.0 ‘이미지의 Base를 어디로부터 가져오겠느냐’는 말입니다. 저는 Jupyter notebook과 Tensorflow가 설치되어 있는 이미지를 Base Image로 사용하려 이전 포스팅에서 사용한 이미지를 Base로 삼고 있습니다. :앞에는 이미지 이름이, 뒤에는 버전정보라고 볼 수 있는 Tag 를 적어줍니다. LABEL: 1LABEL maintainer=\"petercha90@gmail.com\" 보통 MAINTANER라는 명령어를 쓰지만, 곧 ‘will be deprecated’라고 하여, 자체적으로 추천하는 LABEL을 사용해 봤습니다. LABEL의 key값으로 maintainer를 주고, value로 제 e-mail을 써서, 이 이미지의 관리자가 누구인지 밝히고 있습니다. - 추가 정보기 때문에 Build하는데 영향을 주지는 않습니다. RUN: 12RUN apt-get -y -qq update && \\ conda install -y pytorch-cpu torchvision-cpu -c pytorch 가장 많이, 자주 쓰는 명령어입니다. RUN 다음 적히는 명령어를 그대로 실행하게 해줍니다. 먼저 ubuntu 다운을 위해 apt-get update를 해줍니다. -y는 update 도중 생기는 yes/no를 묻는 질문에 막혀서 멈추지 않도록 미리 모두 yes를 주기 위함이고, -qq는 설치 내역 등의 log 출력하지 않도록하는 quiet 옵션입니다. 그 다음, Pytorch를 설치하는 명령어를 실행하도록 하고 있습니다. COPY: 1COPY ./Data /root/work/Data/ COPY 다음에 나오는 첫번째 경로에 있는 파일을 두번째로 적힌 컨테이너 상의 경로로 복사합니다. 저는 바탕화면에 있는 Data 폴더(현재 Dockerfile이 있는 경로가 바탕화면이라서 ./Data)를 /root/work/Data/로 복사하게 했습니다. 두 번째 경로에 해당하는 디렉토리가 없다면 자동 생성합니다. 실제로 /root/work/에는 Data라는 이름의 폴더가 없지만 이번 이미지 Build과정을 통해 생성하게 됩니다. WORKDIR: 1WORKDIR /root/work RUN, COPY, CMD 등의 명령어들을 실행할 경로를 지정해줍니다. 접속했을 때, 맨 처음 위치하게 되는 경로이기도 합니다. 그 다음 명령어 상에서 경로 이동이 생기더라도, 그 다음 명령어에서 자동으로 여기 적은 경로로 위치가 초기화됩니다. EXPOSE: 1EXPOSE 8888 22 컨테이너 실행시, 요청을 기다리는 port를 미리 열어줍니다. 여러개를 설정 할 수도 있습니다. 저는 그 다음 포스팅에서 나올 개념인 ssh접속을 위해서 22번 port도 열어주기로 했습니다. CMD: 12CMD jupyter notebook --NotebookApp.token='' \\ --ip=0.0.0.0 --port=8888 --allow-root 컨테이너가 실제로 실행되었을 때, CMD에 적힌 명령어들을 실행합니다. 위에서 말했듯이, 컨테이너 생성시 Jupyter notebook을 자동 실행하도록 세팅하기 위해 미리 CMD로 해당 명령어를 적어놓았습니다. 그 외에도 자주 쓰는 명령어로 ADD, VOLUME, ENV 등이 있습니다. 더 많은 정보는 공식문서를 참고해주세요. ### Build! 이미지를 생성하는 명령어 build와 이름을 설정해주는 -t(tag)옵션을 사용한 아래 명령어로 Dockerfile을 실행 시키면, 1$ docker build -t docker101 . 시간이 꽤 걸립니다..⏳ 123456...Removing intermediate container a39920cdab45 ---> 547c429ef2eeSuccessfully built 547c429ef2eeSuccessfully tagged docker101:latest$ 라는 메세지가 나오면 성공적으로 Image를 생성한 것입니다! docker images로 생성한 이미지를 확인해봅니다. 와우… 3.37GB Base-image에 Pytorch를 설치해서 4.71GB SIZE가 돼버린 뚠뚠이🐷 docker101 Image가 보이네요! 1234$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEdocker101 latest 547c429ef2ee 13 minutes ago 4.71GBcivisanalytics/civis-jupyter-python3 latest a8928a6d6eaa 10 days ago 3.37GB 그렇다면…! 떨리는 맘으로 Jupyter notebook을 자동 실행되도록 설정한 것까지 잘 되는지 확인해보겠습니다! 팝콘각🍟 1$ docker run --name first_one -d -p 8888:8888 docker101 역시 localhost:8888로 접속을하니 바로 Jupyter notebook으로 접속이 되네요! 의도한 CMD 세팅이 잘 되어 있다는 것을 확인할 수 있습니다. 😎 맨처음 COPY로 복사했던 Data 폴더가 먼저 환영해 주네요 :) 그 안에 학습용 데이터들도 잘 복사가 되어있는 것을 확인할 수 있습니다. 새 Python notebook을 만들어 Pytorch까지 잘 설치되어있는지 확인해 보면..맙소사… Pytorch와 Tensorflow, 둘 다 잘 작동하는군요! 성공적입니다!🎂🎉🎊🎅🎁 Upload 잘만든 이미지를 공유하는 방법으로 아래와 같은 두 가지 방법이 있습니다. Docker Registry 를 설치하여 업로드한다. (like Git) Docker Hub 에 업로드 한다. (like Github) Docker Registry Docker Registry는 git처럼 설치형으로, 만든 이미지를 Registry 서버로 push하고, 다른 서버에서 pull을 받아서 사용합니다. Docker Hub Docker Hub는 도커에서 제공하는 기본 이미지 저장소로, 회원가입만하면, 대용량의 이미지도 무료로 업로드하고 다운로드 받을 수 있습니다. 마치 Github처럼 웹상에서 다운받을 수도 있고, docker 명령어로 이미지를 업로드 하고, 다운 받을 수 있는 공간입니다. 이번 포스팅에서는 Docker Hub를 사용하여 업로는 하는 방법만 다뤄보도록 하겠습니다! Docker Hub Uploading Docker Hub에 만든 이미지를 올리는 것은 3가지 단계만 거치면 끝입니다. 1. Login 2. Renaming 3 Push. Login (login) 먼저 Docker Hub 홈페이지에서 회원가입을 하시고 아래 명령어를 입력하면 Docker Hub 계정에 로그인 할 수 있습니다. 12345$ docker loginLogin with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one.Username: petercha90Password:Login Succeeded Renaming (tag) Docker Hub에 올릴 이미지는, [User_ID]/[Image_name]:[tag]* 라는 규칙으로 이름을 구성합니다. tag 명령어를 사용하여 금방 만들었던 docker101 이미지의 이름을 규칙에 맞게 바꿔주면서, 버전 정보(1.0)도 입력해보겠습니다. 1$ docker tag docker101 petercha90/peter-tensorflow-pytorch:1.0 docker images로 확인해보니, docker101과 용량도 같은데 이름이 petercha90/peter-tensorflow-pytorch이고, TAG가 1.0인 이미지를 확인할 수 있었습니다. 1234 $ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEdocker101 latest 34bbb1c093da 20 minutes ago 4.71GBpetercha90/peter-tensorflow-pytorch 1.0 34bbb1c093da 20 minutes ago 4.71GB Push (push) 이제 push명령어로 Docker Hub에 업로드만 하면 됩니다! 1$ docker push petercha90/peter-tensorflow-pytorch:1.0 또 시간이 좀 걸립니다.. …… ⏳ 1234567891011121314151617181920The push refers to repository [docker.io/petercha90/peter-tensorflow-pytorch]fdd4a938bd06: Pushed be825b2f1c27: Pushed a3442dbff3f9: Mounted from civisanalytics/civis-jupyter-python3 734b701f4670: Mounted from civisanalytics/civis-jupyter-python3 5d22569a8a20: Mounted from civisanalytics/civis-jupyter-python3 99645d10b61c: Mounted from civisanalytics/civis-jupyter-python3 43c65e275431: Mounted from civisanalytics/civis-jupyter-python3 5efecd288488: Mounted from civisanalytics/civis-jupyter-python3 ef945229e9cd: Mounted from civisanalytics/civis-jupyter-python3 dc2c8974046d: Mounted from civisanalytics/civis-jupyter-python3 2a83ea7d2735: Mounted from civisanalytics/civis-jupyter-python3 70191a7dc716: Mounted from civisanalytics/civis-jupyter-python3 c3a15ba40d5e: Mounted from civisanalytics/civis-jupyter-python3 603a1f4a3e0c: Mounted from civisanalytics/civis-jupyter-python3 b57c79f4a9f3: Mounted from civisanalytics/civis-jupyter-python3 d60e01b37e74: Mounted from civisanalytics/civis-jupyter-python3 e45cfbc98a50: Mounted from civisanalytics/civis-jupyter-python3 762d8e1a6054: Mounted from civisanalytics/civis-jupyter-python3 1.0: digest: sha256:ffc16d44d34f063a0856999b0f04fc71f322e4d1d47e026ffed6ad2ab89f6a81 size: 4094 🎉Ta-da!!🎉 Upload가 완료되었습니다! 이전 포스트에서 이미지를 Pull하면서 시작한 것 기억나시나요? 그 것처럼 이제 어디서든 인터넷만 잘 된다면, Anaconda3-Tensorflow-Pytorch이 설치되어 있고 + Jupyter notebook 자동실행이 되는! 보안은어쩔 이미지를 다운받아 사용할 수 있습니다! :sunglasses: 여기까지 이전 포스팅과 함께 Docker에 대해 아주 러프하게 함께 알아 보았습니다. 더 구체적이고 설정 방법들과 Docker를 이용한 다양한 서비스 개발에 대해서는 Reference를 참고하시면 되겠습니다! References 초보를 위한 도커 안내서 - 설치하고 컨테이너 실행하기 초보를 위한 도커 안내서 - 이미지 만들고 배포하기 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2019/05/09/docker-102/"},{"title":"Running a pretrained model on Android with TPU (4)","text":"Google TPU 사용해서 Transfer Learning하여 모바일에 심기 - (4) Google Cloud TPU 학습하기 본 Tutorial은 2020년 6월 기준으로 작성되었으며, Mac OS 기준으로 작성되어 Ubuntu 및 다른 Linux기반 OS에서는 검증되지 않았습니다. 잘못 기재되어 있거나 수정이 필요한 부분들은 알려주시면 감사하겠습니다 :) 이번 포스팅에는, 1) Google Cloud Storage 세팅2) pipeline.config 세팅3) TPU 학습 총 세 가지 과정을 통해 Transfer learning을 함께 진행해보겠습니다. 1. Google Cloud Storage 💾 자, 그럼 이제부터 본격적인 Transfer Learning을 시작해보자. 먼저는 학습을 돌리기 위한 pretrained model과 앞시간에서 생성한 tfrecord 데이터를 Google Cloud Storage bucket에 준비하는 것이다. 필자는 이전 포스팅에서 함께 살펴 본 TPU로 학습할 수 있는 pretrained 모델 중, ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_1 를 다운 받았다. 다운받은 파일을 살펴보면, 내용물은 총 4개다. Model Checkpoint파일 3개. model.ckpt.data-00000-of-00001, model.ckpt.index, model.ckpt.meta. 그리고 구체적인 학습 디테일을 설정할 수 있는, pipeline.config. 이 데이터들을 이제 Storage로 옮겨주자. Bucket 구성하기 생성한 버킷 안에 train, data 폴더를 만들어 주고, train 에는 Model Checkpoint파일 3개. model.ckpt.data-00000-of-00001, model.ckpt.index, model.ckpt.meta를, data폴더 에는 앞 시간에 만든 tfrecord 파일들과 pipeline.config를 넣어주자. 마지막으로 label map 데이터를 추가해야한다. 우리가 Tranfer Learning 하고자 하는 대상은 pascal VOC dataset + 포크레인 = 21가지 클래스다. models/research/object_detection/data/안을 보면, pascal_label_map.pbtxt파일을 찾을 수 있는데, 학습하고자 하는 label마다의 번호와 이름을 명시해 놓은 파일이다. 여기에 우리가 추가로 학습하고자 하는 클래스에 해당하는 가지 수만큼 뒤에 추가해주자. 필자는 아래와 같이 포크레인을 21번째 클래스로 등록해주었다. 12345678910111213141516item { id: 1 name: 'aeroplane'}...... item { id: 20 name: 'tvmonitor'}item { # 포크레인 추가 id: 21 name: 'excavator'} 이렇게 수정이 끝난 label_map 데이터도 data 폴더 안에 넣어주자. 2. pipeline.config 세팅 ⛏ 이렇게 받은 모델이 pretrained 되어있는 대상은 무려 90가지의 class를 구별하는 COCO dataset이다. 그럼 다시 학습을 할 때는 우리가 원하는 데이터 셋의 Class 가지 수만큼 구별할 수 있게 하려면 어떻게 해야할까? 실제 날코딩 레벨에서는 제일 마지막 Fully Conneted layer를 날리고, 원하는 클래스 만큼 구별할 수 있게 세팅한 FC layer를 추가해줘야 하겠지만, pipelin.config의 숫자 하나만 바꿔주면 우리는 이 작업을 쉽게 할 수 있다. 필자는 VOC dataset의 20가지 + 포크레인을 더해서 총 21가지 Class를 구별할 수 있게 세팅하였다. 123456789model { ssd { num_classes: 21 # 90 -> 21! image_resizer { fixed_shape_resizer { height: 300 width: 300 } } 자, 그리고 하단부에는 실제로 학습을 시작하면서 학습데이터는 어디서 가져와야하는지 등을 알려주는 부분이 있는데, 아래와 같이 각자의 bucket 위치로 잘 바꿔서 수정해주자. 대략적으로 수정해줘야 하는 부분은 아래와 같다. 12345678910111213141516171819202122232425262728... fine_tune_checkpoint: \"gs://{YOUR_BUCKET_NAME}/train/model.ckpt\" num_steps: 50000 startup_delay_steps: 0.0 replicas_to_aggregate: 8 max_number_of_boxes: 100 unpad_groundtruth_tensors: false}train_input_reader { label_map_path: \"gs://{YOUR_BUCKET_NAME}/data/label_map.pbtxt\" tf_record_input_reader { input_path: \"gs://{YOUR_BUCKET_NAME}/data/train.record\" }}eval_config { num_examples: {NUM_OF_YOUR_VALIDATION_DATA} metrics_set: \"coco_detection_metrics\" use_moving_averages: false}eval_input_reader { label_map_path: \"gs://{YOUR_BUCKET_NAME}/data/label_map.pbtxt\" shuffle: false num_readers: 1 tf_record_input_reader { input_path: \"gs://{YOUR_BUCKET_NAME}/data/val.record\" }}... 이미 눈치챘겠지만, 사용자는 이 pipeline.config를 통해 batch size와 step size 등등을 조절해서 학습의 디테일을 바꿀 수 있다. 2. Train the model! 🏃 자! 이제 드디어 학습을 시작 하기 전에.. 마지막으로 다시 models/research로 이동하여, 아래 명령어를 실행하자. 학습에 사용될 패키지를 압축하는 과정이다. 123$ bash object_detection/dataset_tools/create_pycocotools_package.sh /tmp/pycocotools$ python setup.py sdist$ (cd slim && python setup.py sdist) # must execute this line, too. Training 드디어! 진짜로! 학습을 시작해보자. 아래 명령어를 각자의 버킷이름에 맞춰서 수정 후 입력. 1234567891011$ gcloud ai-platform jobs submit training `whoami`_object_detection_`date +%s` \\--job-dir=gs://{YOUR_BUCKET_NAME}/train \\--packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz,/tmp/pycocotools/pycocotools-2.0.tar.gz \\--module-name object_detection.model_tpu_main \\--runtime-version 1.15 \\--scale-tier BASIC_TPU \\--region us-central1 \\-- \\--model_dir=gs://{YOUR_BUCKET_NAME}/train \\--tpu_zone us-central1 \\--pipeline_config_path=gs://{YOUR_BUCKET_NAME}/data/pipeline.config 잘 실행이 되었다면, 학습 진행상황 log를 볼 수 있는 링크를 터미널에서 자동으로 알려주는데, 그 링크를 타고 들어가면 아래와 같이 GCP Console이나, Terminal에서 log를 관찰할 수 있다. Evaluation 학습된 모델의 성능을 평가하려면, 아래의 명령어로 할 수 있다. 자동으로 train에 저장되어 있는 가장 학습이 많이된 모델을 자동으로 선택해서 Evaluation을 한다. 1234567891011$ gcloud ai-platform jobs submit training `whoami`_object_detection_eval_validation_`date +%s` \\--job-dir=gs://ddp/train \\--packages dist/object_detection-0.1.tar.gz,slim/dist/slim-0.1.tar.gz,/tmp/pycocotools/pycocotools-2.0.tar.gz \\--module-name object_detection.model_main \\--runtime-version 1.15 \\--scale-tier BASIC_TPU \\--region us-central1 \\-- \\--model_dir=gs://ddp/train \\--pipeline_config_path=gs://ddp/data/pipeline.config \\--checkpoint_dir=gs://ddp/train Evaluation이 잘 끝나면, 아래와 같은 평가표를 받아볼 수 있다. 1234567891011121314151617181920212223...Saving dict for global step 50000: DetectionBoxes_Precision/mAP = 0.57654905, DetectionBoxes_Precision/mAP (large) = 0.7432713, DetectionBoxes_Precision/mAP (medium) = 0.2553529, DetectionBoxes_Precision/mAP (small) = 0.044851754, DetectionBoxes_Precision/mAP@.50IOU = 0.8106554, DetectionBoxes_Precision/mAP@.75IOU = 0.6466306, DetectionBoxes_Recall/AR@1 = 0.4816028, DetectionBoxes_Recall/AR@10 = 0.63472164, DetectionBoxes_Recall/AR@100 = 0.6432252, DetectionBoxes_Recall/AR@100 (large) = 0.7970598, DetectionBoxes_Recall/AR@100 (medium) = 0.39382252,DetectionBoxes_Recall/AR@100 (small) = 0.10790066, Loss/classification_loss = 0.15321705, Loss/localization_loss = 0.10001469, Loss/regularization_loss = 0.12372323, Loss/total_loss = 0.37696198, global_step = 50000, learning_rate = 0.15243073, loss = 0.37696198 Retreive the model to your machine 학습이 성공적으로 끝났다면 아래의 명령어를 통해서, local machine으로 학습이 끝난 모델을 다운받을 수 있다! 먼저, 받아올 모델과 다운받을 곳을 설정하고, 123$ export CONFIG_FILE=gs://${YOUR_GCS_BUCKET}/data/pipeline.config$ export CHECKPOINT_PATH=gs://${YOUR_GCS_BUCKET}/train/model.ckpt-{NUM_STEP_SIZE}$ export OUTPUT_DIR=~/Desktop/tflite 그 다음, 학습된 모델 그래프를 추출해서 받아오자. 12345$ python object_detection/export_tflite_ssd_graph.py \\--pipeline_config_path=$CONFIG_FILE \\--trained_checkpoint_prefix=$CHECKPOINT_PATH \\--output_directory=$OUTPUT_DIR \\--add_postprocessing_op=true 잘 끝났다면, 학습이 끝난 tflite_graph.pb, 과 tflite_graph.pbtxt 두 파일을 다운 받기로 설정한 폴더 안에서 만날 수 있다! 이 파일을 이제 경량화해서 tflite파일로 만들고, 그 경령화된 모델을 android app에 심는 대망의 마지막 작업을 .. 다음 포스팅에서..! 아디다스⭐ P.S. Make bucket public 실제 학습할 시, bucket의 데이터에 접근할 수 없다는 permission error로 인해 고통받는 것이 싫다면, 학습의 편의를 위해 bucket을 아래와 같이 객체 access 방식을 균일한 액세스 제어로 설정하고, 버킷 권한 설정을 통해 전체공개로 바꾸는 것을 추천. Bucket을 전체 공개하는 방법은 다음과 같다. 1) 구성원 추가를 눌러서 나오는 대상이름에 allUsers 혹은 allAuthenticatedUsers를 타이핑해서 추가 한다. 2) 권한으로 아래와 같이, 저장소 기존 개체 소유자, 저장소 기존 버킷 소유자로 선택해서 추가해준다. 수고하셨습니다. :) 다음 시간에는, 학습이 끝난 모델을 경량화하고, Tensorflow에서 제공해주는 Android example 코드에 탑재하는 것으로 튜토리얼을 마무리 짓도록 하겠습니다. 이 글은 DataCrew 에서도 보실 수 있습니다. References Training and serving a realtime mobile object detector in 30 minutes with Cloud TPUs - Link GCP Project Setting - Link Step by Step TensorFlow Object Detection API Tutorial - Part 1 ~ 5 - Link document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/06/21/tensorflow_object_detection_with_tpu_4/"},{"title":"Running a pretrained model on Android with TPU (5)","text":"Google TPU 사용해서 Transfer Learning하여 모바일에 심기 - (5) 모델 경량화, Android App에 모델 탑재하기 본 Tutorial은 2020년 6월 기준으로 작성되었으며, Mac OS 기준으로 작성되어 Ubuntu 및 다른 Linux기반 OS에서는 검증되지 않았습니다. 잘못 기재되어 있거나 수정이 필요한 부분들은 알려주시면 감사하겠습니다 :) 이번 포스팅에는, 1) 모델 경량화(Make tflite file via quantization)2) Running our model on Android 두 가지 과정을 통해 튜토리얼을 마무리 짓도록하겠습니다. 1. 모델 경량화 - Quantization. 🍉 저번 시간까지 진행한 내용을 잘 마쳤다면, 현재 학습을 마친 모델 데이터인 tflite_graph.pb 와, tflite_graph.pbtxt를 확인할 수 있다. 그 다음엔Tensorflow로 경로를 이동한 뒤, 아래 명령어를 시켜주면 8bit Quantization을 통해 모델을 아주 작은 용량의 tflite 파일로 변환시켜준다. $OUTPUT_DIR은 저번 시간에 설정해 주었다. 1234567891011$ bazel run -c opt tensorflow/lite/toco:toco -- \\--input_file=$OUTPUT_DIR/tflite_graph.pb \\--output_file=$OUTPUT_DIR/my_detect.tflite \\--input_shapes=1,300,300,3 \\--input_arrays=normalized_input_image_tensor \\--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\--inference_type=QUANTIZED_UINT8 \\--mean_values=128 \\--std_values=128 \\--change_concat_input_ranges=false \\--allow_custom_ops output_arrays 옵션의, ‘TFLite_Detection_PostProcess’, ‘TFLite_Detection_PostProcess:1’,‘TFLite_Detection_PostProcess:2’, ‘TFLite_Detection_PostProcess:3’ 는 각각, 순서대로 아래와 같은 의미다. detection_boxes, detection_classes,detection_scores, num_detection 이외에 다른 옵션들의 의미를 알고 싶다면, 여기 에서 대략적으로 확인할 수 있는데, toco가 아니라 tflite_convert에 대한 설명인 이유는, 구글이 이제는 Tensorflow toco를 업데이트하지 않고 구버전으로 내버려두고, TFLiteConverter 명령어 위주로 API를 가져가는 추세라서 그런 것 같다. 아무튼 경량화가 다 끝이났다면, 학습된 그래프 데이터가 있던 곳에서 우리의 작고 소중한✨ my_detect.tflite 파일을 만나볼 수 있다. 2. Running our model on Android 📱 우리는 이번 튜토리얼에서 우리가 학습한 모델이 잘 작동하는지 확인해 볼 수 있는, 아주 간단한 어플에 모델을 심어볼 것이다. Tensorflow에서는 공식적으로 Tensorflow lite Model을 android, macOS, 그리고 Raspberry Pi에서 사용할 수 있는 예제코드를 제공한다. Godgle..👍 여기 에서 예제코드 전체를 먼저 다운받도록 하자. 2.1. Android Studio 당연하게도 먼저는 안드로이드 스튜디오 를 설치해야 한다. 그리고 안드로이드 OS의 공기계도 준비하자. 이제 고지가 거의 코 앞까지 왔기 때문에 조만간 결과물을 확인해봐야 할테니! (팝콘준비🍿) 우리는 mobilenet 기반의 object detection 모델을 사용하기 때문에, 다운받은 예제코드에서 아래와 같이, lite/examples/object_detection를 차례로 들어가면 android, ios, raspberry_pi 예제 코드들을 만날 수 있다. 이 폴더를 원하는 곳으로 옮겨도 좋고, 그냥 그대로 둬도 좋다. Android Studio를 실행해서, Open an existing Android Studio project를 클릭하고 이 폴더를 선택해서 프로젝트를 열어주자. 2.2. Change the model 그 뒤에, 수정해줘야할 코드는 딱 한 군데다. DetectorActivity.java의 57, 58번째 라인에서 확인해 볼 수 있는, TF_OD_API_MODEL_FILE, TF_OD_API_LABELS_FILE를 자신의 모델, 자신의 labelmap 파일 경로로 수정해주면 끝! 아래 사진과 같이, 다운받은 폴더를 살펴보면 assets폴더 아래에 기존의 detect.tflite와 labelmap.txt를 볼 수 있는데, 같은 곳에 my_detect.tflite, my_labelmap.txt를 넣어주면 된다. 기존에 존재하고 있는 labelmap.txt를 확인해보면 알겠지만, 모델이 학습한 Classes가 적혀있는 파일이다. 각자의 모델이 가지고 있는 클래스들을 적어서 생성하면 된다. 2.3. Install the app into android 이제 어플이 폰에서 잘 작동하는지 확인해봐야 할 차례! 먼저, 가지고 있는 안드로이드 기기에서 개발자 옵션 및 USB 디버깅 사용 설정을 해주자. 그 뒤에 안드로이드를 연결하면, 아래 사진에 노란박스로 표시한 부분에서 Android Studio에서 해당 기기를 자동으로 인식해주는 것을 볼 수 있을 것이다! 연결이 잘 되었다면, 아주 그냥 바로 Run ▶ 버튼을 클릭하여 앱을 안드로이드 기기에 빌드하고 실행할 수 있다. 3. Finish! 🎉 신나게 사용해보자 😎 필자는 포크레인을 잡아내는 MobileNet-SSD Model로 Transfer learning을 했었는데, 아직 작은 이미지는 잘 못잡지만 어느 정도 학습이 잘 된 것 같다! 이 튜토리얼을 끝까지 읽어주셔서 감사합니다. :) 튜토리얼을 쓰다가보니, Tensorflow 2.0 시대를 맞이해 Tensorflow 2.0+와 toco가 아닌 TFLiteConverter를 사용하는 것으로 글로 재정비하는 것이 좋겠다고 생각이 들었습니다.😅 시간이 허락하는 내에서, 다시 글을 재정비해서 업데이트하도록 하겠습니다. 이 글은 DataCrew 에서도 보실 수 있습니다. References Training and serving a realtime mobile object detector in 30 minutes with Cloud TPUs - Link GCP Project Setting - Link Step by Step TensorFlow Object Detection API Tutorial - Part 1 ~ 5 - Link document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/06/28/tensorflow_object_detection_with_tpu_5/"},{"title":"AlexNet","text":"CNN의 강려크함을 세상에 널리 알려주신 조상님. AlexNet 리뷰! ImageNet Large-Scale Visual Recognition Challenge(이하 ILSVRC)-2012에서 Top-5 test 기준, 에러율 15.3%로 1등을 차지하면서 26.2%인 2등과의 격차를 10% 이상 벌리며, CNN의 우수함을 전세계에 입증한, AlexNet Review입니다. 😃 논문 작성 당시 토론토 주립대 학생이었던 Alex Krizhevsky가 1저자로, Geoffrey E. Hinton 교수님이 지도 교수로 참여하신, 'ImageNet Classification with Deep Convolutional Neural Networks’가 AlexNet이 나온 논문의 풀네임입니다. 역시 모델의 이름은 저자의 이름을 따서 지었습니다. Structure 🏗 네, 딥러닝을 공부하시는 분들이라면 정말 안보기가 힘들 정도로 유명한 AlexNet의 구조는 아래의 그림과 같이 총 5개의 Convolution layers와, 3개의 fully-connected layers로 구성되어 있습니다. 위 아래로 filter들이 절반씩 나뉘어서 두 개의 GPU에 할당되고, 세 번째 Convolution Layer를 제외하고는 각자 계산을 마치고 마지막 Classifier에 모여서 결과를 도출하는 방식입니다. 굳이 커널을 절반으로 나눈 이유에 대해서는 아래에서 더 설명하도록 하겠습니다. 모델의 Input 부터, Output까지 구조를, 두 GPU들의 channel size를 합쳐서 대략적으로 이해하기 쉽도록 정리하면 아래와 같습니다. 1. ReLU 😇 저자는 AlexNet의 결과에 기여한 공이 큰 순서대로 비법을 소개합니다. 그 첫 번째가 Rectified Linear Units, ReLU Nonlinearity입니다. 지도 교수님인 Hinton이 널리 알린 Activation Funtion이죠. ReLU는 'saturating nonlinearity’를 가진 sigmoid나 tanh보다 학습의 속도를 여러배 빠르게 해준다고 하면서 Cifar-10 데이터로 실험한 결과를 논문에 첨부합니다. 점선인 tanh로 35 epoch이 지나서야 도달하는 Training error rate에 실선인 ReLU를 사용하면 6 epoch만에 도달한다는 내용입니다. ReLU가 학습속도를 비약적으로 빠르게 해줍니다. 2. Multiple GPUs 💪 논문 작성 당시에 사용했던 GPU는 GTX 580으로 메모리가 3GB에 불과 했습니다. 그래서 2개의 GPU를 사용한 단순한 이유로는 GPU의 메모리는 네트워크가 학습할 수 있는 최대 학습의 양을 제한한다고 언급합니다. 그리고 Multiple GPU를 사용하기에 시도 할 수 있는 네트워크 구조(각각의 GPU가 모든 kernel을 공유하는 것이 아니라 나눠서 각각 다른 feature를 뽑는 구조)를 가지는 것이 실제로 같은 형태의 one-GPU 네트워크에 비해, top-1 error rate와 top-5 error rate를 각각 1.7%, 1.2%씩 낮출 수 있는 비결이라고 합니다. 위의 네트워크 구조에서 보실 수 있듯이, 단순히 Multi GPU라서 두 배로 Kernel을 늘려서 연산을 할 수 있다는 장점 말고도, AlexNet에서는 Layer끼리 별개의 연산을 하다가도, 3번째 Conv layer에서는 또 서로 가지고 있는 kernel들을 교류하기도 합니다. 이런 것들이 (왜 이것이 가능한지 정확히는 모르겠지만) 아래와 같이 각각의 GPU가 서로 역할을 나눠서 학습을 하게 합니다. 위의 보시는 이미지는 224x224x3 input image에 대해 첫 번째 convolution layer를 통과한 뒤 얻을 수 있는 96개의 convolutional kernels입니다. GPU1은 위의 3줄 48개에 해당하는 주로 흑백을 이루는 convolutional kernel들을 학습하고, GPU2는 아래 color feature들이 주로 이루는 나머지 절반 48개의 convolutional kernel들을 학습합니다. 저자는 이런 특별한 현상이 random weight initialization을 어떻게 하든 학습때마다 관찰되었다고 합니다. 3. Local Response Normalization 💫 bx,yib^i_{x, y}bx,yi​는 아래와 같은 Local Response Normalization을 거친 결과를 의미합니다. ax,yia^i_{x,y}ax,yi​가 의미하는 것은 (x, y)좌표의 i번째 kernel의 결과가 ReLU를 통과 한 뒤의 결과값입니다. 분모가 의미하는 것은, 각각 jjj번째 같은 (x, y)위치에서 계산되는 iii와 근접한(adjacent) kernels들을 합하고, 또 그것을 Hyperparameter로 설정하는 k,α,βk, \\alpha, \\betak,α,β로 적절히 계산하여 나눠주겠다는 의미입니다. 저자는 k=2,n=5,α=0.0001,β=0.75k=2, n=5, \\alpha=0.0001, \\beta = 0.75k=2,n=5,α=0.0001,β=0.75로 사용했다고 합니다. N은 해당 layer의 총 kernel 개수입니다. 이렇게 계산에 줌으로써, 각 iii번째 kernel들은 동일한 위치의 다른 주변 kernel들과 평준화를 이룹니다. 이 Normalization의 효과로, neuron들이 output 간의 큰 activity를 일으킬 수 있게 경쟁을 유도할 수 있다고 합니다. 이는 실제 neurons들에게서 발견되는 형태로, ‘lateral inhibition’(뜻 아시는 분은 댓글로 알려주시면 감사…)을 구현하게 된다고 합니다. 저자는 이를 통해 top-1과 top-5 error rate를 각각 1.4%, 1.2% 낮출 수 있었다고 말합니다. 4. Overlapping Pooling 😲 전통적으로 Pooling의 grid size와 stride와 맞춰줌으로써, 겹치는 부분이 없도록 pooling을 수행하지만, AlexNet에서는 stride는 2, grid size는 3으로 세팅하면서 Pooling을 overlapping 시키면서 정보를 조금씩 더 가져가게 설정했습니다. 그 결과 top-1, top-5 error rate를 각각, 0.4%, 0.3% 줄였다고 하고, 이렇게 트레이닝한 모델들은 약간이지만 Overfit이 쉽게 되지 않는 경향을 관찰했다고 저자는 말합니다. Overall Architecture Response-normalization layer는 첫 번째와 두 번째 convolution layer 뒤에 적용 되었고, Max-pooling 또한 그 뒤에 연달아서 수행, 그리고 5 번째 conv layer 뒤에서도 Max pooliing이 수행되었습니다. ReLU는 모든 conv와 fully-connected layer뒤에 적용되었습니다. Reducing Overfitting 🤔 6천만개의 parameter들을 가지고 있는 AlexNet이 1000가지 ILSVRC 데이터에 맞춰 학습된다는 것은 엄청난 overfitting 없이는 학습하기 힘들다는 것을 의미합니다. 그래서 저자는 아래와 같은 두 가지 방법으로 Overfitting에 대항했다고 합니다. 1. Data Augmentation 첫 번째로, 256x256 Image로부터 224x224 Random Crop과 Horizontal flip을 사용해서 데이터를 CPU 상에서 Augment한 뒤에, GPU 연산으로 넘기면서 로컬 머신에 저장하지 않고 Data Augmentation을 구현하였습니다. 두 번째로, RGB channels의 강도를 조절하여 Data Augmentation을 수행했습니다. 아래와 같이 RGB pixel 값들의 3x3 공분산 matrix의 eigenvector와 eigenvalue를 뜻하는 PiP_iPi​, λi\\lambda_iλi​값에 랜덤 값인 α\\alphaα를 곱해서 RGB image pixel에 더해줬습니다. 2. Dropout Dropout은 AlexNet에서 처음 두 fully-connected layer들에 적용되었으며, Dropout을 사용하지 않았을 경우, 심각한 overfitting에 걸렸다고 합니다. 그리고 수렴하는데 필요한 epoch을 두배 가량 낮춰줬다고 합니다. 여러모델의 결과값을 합치는 것이 좋지만, 한 모델을 학습하는데 몇 날, 몇 일이 걸리는 경우 0.5확률의 Dropout이 현실적인 대안이었다고 말합니다. Details of learning Optimizer로 SGD, Batch size는 128, 0.9 momentum에 weight decay로 0.0005를 사용하였습니다. 모든 layer들에 학습초기에 ReLU에 양수 값들을 제공하기 위해서, zero-mean Gaussian 분포로 초기화 합니다. 초기 learning rate는 0.01을 사용하였고, 해당 learning rate에서 validation error가 더이상 나아지지 않을 경우 1/10로 낮추는 방식으로 학습이 끝나기 전까지 총 3번 learning rate를 줄입니다. 총 90 epoch 학습하였고, 총 120만장의 이미지에 대해, 두 대의 NVIDIA GTX 580 3GB GPU로 꼬박 5박 6일Wow이 걸렸다고 합니다. Results ✨ ILSVRC-2010에서 다른 방법들에 비해서 탁월히 높은 성능을 보이는 AlexNet을 볼 수 있습니다. top-1, top-5 error rate를 37.5%, 17.5%를 아래와 같이 달성합니다. 또한 ILSVRC-2012에도 출전해서는 ImageNet 2011 Fall 데이터에 pre-trained된 모델 두 개와 5개의 다른 CNN 모델들의 예측에 평균을 취하는 방법으로 top-5 error rate 15.3%를 기록하며 2등 기록인 26.2%보다 10% 이상 높은 성능으로 1위를 차지합니다. 여기까지, AlexNet 리뷰를 마치도록 하겠습니다. 다음 포스팅에서는 ZFNet 리뷰를 하겠습니다. 읽어주셔서 감사합니다. 🙇 이 글은 DataCrew에서도 보실 수 있습니다. References AlextNet Paper document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/07/09/AlexNet/"},{"title":"LeNet","text":"Convolution의 시작. 딥러닝 조상님 LeNet 리뷰! ’DO NOT REINVENT THE WHEEL.’\\text{'DO NOT REINVENT THE WHEEL.'} ’DO NOT REINVENT THE WHEEL.’ 위의 격언처럼, 역사적으로 어떠한 기존의 한계 때문에 어떤 시도들을 해왔는지, 또 그런 시도들이 현대에 와서는 어떤 것으로 대체 되었는지 아는 것만으로도 시행착오를 줄일 수도 있고 또 다른 발상을 도울 수 있다고 생각합니다. 그런 의미에서 1998년 CNN의 시작을 알린 LeNet을 리뷰하는 것으로 시작하겠습니다. CNN의 작동방식을 잘 아는 분들은 스킵하셔도 좋습니다. LeNet-5 LeNet을 소개하는 논문의 제목은 Gradient-Based Learning Applied to Document Recognition입니다. Yann LeCun이 1저자이고, Yoshua Bengio 교수님도 함께 공동저자로 참여하였습니다. 네트워크의 이름이 LeNet인 것도, Yann LeCun의 이름을 딴 것 같습니다. 1998년 당시, 지금은 많은 분들에게 익숙한 이미지에서 feature를 배운다는 개념과 Gradient-Based Learning, Back-Propagation, 그리고 CNN의 작동 방식 + 연속된 숫자를 어떻게 인식할 것인가에 대한 고민으로 제안하고 있는 방식 등을 모두 설명하다 보니, 해당 논문은 총 페이지만 46페이지에 달할 정도로 많은 양을 가지고 있습니다. 본 포스팅에서는 CNN의 탄생배경과 LeNet-5에 대한 설명을 중심으로 진행합니다. Convolutional Neural Networks 👍 1. Traditional Pattern Recognition 방법론들의 한계 1998년 당시, 전통적인 Pattern Recognition 방법은 어디까지나 Hand-designed Feature extractor라는 한계를 가지고 있고, 그렇게 만들어진 Extractor들이 내는 결과에만 크게 의지하여 들어온 정보 중 상관이 있는 정보와 상관 없는 정보를 분류하고 결론을 낸다고 지적합니다. 그래서 결국엔 가능한한 좋은 학습이라는 것은 feature extractor 자체에서 발생되어야 한다고 말합니다. 그렇기에 데이터 자체 정보를 온전히 사용하여 스스로 그 특징을 학습을 하는 Fully-connected multi-layer networks를 classifier로 사용할 수도 있다는 것을 말합니다. 하지만 이 또한, 아래와 같은 한계가 있다고 설명합니다. 2. Fully-connected multi-layer networks의 한계 첫 번째로는 분류하게 될 2D 이미지 정보라는 것은 실제로는 많은 variables(pixel)로 구성 되어 있기에 이것을 fully-conneted layer로만 학습을 하게 될 경우 너무 많은 학습량, 학습 시간, 그리고 그 많은 weights를 저장해야할 hardware적인 요소를 생각할 때에 비효율적이라고 합니다. 두 번째로, 실제로 위상정보(Topolopy of the input)가 철저히 무시된다는 문제가 있다고 말합니다. 실제 이미지 정보나, Time-frequency representations of speech에서 2D Local feature라는 것은 핵심적인 정보를 쥐고 있기 때문에, 각각이 시공간적으로 주변부 데이터들과 어떠한 관계가 있는지에 대한 정보들을 다 파편으로 만들고, 다 뒤죽박죽으로 쓰겠다고 하는 것 자체가 잘 못됐다고 지적합니다. 3. 그래서 Convolutional Networks. ✌️ Convolution Network는 이미지가 가질 수 있는 어느 정도의 shitf, scale, 그리고 distortion invariance들에 대해 다음과 같은 세가지 특징이 있기 때문에 강하다고 말합니다. Local receptive fields: 우리가 알고있는 Convolution Kernel의 작동방식을 설명하는 부분입니다. Kernel이 전체 이미지를 돌아다니면서 만나게 되는 Kernel Size와 동일한 크기의 영역들을 Local receptive fields라고 지칭합니다. 이렇게 작은 영역들을 돌아다니면, neuron들은 방향성, 꼭지점, 커브 등과 같은 기초적인 visual feature들을 추출할 수 있습니다. 이러한 feature들은 뒤에 이어서 계산되는 레이어에서 더 높은 차원의 feature들을 구성할 수 있도록 합성됩니다. 이것이 가능하기 때문에 전체 이미지에서 distortion이나 shift가 발생하여도 결국엔 각 클래스를 구분짓는 패턴이 특정 local receptive field에서 만날 수만 있다면, 해당 특징을 반영한 feature map을 만들어 낼 수 있습니다 Shared weights: 전체 이미지, 혹은 전체 feature map은 같은 Filter의 weights + bias를 공유하게 되는 것을 뜻합니다. 예를 들어, 5x5 Kernel은 5x5사이즈와 설정된 Stride에 맞춰 전체 이미지(or feature map)를 돌아다니며 계산을 하지만, 5x5=25의 weights와 + 1개의 bias만 back propagation으로 학습을 할 뿐입니다. 이처럼 계산되는 영역마다 학습 파라미터가 느는 것이 아니라, Filter가 총 몇개로 설정하는가에 따라 output인 feature map의 수와 학습해야하는 parameter가 늘 뿐입니다. 이러한 기법으로 계산을 수행할 local machine에게 요구되어지는 총 계산 capacity를 줄여주고, 학습할 parameter의 수를 줄여줌으로써 자연스럽게 Overfitting을 방지하게 되어 test error와 training error 사이의 gap도 줄여줍니다. 실제로 논문에서 이 weight sharing 덕분에, LeNet-5는 총 340,908번의 connetions가 있지만, only 60,000 trainable free parameters만 가진다고 말합니다. Sub-sampling: 현대에 와서 Pooling이라고 말하는 그 개념입니다. 다만 차이점이 있다면, 논문에서는 개념상 지금의 2x2 Average Pooling의 개념을 쓰지만 Layer라고 부를 수 있도록, input feature map의 수에 해당하는 (weight + bias)를 학습가능한 parameter로 가지고 있다는 점입니다. 우리가 이미지를 분류하고자 할 때, 사용하고자 하는 것은 각 Class마다 가지고 있는 패턴이지, 해당 숫자가 어디에 위치하는지를 알고 싶은 것이 아니라고 말하면서, 위치 정보를 반영하는 것은 잠정적으로 해롭다고 표현하고 있습니다. 그래서 Sub-sampling은 위치정보가 encoding되는 위험을 해상도를 낮춤으로써, 위치에 관한 영향도를 간단히 줄일 수 있다고 소개합니다. 결론적으로 shifts and distortion에 대한 민감도도 덜하게 할 수 있게 됩니다. 또한 그렇게 위치정보를 소실시키면서 생기는 손실은, feature map size가 작아질수록 더 많은 filter를 사용하여 다양한 feature들을 뽑게하면서 상호보완할 수 있도록 하는 정책을 취합니다. 하지만, 이 주장은 후에 Hinton 교수님의 CapsuleNet과 같은 논문에서, 위치정보를 상실한 분석이 진정한 이미지 분류를 뜻할 수 없다고 반박을 당하게 됩니다. 실제로 MobileNet과 함께 널리 퍼진 Depthwise-Separable Convolution에서도 pooling을 쓰지 않고 최대한 정보를 다 가져가는 방법을 추구하기도 한다는 것도 기억할만 합니다. Structure 🏗 그 유명한 LeNet-5의 구조를 함께 보시겠습니다. LeNet-5의 구조는, [Conv(C1) - Subsampling(S2) - Conv(C3) -Subsampling(S4) - Conv(C5) - FC - FC] 총 7개의 layers로 구성되어 있습니다. 여기서 FC 두 레이어를 제외하면 5개의 CNN Structure가 되기 때문에 LeNet-5라고 명명하고 있습니다. C5 layer의 Output에 Full connection이라고 적힌 이유는 C5에서 input으로 받는 5x5 feature maps에 5x5 Kernel을 사용해서 feature map size가 모두 1x1이 되었기 때문입니다. 저 같은 경우, 논문에 실려있는 이미지는 Input이 각 레이어를 통과한 뒤 나오는 Feature Map들을 그린 것이라, 각각의 레이어들이 투명한 것처럼 보여 한눈에 구조가 잘 보이지 않았습니다.참 못생기게 그렸죠. 총 7개 Layer들의 세부정보는, 간략한 아래의 Pytorch 코드를 통해 확인하실 수 있습니다. 섬세한 노력들 S2에서 C3로 넘어가면서 filter의 수를 6개에서 16개로 늘립니다. 이 과정에서 지금의 Tensorflow나 Pytorch같은 Framework들이 제공하는 API로는 구현하기 힘든 설계들을 하는데요, 6개의 feature maps를 처음 6개는 연속되게 이웃하는 3개의 feature map들의 조합에서 구하고, 그다음 6개는 연속되게 이웃하는 4개 feature maps의 조합, 그 다음 3개는 sparse하게 남는 4개의 조합, 그리고 마지막 16번째는 6개를 모두 다 반영한 feature를 뽑을 수 있도록 아래와 같이 구성합니다. 또한, F6의 Output이 84인 이유는 마지막으로 분류될 10가지가 아래의 full ASCII set을 해석하기 적합한 형태로 나와주길 바라는 마음에서 그렇게 설정했다고 합니다. 각각의 문자가 7x12 bitmap이기 때문에 84인 것이죠. 실제로 F6는 마지막 분류직전에 공을 많이 들여서, 손수 parameter들을 적절한 값으로 초기화하고, 최소한 학습 초반에는 고정된 수치로 적용될 수 있도록 하는 방법을 취하고 있습니다. 또한 Euclidean Radial Basis Fucntion을 뜻하는 RBF를 사용하면서 더 섬세한 작업을 MSE Loss Function을 사용하기 전에 한 번 더 F6에서 취해주는데, 이와 관련된 부분은 필요이상으로 상세하게 들어가는 것 같아 생략하도록 하겠습니다. Results ✨ MNIST testset Error rate로 LeNet-5와 이전의 다양한 기법들의 성능을 비교한 표는 아래와 같습니다. 아래 그림에서 deslant가 의미하는 것은 기울어진 숫자들을 똑바로 세우는 전처리를 거친 데이터셋을 의미하고, dist는 기존 데이터셋에 distorted 데이터를 더 추가한 데이터셋을 뜻합니다. 28x28-300-10과 28x28-300-100-10과 같은 요소들은 fully-connected layer로만 구성되어있는 Network를 뜻합니다. 28x28-300-10은 One-hidden layer인 fully-connected Neural Network을 의미하여 28x28 input에 hidden unit 300개에 10개 output으로 나오는 모델입니다. 마찬가지로 숫자가 28x28-300-100-10과 같은 요소들은 Two-hidden layer인 fully-connected Network를 의미합니다. 결론적으로는 LeNet-5를 사용하면 1% 미만의 성능을 기본적으로 확보할 수 있다는 것을 말해주고 있습니다. 여기까지, LeNet 리뷰를 마치도록 하겠습니다. 다음 포스팅은 그 다음 조상님이신 AlexNet 리뷰를 하겠습니다. 읽어주셔서 감사합니다. 🙇 이 글은 DataCrew에서도 읽으실 수 있습니다. References LeNet paper document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/07/06/LeNet/"},{"title":"ZFNet","text":"CNN의 속 시원하게 들여다 보고싶다! ZFNet 리뷰! 이번에 소개할 논문은 ‘Visualizing and Understanding Convolutional Networks’입니다. 논문 제목 그대로 CNN의 필터를 시각화함으로써 모델이 학습하고 있는 대상을 확인할 수 있고, 그로 인한 피드백으로 AlexNet의 구조를 좀 더 개량하여 더 좋은 성능을 이끌어 냅니다. 공동저자인 Matthew D. Zeiler와, Rob Fergus의 이름을 따, 모델의 이름은 ‘ZFNet’입니다. 1. Introduction (1) 많은 데이터, (2) Powerful한 GPU의 등장 (3) Dropout과 같은 좋은 regularization 방법들의 등장으로 AlexNet과 같은 모델들이 나올 수 있었습니다. 하지만, 그런 딥러닝 모델이 어째서 그런 좋은 성능을 달성할 수 있었는지, 왜 그것이 가능한지에 대한 명쾌한 이해가 없이는 그저 많은 trial-and-error로만 귀결될 뿐이라는 것을 지적합니다. 그래서 본 논문에서 Deconvolutional Network(deconvnet)을 제안합니다. 이 deconvnet을 통해 AlexNet의 성능을 향상 시킨 모델이 ZFNet입니다. Visualization ZFNet 이전에는 그나마 픽셀차원에서 확인이 가능한, 첫 번째 레이어의 feature map 정도만 시각화 하는 정도에 그쳤으나, Deconvnet을 통해 그 이상의 깊은 layer들의 feature map도 시각화가 가능함을 보입니다. 특히, feature map을 만드는 pattern들이 무엇인지 알려주는 filter를 시각화 한다는 점을 특징으로 꼽습니다. 2. Approach ZFNet은 기본적으로 AlexNet의 구조를 가지고 있습니다. 다만, 다음에서 설명하게 될 이유로 필터사이즈와 같은 부분들이 좀 더 계량이 되어 있습니다. AlexNet과 같이 5개의 Conv layer들과 2개의 fc layer로 이루어져 있지만 AlexNet 처럼 2개의 GPU에 filter를 반씩 학습하는 구조는 사용하지 않습니다. 첫번째 Conv layer를 보시면, 필터사이즈가 AlexNet에서 11x11이었던 것이 7x7로, stride 4가 2로 줄어들었습니다. 이런 변화를 주게 된 이유는 4.1 Architecture Selection에서 다시 설명하겠지만, 이런 변화를 준 레이어들의 필터를 시각화를 해보았더니, 보여주는 패턴이 우둘투둘(aliasing)한 정도가 덜하고, 결론적으로 성능을 향상 시켰기 때문입니다. 2.1. Visualization with a Deconvnet Deconvnet의 개념을 시각화 하면 아래와 같습니다. 최대한 Convolution layer의 행위를 반대로 하려고 노력하고 있습니다. 각 Conv layer 마다, Deconv layer를 붙여서 아래와 같은 작업을 통해 Conv layer를 통과하기 전 Pixel Size로 다시 바꿔줍니다. 일반적으로 Conv layer가, Conv-Retification-Pool 3단계로 구성되어있다고 했을 때, 반대로 Deconv layer는 Unpool-Rectification-Transposed Conv, 총 3단계로 이해하실 수 있습니다. Deconv를 시행하기 전에는 각 Conv layer를 통과하면서 생긴 activation만 살피기 위해서, 해당 Conv layer를 통해 생성되지 않은 다른 activation은 0으로 만든 뒤 Deconv layer의 input으로 넣어 줍니다.(그림에서 Layer Above Reconstruction 부분) 그 뒤에 Unpooling을 수행하고, Relu와 같은 Rectification을 거쳐, Transposed Conv를 수행합니다. 특별히 Unpooling을 할 때는, Pooling을 수행할 때 가장 큰 값의 위치를 기억하고 있는 Switches를 사용해서 다시 제 자리에 값을 복원시켜줍니다. (이미 0 or Max value를 가지고 있는 Unpooled Maps를 굳이 왜 다시 Relu에 넣는지는 이해가 가지 않습니다. 실험을 통해 차이를 봐야할 것 같습니다.) 그렇게 이 단계들을 원본 Input pixel Size에 도달할 때까지 반복합니다. 이 Deconvnet의 방식은 학습되는 대상이 아니라, 단순 계산을 통해 Strong activation만 다시 시각화 해보는 하나의 function에 불과합니다. 하지만, 확실히 하나의 Conv layer를 거친 뒤에 생성되는 Feature map에서 어떤 부분들이 자극을 받았는지는 다소 정확하게 표현할 수 있다는 장점이 있습니다. 3. Convnet Visualization3.1. Feature Visualization Deconvnet을 사용해서, ImageNet validation set의 feature map들을 시각화한 결과들을 보면 다양한 해석을 할 수 있습니다. 바로 아래에 보이는 결과를 보면, 특히 오른쪽에 있는 빨간 박스 부분들을 보면, 좌측의 9개 이미지들에서 발견되는 것처럼 단순히 특징이라 부를 수 있는 패턴에만 집착하는 것이 아니라, foreground가 아닌 배경에 있는 잔디들을 감지하고 표현한다는 것도 알 수 있습니다. 아래의 결과를 보면 Layer 1, 2에서는 주로 Corner와 Edge, Color같은 단순한 정보를, Layer 3에서는 보다 복잡한 Texture를, Layer4에서는 Class 별로 구체적인 특징에 해당하는 Feature들을, Layer5에서는 전체적인 Pose Variation에 대한 Feature들이 추출된다는 것을 알 수 있습니다. 3.2. Feature Evolution during Training 또한, 각 layer 별로, 상위 Layer일수록 학습이 어느정도 진행이 된 뒤에서야 Feature map이 제대로 생성된다는 것을 시각화 결과 확인할 수 있었습니다. 그래서, 모델이 완전히 Converge 되기 전까지 학습을 유지해야한다는 것을 확인 할 수 있습니다. 아래 그림을 보면, Layer 별로 8개의 열로 표현되는 feature map들을 확인하실 수 있는데 첫 번째 열부터 8번째 열까지 각각 [1, 2, 5, 10, 20, 30, 40, 64] epoch이 지난 뒤의 시각화 결과입니다. 확실히 epoch이 어느정도 지나야만 특히 상위 layer들에서 feature map들이 더 선명하게 나오는 것을 볼 수 있습니다. 3.3. Feature Invariance 데이터 변형에 대해서 각 Layer들이 얼마나 민감하게 반응을 하는가에 대한 실험도 진행을 합니다. 아래와 같이 a1은 특정 축을 기준으로 한 데이터 변형으로, b1은 scale. c1은 rotation 각도에 따른 실험을 진행하면서 그 변화를 관찰하였습니다. 변형을 하지 않은 오리지널 데이터로 생성된 feature vector들과 변형된 이미지들의 feature vector 사이의 Euclidean 거리를 측정 한 결과, 비교적 작은 변화에 해당하는 축변형이나 Scale에 대한 변화에는 Network의 Output이 안정적이지만, rotation에는 약했습니다. 아래 그림의 첫 번째 열에 해당하는 a2, b2, c2는 첫 번째 layer의 반응이고, 두 번째 열인 a3, b3, c3는 7번째 layer의 반응입니다. 보시다시피, vertical translation과, Scale에서는 1st layer는 상대적으로 영향을 많이 받고, 7th layer는 상대적으로 큰 영향을 받지는 않습니다만, rotation에 대해서는 둘 다 영향을 많이 받습니다. 세 번째 열은 네트워크의 Output인 true label에 대한 확률입니다. 마찬가지로, rotation인 경우에 상대적으로 다른 이미지 변형보다 더 큰 영향을 받습니다. 4.1. Architecture Selection 학습된 모델의 시각화는 그 작동 방식에 대해 통찰력을 줌과 동시에, 먼저는 좋은 네트워크 구조를 고를 수 있도록 도와주는 역할을 할 수 있습니다. AlexNet의 첫 번째, 두 번째 layer를 시각화 한 결과인 아래그림의 (b)와, (d)를 살펴 보면, 여러가지 문제들을 알 수 있습니다. 첫 번째 layer의 Filter들을 보면, 아예 패턴이 없는 것이나 마찬가지거나 패턴이 뚜렷하거나, 극단적인 필터들이 생성되어 강도가 중간에 해당하는 필터를 찾기가 힘듭니다. 또한, 두 번째 layer의 필터들을 시각화해보면 울퉁불퉁한 aliasing artifacts들이 많은데, 이는 첫 번째 conv layer에서 사용한 너무 크게 stride를 4로 설정하였기 때문입니다. 이를 수정하기 위해서 첫 번째 레이어의 필터 사이즈를 11x11에서 7x7로 줄이고 stride도 4에서 2로 줄이는 조치를 취합니다. 그 결과인 ZFNet의 첫 번째 레이어와 두 번째 레이어의 필터 시각화 결과인 (c)와 (e)를 보면, 더 많은 정보 - (c)에서는 죽은 필터로 볼 수 있는 회색칸이 확실히 작아졌고, (e)에서는 aliasing이 상대적으로 많이 개선 - 를 담고 있는 것을 시각화 결과 확인 할 수 있습니다. 더 중요한 점은 뒤의 5.1에서 확인할 수 있듯이, 이 변화가 실제로 Classification 성능을 올려주게 된다는 것입니다. 4.2. Occlusion Sensitivity 모델의 Classiffication 접근법을 보면서 드는 자연스러운 질문은, 이 모델이 실제로 이미지 안 물체의 위치를 진짜로 파악을 하는 것인가, 아니면 단지 주변의 이미지를 보고 context 정보를 보고 판단하는가 입니다. 그래서 저자는 아래와 같이 이미지의 중요한 부분을 가리고 나서 어떻게 모델은 반응하는지 시각화를 하면서 판별하는 실험을 해봅니다. (a)라는 input image가 있을 경우, 원본 이미지에 아무런 훼손이 안되었을 경우에는 가장 강하게 발현되는 layer 5의 feature map은 (b)이고, 시각화 결과 (c)의 검은색 테두리 박스와 같이 결과를 볼 수 있습니다. 강아지 예시의 경우, 강아지 얼굴을 회색 사각형으로 가려보았더니, 가리기 전에는 (d)와 같이 포메라니안으로 예측하는 부분(파란색)이 컷는데, 해당 부분의 feature map의 반응이 크게 줄어 드는 것을 확인할 수 있었다고 합니다. 그리고 (e)처럼 가장 가능성이 큰 label을 표현하도록 Classifier를 시각화를 해보니, 원래는 (e)와 같이 포메라니안이라고 대부분 예측을 했지만, 얼굴을 가리고 나니 테니스 공이라고 예측을 주로 했다고 합니다. 이 예시로 실제로 모델은 물체의 위치에 민감하게 반응하는 것이 맞다고 결론을 내릴 수 있고, ZFNet의 필터를 시각화하고 AlexNet의 구조를 변형시킨 것이 의미 있는 행동이었다고 말합니다. 두 번째, 세 번째 레이싱 카와 아프간 하운드 예시는 가장 강하게 발현되는 feacture map의 위치가 실제로 예측을 할 때 결과로 나오는 물체와 다를 수 있다는 것을 보여주는 예시입니다. 자동차의 경우 숫자 8이 적여있는 부분이 가장 반응을 강하게 피쳐맵에서 했지만, 실제로는 Classifier를 시각화했을 때는 자동차 바퀴에서 반응을 크게 합니다. 마찬가지로 강아지 바로 위의 아주머니 얼굴이 가장 반응을 크게 했지만, Classifier는 강아지의 확률을 크게 가져갑니다. 이런 작동 방식이 가능한 이유로 저자는 모델이 여러개의 feature map을 사용하기 때문이라고 말합니다. 4.3. Correspondence Analysis 위의 occlusion 실험에 만족하지 못한(?) 저자는, 이미지가 모두 다를 똑같은 특정 부위를 가려보면 모델이 인풋 이미지는 달라도, 비슷한 특징을 가진 부분에 대해 일관성 있게 생각하는지 여부를 알 수 있는 실험이 될 것이라 생각하고 또 다른 실험을 해봅니다. 아래와 같이 다섯 마리의 강아지 사진이 있을 때, 첫 번째 열은 원본, 두 번째는 모두 왼쪽 눈을 가린 사진, 세 번째는 오른쪽 눈, 네 번째는 코를 가렸습니다. 그 이후로는 각각 랜덤한 부위를 가렸습니다. 그리고 나서, 각 열마다 5th, 7th layer의 feature vector를 뽑아서, 원본의 feature vector에서 뺀 차이 값을 계산하고, 그 차이값들이 서로 다른 이미지의 차이 값들과 어떠한 차이점이 있는지를 아래의 공식과 같이 Hamming distance를 계산한 뒤에 합칩니다. l이 layer를 뜻하고, i, j는 서로 다른 이미지들을 뜻합니다. 아, epsilon은 위에서 설명한, layer l에서 원본 사진과 가려진 사진의 feacture vector 차이값을 뜻합니다. 결론적으로, 그래서 논문에 실었겠지만 저자의 상상이 맞았다는 것을 아래와 같이 증명합니다. 5가지 이미지를 무작위로 가렸던 사진들보다, 왼쪽 눈, 오른쪽 눈, 코와 같이 일관성있게 다 같은 부위를 가렸던 것에 대해 Hamming distance가 비교적 낮은 수치를 기록합니다. Layer 7에서는 그 수치가 별로 다르지 않은 것은 ‘아마도’ 상위 레이어인 만큼 강아지의 특징이 아니라 마지막으로 종을 구별(Classify)하려고 하기 때문이지 않나.라고 저자는 말합니다. 5. ExperimentsImageNet 2012 ImageNet 2012 Classification 데이터로 실험을 한 결과, AlexNet을 다시 재현한 모델이 AlexNet 논문에서 말하는 ImageNet 2012 실험결과와 Val Top-5의 경우 0.1%밖에 차이가 나지 않은 것을 밝히면서, ZFNet의 성능이 더 좋다는 것을 아래와 같이 증명합니다. Caltech-256 클래스별 이미지 개수가 적기로 유명한 Caltech-256 데이터셋으로는 ImageNet 데이터에 Pretrained ZFNet의 성능을 뽑냅니다. 그 때 당시 최신 성능 냈던 Bo et al.의 결과와 비교하며 아래와 같이 월등한 성능을 자랑합니다. pretrained이 아닌 경우는 저조하지만, pretrained의 성능이 ImageNet feature extractor의 파워를 보여준다고 말합니다. PASCAL 2012 마지막으로, PASCAL2012 입니다. 이미지 내 여러가지 클래스가 포함되어 있어서, ImageNet에 Pretrained된 ZFNet에게는 불리한 조건이었지만, 5개 클래스에 대해서는 최고 좋은 성능을 내기도 했다면서 Test set에 대한 결과를 아래와 같이 밝혔습니다. ([A]= Sande et al., 2012 and [B] = Yan et al.,2012) 여기까지, AlexNet 리뷰를 마치도록 하겠습니다. 다음 포스팅에서는 ZFNet 리뷰를 하겠습니다. 읽어주셔서 감사합니다. 🙇 언제 시간이 여유로울 때, ZFNet에서 말하는 시각화 하는 코드를 짜서 직접 여러 모델들의 layer를 관찰해봐야 겠네요! 이 글은 DataCrew에서도 보실 수 있습니다. References AlextNet Paper document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2020/07/16/ZFNet/"}],"tags":[{"name":"blog","slug":"blog","link":"/tags/blog/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"tutorial","slug":"tutorial","link":"/tags/tutorial/"},{"name":"useful-info","slug":"useful-info","link":"/tags/useful-info/"},{"name":"machine-learning","slug":"machine-learning","link":"/tags/machine-learning/"},{"name":"ml","slug":"ml","link":"/tags/ml/"},{"name":"logarithm","slug":"logarithm","link":"/tags/logarithm/"},{"name":"log","slug":"log","link":"/tags/log/"},{"name":"machine_learning","slug":"machine-learning","link":"/tags/machine-learning/"},{"name":"pandas","slug":"pandas","link":"/tags/pandas/"},{"name":"dataframe","slug":"dataframe","link":"/tags/dataframe/"},{"name":"TPU","slug":"TPU","link":"/tags/TPU/"},{"name":"deep_learning","slug":"deep-learning","link":"/tags/deep-learning/"},{"name":"transfer learning","slug":"transfer-learning","link":"/tags/transfer-learning/"},{"name":"tensorflow object detection api","slug":"tensorflow-object-detection-api","link":"/tags/tensorflow-object-detection-api/"},{"name":"tensorflow","slug":"tensorflow","link":"/tags/tensorflow/"},{"name":"Colab","slug":"Colab","link":"/tags/Colab/"},{"name":"basic","slug":"basic","link":"/tags/basic/"},{"name":"CNN","slug":"CNN","link":"/tags/CNN/"},{"name":"Batch","slug":"Batch","link":"/tags/Batch/"},{"name":"Epoch","slug":"Epoch","link":"/tags/Epoch/"},{"name":"Filter","slug":"Filter","link":"/tags/Filter/"},{"name":"Kernel","slug":"Kernel","link":"/tags/Kernel/"},{"name":"Padding","slug":"Padding","link":"/tags/Padding/"},{"name":"Pooling","slug":"Pooling","link":"/tags/Pooling/"},{"name":"Google Cloud Platform","slug":"Google-Cloud-Platform","link":"/tags/Google-Cloud-Platform/"},{"name":"GCP","slug":"GCP","link":"/tags/GCP/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"ssh","slug":"ssh","link":"/tags/ssh/"},{"name":"container ssh","slug":"container-ssh","link":"/tags/container-ssh/"},{"name":"regularization","slug":"regularization","link":"/tags/regularization/"},{"name":"loss","slug":"loss","link":"/tags/loss/"},{"name":"L1","slug":"L1","link":"/tags/L1/"},{"name":"L2","slug":"L2","link":"/tags/L2/"},{"name":"Lasso","slug":"Lasso","link":"/tags/Lasso/"},{"name":"Ridge","slug":"Ridge","link":"/tags/Ridge/"},{"name":"convex_optimisation","slug":"convex-optimisation","link":"/tags/convex-optimisation/"},{"name":"keras","slug":"keras","link":"/tags/keras/"},{"name":"mobilenet","slug":"mobilenet","link":"/tags/mobilenet/"},{"name":"tensorpack","slug":"tensorpack","link":"/tags/tensorpack/"},{"name":"pseudo-label","slug":"pseudo-label","link":"/tags/pseudo-label/"},{"name":"Optimizer","slug":"Optimizer","link":"/tags/Optimizer/"},{"name":"Loss function","slug":"Loss-function","link":"/tags/Loss-function/"},{"name":"Back propagation","slug":"Back-propagation","link":"/tags/Back-propagation/"},{"name":"AlexNet","slug":"AlexNet","link":"/tags/AlexNet/"},{"name":"LeNet","slug":"LeNet","link":"/tags/LeNet/"},{"name":"ZFNet","slug":"ZFNet","link":"/tags/ZFNet/"}],"categories":[{"name":"Tutorial","slug":"Tutorial","link":"/categories/Tutorial/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/categories/Machine-Learning/"},{"name":"Deep Learning paper","slug":"Deep-Learning-paper","link":"/categories/Deep-Learning-paper/"}]}