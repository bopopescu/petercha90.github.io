{"pages":[],"posts":[{"title":"How to create a Hexo Blog","text":"Simple Hexo Tutorial입니다. Hexo는 Github pages를 사용해서 만드는 블로그입니다. 그러니, Github 계정을 가지고 계셔야 사용하실 수 있습니다. Github pages는 username.github.io라는 이쁜 고유 도메인(무려 https)도 주는 아주 고마운 블로그입니다. 지금 보고 계신 이 블로그도, Hexo로 만들어졌습니다. 도메인만 제가 사서 바꿨을 뿐이구요. 이 블로그를 설치하고 관리하고 사용하려면, HTML, Markdown, git에 대한 기본적인 이해는 있어야 합니다. 사실, 언급한 기술들을 잘은 몰라도, 대충 한 번씩만 해보셨으면, 하시다보면 익숙해져서 할만 하실 것도 같습니다. 😃 1. Make a repository at Github Github에 가셔서 새로 Repository를 만드시는데, 그 repository의 이름은 꼭, username.github.io로 하셔야 합니다. 아래 보이는 사진처럼요. 2. Install Hexo node.js & git Hexo를 설치하고, 사용하려면 node.js와 git이 설치되어 있어야 합니다. 이 두가지 툴이 잘 설치가 되어있다면, 아래와 같은 명령어 한 줄로 간단하게 Hexo를 설치할 수 있습니다. $ npm install -g hexo-cli local directory 본인이 사용 하고자 하는 폴더를 만드시고 들어가셔서, 명령어로 $ hexo init $ npm install 를 순차적으로 입력합니다. 그리고 나서 설치가 끝나면, Hexo blog를 시작할 수 있는 파일들이 설치 된 것을 확인 할 수 있습니다. 저는 temp_blog라는 폴더를 만들어서 설치해 보았습니다. 3. Start Hexo Blog 믿기지는 않지만, 이렇게 위의 3개 명령어로 당신의 Hexo Blog가 생겼습니다. 생성된 Blog를 확인해 보도록 합시다. $ hexo server localhost:4000으로 접속을 하라고 하니, 웹브라우저 창을 통해 http://localhost:4000/ 을 입력하거나, 주소를 클릭해서 확인해 봅시다. Ta da!! 블로그가 생겼다는 것을 확인할 수 있습니다! 😄 가장 기본 테마인 landscape로 설정되어있고, 글은 Hello World 밖에 없지만, 이제 여러분의 취향대로 바꿔나가기만 하면 됩니다! 자, 그럼 차근차근히 어떻게 블로그의 설정을 바꿀 수 있는지, 글은 어떻게 쓰면 되는지, 실제로 나에게 부여된 도메인에 배포를 할 수 있는지 알아봅시다. 4. Hexo _config.yml 우리가 설치한 폴더를 보면, _config.yml파일을 확인 할 수 있습니다. 대략적인 모습은 아래와 같습니다. 간단하게, title, timezone, url, 그리고 deploy부분만 본인에게 맞는 정보로 고쳐보기로 합니다. 1234567891011121314151617181920212223242526272829303132# Hexo Configuration## Docs: https://hexo.io/docs/configuration.html## Source: https://github.com/hexojs/hexo/# Sitetitle: Stand firm Petersubtitle:description:author: John Doelanguage: entimezone: Asia/Seoul# URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'url: http://petercha90.github.ioroot: /permalink: :year/:month/:day/:title/permalink_defaults:...# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: landscape# Deployment## Docs: https://hexo.io/docs/deployment.html## 배포방식은 git으로, type은 본인의 repository 주소!deploy: type: git repo: https://github.com/PeterCha90/petercha90.github.io.git Deployment부분에서 deploy 방식(git)과, 우리 블로그와 실제 부여받은 도메인과의 연결을 할 수 있게 주소를 잘 적어줘야, 이후에 나오는 hexo deploy명령어를 통해 실제로 블로그를 배포할 수 있게 됩니다. hexo server로 켰던 로컬 서버를 Ctrl + c로 종료한 다음, hexo server로 서버를 구동시킨 뒤, localhost:4000으로 접속해서 확인해 보면, title이 잘 바뀌어 있습니다 😄 5. Create a new post 그럼, posting은 어떻게 하면 될까요?! 이역시, 매우 쉽습니다. $ hexo new “sample_posting” 이 명령어를 치면, 자동으로 /source/_posts/ 밑에, sample_posting.md이라는 파일이 뙇.하고 생겼다는 메세지를 볼 수 있습니다. 이 파일을 열어보시면 뭐가 적힌게 별로 없습니다. 12345---title: sample_postingdate: 2018-01-13 22:56:29tags:--- 그래서 제가 몇 자 적어보았습니다. 123456789101112---title: sample_postingdate: 2018-01-13 22:56:29tags:---Markdown > 몇자 적어 보았습니다. ## 두 자 적어 봤습니다. **세 자 입니다.** 그럼 이 sample_posting.md파일을 저장하시고, 아래의 명령어를 수행해 봅니다. $ hexo generate 뭔가 많이 생성됐다는 메세지가 나왔습니다. 그리고 다시 hexo server를 사용해서 서버를 여시고, 로컬 4000번 포트로 접속해서 확인해 주세요 😄 짜잔~ 이렇게 우리의 첫 번째 Posting이 잘 등록 됐습니다! 🎉🎉 위의 제가 적은 내용과 결과를 보시면 알 수 있듯이, Hexo는 기본적으로 Markdown 언어를 지원합니다. 6. Deploy to remote sites 자, 그럼 이번에는 마지막으로 이렇게 작성한 블로그와 포스팅을 우리의 실제 원격 저장소(Github repository)로 배포해보도록 합니다. 먼저, 아래와 같이 hexo-deployer-git을 먼저 설치해주시고, 1$ npm install --save hexo-deployer-git 아래의 명령어를 입력하시면, 배포가 끝났습니다!😆 1$ hexo deploy 2, 3분 뒤에 username.github.io로 접속하셔서 확인해 보세요 😃 7. Summary hexo server Blog contents가 올바르게 보이는지 확인할 수 있는 로컬 서버를 구동합니다. hexo new \"title\" 새로운 \"title\"이라는 이름의 posting을 작성합니다. hexo generate 수정된 사항으로 deploy할 수 있도록 contents를 생성합니다. hexo deploy 원격 저장소(github repository)에 실제로 배포합니다. Tips : hexo generate = hexo g hexo deploy = hexo d hexo clean: 게시글에 html문법이 rendering되지 않고 깨져서 나올 때, 가끔씩 실행 Contents 생성 후 바로 배포하기 hexo generate deploy = hexo g -d 8. Themes 기본 테마도 훌륭하지만, 좀 더 다른 느낌의 테마를 원하신다면, 여기로 들어가셔서, 테마를 고르시고 본인의 _config.yml파일을 수정하면 됩니다!구체적인 테마 적용 방식은 보통 선택한 테마에서 설명을 해주기 때문에, 세부설정이 제각각 다릅니다. _config.yml 파일에서, theme이라는 key 값을 본인이 선택한 테마의 이름으로 바꾼다는 점은 공통이겠지만요 😄 지금 이 튜토리얼을 그대로 따라하셨다면 지금 여러분들의 상태는 아래와 같을 겁니다. theme: landscape document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2018/01/06/Start/"},{"title":"Logarithms","text":"Deep Learning 논문에서 자주 등장하는 Log. Machine learning 및 통계에서 자주 쓰는 이유! Log를 왜 쓰는 걸까 많은 딥러닝 논문들을 읽다보면, 하나같이 수식으로 설명하기 시작하는데, 그 때 꼭 나오는 친구가 이 Log다. 이미 아시는 분들에겐 시시한 이야기였겠지만, 대충 어렴풋이 필자와 같이 '숫자 크기 작게 해주려고 하는 거겠지…'정도로만 생각하고 있었다면(맞긴 맞다만…), 정확한 의미를 알아보자. 아래는 상용로그인 logx\\text{log}xlogx와, lnx\\text{ln}xlnx로 많이 쓰는 자연상수 eee를 밑으로 하는 logex\\text{log}_exloge​x의 그래프다. 이 그래프를 보면서 할 수 있는 생각은, ‘xxx값이 엄청나게 커진다고 해도, yyy 증가율은 격하게 변하지 않구나.’ →\\to→ ‘xxx수치가 낮을 때는 민감하게 반응하고, 높을 때는 둔감하구나.’ →\\to→ ‘그럼 수치가 대부분 낮을 때는 비교하기 좋을 거 같고, 이상치와 같은 비정상적인 예외 케이스들도 같이 고려할 수 있겠다.’ 정도가 될 것이다. 그럼 이걸 활용하면 아래와 같은 상황에 유용하게 쓸 수 있다. logloglog는 어디에 사용되나요 대표적으로 두 가지의 경우에 많이 사용되는 것 같다. 데이터들 간의 수치적인 간극이 너무 커, 주어진 수치를 그대로 사용하면 회기분석시에 결과가 왜곡될 수도 있어서. 비선형적인 데이터의 분포를 선형적으로 쉽게 보기 위해서 1. 아래의 사진은 동물들의 체중별 뇌의 크기를 나타낸 scatter plot이다. 파충류같은 작은 동물들 부터, 코끼리, 고래, 공룡까지 다 들어 있다고 생각해보면 그 비교대상들이 서로 가지는 수치적인 차이는 실제로 왼쪽 그림과 같다. 오른쪽은 같은 데이터에 상용로그를 취한 것이고 실제로 그 경향성을 더 파악하기 쉬운 형태가 되었다. 아래 사진도 그런 방식으로 log\\text{log}log를 취해서 파충류부터 고래와 코끼리, 공룡까지 비교한 plot인데, x축과 y축의 수치를 자세히 보면 log\\text{log}log를 취한 그래프임을 알 수 있다. 2. 아래는 y=2xy=2^xy=2x 그래프와, 이 그래프에 자연로그를 취한 ln2x\\text{ln}2^xln2x의 그래프다. 비선형 그래프인 y=2xy=2^xy=2x를 선형으로 만들어 주고 있다. 이처럼, 로그는 복잡한 비선형인 수식들을 간소화 시켜주는 역할을 한다. 곱셈과 나눗셈이 log연산이 되면서 +, -로 바뀌기 때문에 아무래도, Model에서 computation을 많이 해야하는 부담도 좀더 줄어 들지 않을까?! 이런 이점 때문에도 많이 쓰는 것 같다. 이상, log\\text{log}log를 왜 쓰는지 한 번 알아 보았다. Reference blog1, blog2 Desmos(그래프 그려주는 사이트) document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2018/09/12/Logarithms/"},{"title":"Pandas Cheat Sheet","text":"Pandas의 Dataframe 관련 기능정리. Cheat Sheet for Pandas Dataframe. 0. Start Import pandas 1import pandas as pd 1. Creating a dataframe Dictionary Style: 12345df = pd.DataFrame( {\"a\" : [4, 5, 6], \"b\" : [7, 8, 9], \"c\" : [10, 11, 12]}, index = [1, 2, 3]) Array Style: 123456df = pd.DataFrame( [[4, 7, 10], [5, 8, 11], [6, 9, 12]], index=[1, 2, 3], columns=['a', 'b', 'c']) 2. Select an row or column There are 5 ways to access the value that is at index 0, in column ‘a’. loc selects the value by label, not index. iloc accesses to the value using row and column index. 12df.loc[0]['a']df.iloc[0][0] at selects the value by label, not index. iat accesses to the value using row and column index. 12df.at[0, 'a']df.iat[0,0] ix can use both label and index. 1df.ix[0, 'a'] The difference between loc and at is the return type. loc can return more than one row, it means loc can return a scalar, a Serise or a Dataframe. On the other hand, at only can access to a cell of certain position, it means it returns a scalar only. Actually it returns a scalar faster than loc, so if you have to deal with great amount of data, it would be more suitable. 123456789101112df.at[2, 'b'] # A scalar.df.iat[2, 2]df.loc[2]['b']df.iloc[2][2]df.ix[2, 'b'] df.loc[2][:'b'] # A Series.df.iloc[2][:2]df.ix[2, :'b'] df.iloc[:2][:2] # A dataframe.df.ix[:2, :'b'] If you want to retrieve the value(s) from a series, 12sr = df.bsr.values() Select multiple rows or columns 1234567# rowsdf.iloc[[0, 2]] df.loc[[1, 3]] # row names # columnsdf.iloc[:, [0, 2]]df[[\"a\", \"c\"]] # column names 3. Selecting rows with conditions Condition in the brackets. 1234df[df.a > 4]df[(df.a > 4) & (df.b < 9)]# You have to wrap all conditions with parentheses.df[((df.a > 4) | (df.b < 9)) & (df.c > 10)] Condition using query. 123df.query('a > 4')df.query('a > 4 and b < 9')df.query('(a > 4 or b < 9) and c > 10') 4. Adding rows or columns Adding new rows 12df.loc[3] = 0 # fill with 0df.loc[3] = [5, 7, 1] # Add a row Creating a new column 12345df[\"d\"] = 0 # fill with 0 df[\"d\"] = df.a # copy column 'a'df[\"d\"] = pd.Series([13, 14, 15], index = df.index) # fill with new valuesdf.loc[ : , \"d\"] = pd.Series([13, 14, 15], index = df.index) 5. Delete Indices, Rows or Columns Delete rows 1234567df.drop([0, 2], axis=0) # Delete the rows with labels 0, 2df.set_index(\"a\", inplace=True) # Delete all rows with label 4df.drop(4, axis=0, inplace=True)df.reset_index(inplace=True)df = df.iloc[2:, ] # Delete the first two rows using iloc selector Delete columns. 12df.drop(\"a\", axis=1, inplace=True) # Delete a columndf.drop([\"a\", \"c\"], axis=1, inplace=True) # Delete multiple columns 6. Combine Dataframes pd.concat() 1234567891011121314151617df2 = pd.DataFrame( # Make two more dataframes {\"a\" : [1, 4, 7], \"b\" : [2, 5, 8], \"c\" : [3, 6, 9]}, index = [1, 2, 3])df3 = pd.DataFrame( {\"d\" : [1, 7], \"e\" : [2, 8]}, index = [1, 3])# concat rows df4 = pd.concat([df, df2]) # indices can be duplicateddf4.reset_index(drop=True, inplace=True) # Reset index# concat columnsdf5 = pd.concat([df, df3], axis=1) pd.merge() 12345678910df6 = pd.DataFrame( # Make another dataframes {\"a\" : [4, 5, 7], \"f\" : [20, 50, 80], \"g\" : [30, 60, 90]}, index = [1, 2, 3])df7 = pd.merge(df, df6) # Inner join df8 = pd.merge(df, df6, how='outer') # Outer joindf9 = pd.merge(df, df6, how='left') # left joindf10 = pd.merge(df, df6, how='right') # right join 7. Iterate over a dataframe iterrows() 12for index, row in df.iterrows() : print(row['a'], row['b'], row['c']) 8. Reading and Writing a dataframe Reading a csv file: 1234pd.read_csv('name.csv')# if the file has a date column,pd.read_csv(\"name.csv\", parse_dates=[\"column_name\"])# Then, pandas transform it as a numpy.datetime64 column. Writing as a csv file: 123df.to_csv(\"name.csv\", index=False)# optionsdf.to_csv(\"name.csv\", sep='\\t', encoding='utf-8') Writing as an Excel file: 123writer = pd.ExcelWriter('name.xlsx')df.to_excel(writer, 'DataFrame')writer.save() 9. Other Useful functions Following commands are used often. 123456789101112df.head(n) # Select first n rows df.tail(n) # Select last n rowsdf.nlargest(n, 'a') # Select and order top n entries.df.nsmallest(n, 'c') # Select and order bottom n entriesdf.sample(frac=0.5) # Randomly select fraction of rows.df.sample(n=2) # Randomly select n rowsdf.isna() # Check whether each element has NA or not df.dropna() # Drop rows with any column having NA/null datadf.fillna(value) # Replace all NA/null data with valuedf.rename(columns={\"a\": \"k\"}, inplace=True) # Renaming \"a\" column as \"k\"df.describe() # To see basic statisticsdf.reset_index(drop=True, inplace=True) # Reset index References DataCamp pandas.pydata.org Shane Lynn Data Science School document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2019/02/16/pandas-101/"},{"title":"tf.identity()","text":"Tensorflow에서 자주 등장하는 tf.identity()에 대해 알아봅니다. tf.control_dependencies()와 함께 모델을 구현하다보면 거의 제일 마지막인 fully connected인 Linear에 이르러 맨 마지막 logits을 뽑아내기 전에 꼭 한 번쯤은 보게 되는 친구가 이 tf.identity(). 습관적으로 그냥 쓰는건가보다 하다가 문득 더 깊이 이해하고 넘어가야하겠다 싶어서 이렇게 포스팅으로 남긴다. tf.identity()를 이해하기 위해서는 tf.control_dependencies()를 먼저 이해해야 한다. Tensorflow의 공식문서에 따르면 그 설명이 아래와 같다. tf.control_dependencies() tf.control_dependencies(control_inputs) control_inputs: A list of Operation or Tensor objects which must be executed or computed before running the operations defined in the context. Can also be None to clear the control dependencies. If eager execution is enabled, any callable object in the control_inputs list will be called. Context 안에서 정의된 Operation이 Running되기 \"전\"에 “먼저” 실행시켜줄 친구들을 control inputs으로 넣어주면 먼저 실행해준다는 의미다. 그럼 이제 tf.identity()를 살펴보자. tf.identity() tf.identity(input, name=None) Return a tensor with the same shape and contents as input. input: A Tensor. name: A name for the operation (optional). 그냥 입력된 Tensor랑 똑같은 Shape, 똑같은 contents를 돌려준다. 그럼 왜 쓰는 거지?? 1234567891011x = tf.Variable(0.0)x_plus_1 = tf.assign_add(x, 1)with tf.control_dependencies([x_plus_1]): y = tf.identity(x)init = tf.initialize_all_variables()with tf.Session() as session: init.run() for i in range(5): print(y.eval()) 여기에서 가져온 예시를 보면, 이유를 좀 알 수 있는데, 만약에 y = tf.identity(x) 부분이 그냥 y = x라면 아무 일도 일어나지 않음을 알 수 있다. print의 결과가 죄다. 0 0 0 0 0 이다. 왜냐하면, tf.control_dependencies()는 \"Operation\"이 실행되기 전에 inputs으로 들어온 부분을 처리해준다고 했는데, y = x는 아무런 operation이 없기 때문에 x_plus_1를 실행하지 않는다. 결론 Tensorflow로 모델을 만들 때, 주로 맨 마지막에 Fully connected Layer에서 Output에 해당하는 Logits을 뽑을 때 이 tf.identity()를 많이 쓰는 이유는 tf.control_dependencies()를 실행시켜주기 위한 건덕지(?)를 만들기 위해서다. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2018/09/07/tf-identity/"},{"title":"Basic Deep learning 02","text":"Deep Learning 개념 및 용어들을 알아봅니다. Batch, Epoch, CNN Peter Cha Deep Learning을 이해하고, 직접 Deep Learning을 구현하고자 했을 때 필요한 기본 개념들을 정리해 보았습니다. 이번 포스팅에서는 Epoch, Batch라는 단어들의 의미와, 기본적인 CNN - Convolutional Neural Network에 대한 키워드들을 다룹니다. 유명한 MNIST 데이터를 학습하는 모델을 만들고 싶다고 했을 때, 언급한 키워드들이 어떤 의미로 사용되는지 예시로 함께 보려합니다. In this post, you will learn the Concepts needed when you need to understand the process of training AI or implement the AI by yourself. We are going to talk about Epoch, Batch, and basic CNN knowledges. Supposed we want to make a model which classifies MNIST data, let’s check how the keywords above can be used. MNIST MNIST data는 아래에서 보시는 것처럼 0 ~ 9까지의 숫자가 적혀 있는, 손글씨 data입니다. 따라서 총 10가지의 class가 있습니다. 이 데이터를 이용해서 우리가 학습시키고 싶은 모델은 따라서 새로운 손글씨 data를 보더라도 0 ~ 9중에 어떤 숫자인지 잘 맞추는 AI가 될 것입니다. Images from tensorflow.gitbooks.io. As you can see above, MNIST is a dataset of handwritten digits, 0 to 9. Therefore, MNIST dataset has 10 classes to distinguish. Using this data, Our model to be trained will be able to distinguish 0 ~ 9 handwritten digits. Epoch, Batch MNIST는 Training data로 총 6만 장의 수기로 된 숫자를 제공하고, Test용으로 1만 장을 제공하는 Dataset입니다. 자, 그럼 우리는 6만장을 한꺼번에 모델에게 주고 학습해!라고 하면 될까요? 할 수는 있더라도 꽤 여유로운 메모리를 가진 local machine이 아니고서는 좀 힘들겠죠? MNIST가 아닌 더 큰 용량의 데이터일 수록 더 그럴 것입니다. 그래서, 우리는 이 데이터들을 특정한 양으로 나눠서 조금씩 학습을 할 수 있게 넣어주는데요, 그 작은 단위를 Batch라고 부르고, 그 Batch의 크기가 어떠한지를 일컫는 말로, Batch size라고 말합니다. 우리가 Batch size를 100으로 정했다고 하면, 총 몇 번의 반복을 해야 총 60000장의 Training data를 다 한 번씩 모델이 학습할 수 있게 될까요? 600번 일 것입니다. 그럼 실제로, 우리 모델은 100장의 데이터를 가져와서 한 번 돌고(학습하고), 앞에서 배운 Back propagation을 통해, Weight를 Update하게 되면, 그 다음 100장을 가져와서 또 학습을 똑같이 반복하는 이 행위를 총 600번을 하게 됩니다. 그렇게 600번을 다 돌았을 때, 우리는 '1 epoch을 돌았다'라고 말합니다. 참고로, 이 600번을 Step size라고 일컫습니다. CNN CNN은 Convolution Neural Network의 약어로, Convolution 계산이 어떻게 Neural Network 2D 이미지 계산과 관련이 있는지는 여기를 참고해주세요. 이 글에서는, CNN에서 자주 언급되는, Filter, Kernel, Stride, Pooling, 그리고 Padding에 대해서 알아봅니다. Feature(= Channel or Activation map) 잠시 MNIST대신에 고양이가 어떻게 생겼는지를 학습하는 Model을 만들고 있다고 생각해 봅시다. 그러면 RGB color로 된 사진을 넣어주게 되고, 우리는 모델에게 이 고양이에 대한 특징(Feature)을 추출해서 학습을 하라고 할 것입니다. Images from ireneli.eu. 자, 그러면 우리 모델이 맨 처음 보게될 이미지는 Width, Height, 그리고 Red, Green, Blue 3가지로 이루어진 이미지를 받게 되는 것이죠. 이 때, 우리는 Channel이라고 부르는 부분으로 이 RGB인 Depth를 지칭합니다. 그러면 원본 이미지는 'channel의 size가 3이다'라고 말 할 수 있게 됩니다. 우리 MNIST 이미지는 가로 28 pixel, 세로 28 pixel 짜리, 흑백 이미지입니다!(이미지에는 32라고 적혀있지만…) 그래서 MNIST 이미지의 channel의 Size는 1입니다. Images from parse.ele.tue.nl. 이번에는 왜 이 Channel의 또다른 이름이 Feature, Feature maps인지 알아봅시다. 우리 MNIST 데이터가 위의 그림과 같이 들어간다고 했을 때, C1C_1C1​을 보시면 5x5 크기의 Conv를 통과한 뒤, 이미지가 4겹(?)이 됐습니다. 이 때 우리는 Feature map의 size가 4가 됐다고 말할 수 있습니다. 그리고 C2C_2C2​를 보시면 feature map이 12개가 됐죠. 이런 행위를 해석을 하자면, 들어온 이미지에 대해서 특징을 추출했는데, C1C_1C1​에서는 특징을 4개를 추출하고, C2C_2C2​에서는 특징을 12개를 추출한 뒤 결과물이라고 생각하시면 됩니다. '특징(Feature)을 추출한다'라는 말이 무슨 말인지 이해를 돕기 위해서, 아래 사진을 준비했습니다. Images from deliveryimages. 위 사진은, 사람 얼굴을 학습하는 CNN 모델의 Layer별로 추출한 특징들을 시각화 한 것입니다. 맨처음엔 Pixel로 구성돼있는 원본 이미지에서 맨처음에는 취운 특징인 가로, 세로, 대각선, 원, 곡선 등등의 비교적 단순한 edge들만 특징으로 추출하고 있습니다. 하지만 모델의 구조가 더 깊이 들어갈 수록, 그 단순한 Feature들을 조합해서 조금더 복잡한 눈, 코, 입 등을 그릴 수 있게 되고, 그 자체를 Feature로 삼을 수 있게 됩니다. 그러다 보면 사람의 전체전인 얼굴이라는 object를 Detect할 수 있는 모델이 되는 것이죠. 그래서 제 개인적으로는 들어올 때는 Input Image의 Color라는 의미의 Channel로 부르는 것이 더 와닿다가, Layer를 통과할 수록 더욱더 정교한 특징들을 이미지에서 뽑기 때문에, Feature map이라는 말이 더 와닿으니 서로 혼용해서 같은 녀석을 부르는 것 같다는 느낌이 있습니다. Kernel(= Filter) & Stride 바로 위에서 설명한 Feature Map은 이제 설명할 Kernel, 혹은 Filter라는 녀석을 통과한 뒤 나온 결과물 입니다. 이미지가 들어왔을 때, kernel이라는 Window를 정하고, 그 Window를 움직이면서 그 Kernel이 가지고 있는 weight값들과 Input image와의 연산을 통해 새로운 값을 가진 Image를 생성하게 되는 것이죠. 아래 예시를 통해 더 자세히 알아 봅시다. Images from blog.bkbklim.com. 위 Animation에서 Input이미지는 5x5 Size이고, 3x3 Size의 Kernel, 혹은 Filter가 한 칸씩 움직이면서 Image와 자신이 가지고 있는 Feature map을 계산하여 결과값을 내놓고 있습니다. Kernel은 X자 모양의 Filter네요. element-wise 곱, 즉, dot product를 계산하여 자신의 필터에 부합하는 위치면 곱한 값이, 아니면 0이 곱해져서 의미없는 0이 나오게 됩니다. 그렇게 나온 결과들을 다 더한 값 하나만 결과로 내놓습니다. 여기서 등장하는 Stride! 자연스럽게 이 애니메이션에서는 한 칸씩 움직이고 있습니다만, 어디까지나, Stride가 1x1일 때의 움직임입니다. Stride는 어느 정도의 간격을 가지고 Kernel 계산을 진행할 것인지를 나타내는 척도입니다. Stride가 2x2였다면, 두칸씩 움직일테고, 이 Input Size와는 맞지 않기 때문에 에러를 일으킵니다. 아래는 Filter를 설명하는 또 다른 이미지 인데요, 우리가 언급한 저 Filter에 보이는 www가 심상치 않습니다. Backpropagation을 하면서 Update되는 Weight들은 다 저, Filter의 Weight들입니다! 더 명확한 특징들을 추출하기 위한 세련된 Filter가 되기 위해 그 weight들을 맞춰나가는 것이죠. 그럼 이렇게 만들어진 Filter들을 통과하는 Feature map들은 어떻게 형성되는지, 더 명확한 이해를 위해 아래 사진을 보실까요. Images from deliveryimages. 위와 같은 Filter가 하나 있습니다. 곡선을 찾는 Filter네요. 그럼 이 Filter가 들어온 쥐 이미지를 Stride에 맞게 돌아 다니게 됩니다. 그러면서, 이 곡선에 해당하는 위치가 있는지 찾습니다. 이 Filter가 한바퀴 다 돌고 완성된 Feature map은, 아래와 같이 해당 Filter에 반응하는 부분에 높은 숫자를, 아닌 부분에서는 0에 가까운 수를 가진 Feature map이 되는 것이죠. Images from deliveryimages. 작아지는 Image Size 위에 나온 Animation에서 주목할 점은, 5x5 이었던 이미지 사이즈가, Kernel 계산을 마친 후, 3x3가 됐다는 것입니다! 이런 계산이 저 위에 사진에서 보셨듯이 32x32 size였던 MNIST 이미지가, C1C_1C1​에서 28x28이되고, C2C_2C2​에서 10x10 size로 줄게 되었는지 설명할 수 있게 됩니다. Padding 이미지가 계속 Convolution layer를 통과하면서 작아지면 나중에는 1x1까지 가다못해 없어지지 않을까요?! 그래서 이미지의 Size가 너무 줄어들지 않게 이미지 주변에 값을 넣어주는 기법을 padding이라고 합니다. 가장 많이 쓰이는 Zero padding으로 예시를 보이자면 아래와 같습니다. 저렇게 Kernel Size가 2x2이고, Stride가 1x1인 경우, 원본이 3x3 Size였으면 2x2 kernel로 계산을 해도 총 9번을 계산하여 원본 사이즈를 그대로 유지할 수 있게 됩니다. Pooling Pooling은 Filter 계산이 아닌, kernel size에 해당하는 영역에 있는 값들을 일괄적으로 처리하고 싶을 때 사용합니다. 대표적인 Pooling으로 많이 쓰이는 Max Pooling의 개념은 아래와 같습니다. Images from deliveryimages. 말 그대로 2x2 Max pooling을 하겠다 하면, 해당 사이즈에서 가장 큰 값만 취하는 것이죠. 가장 중요한 정보만 취하겠다는 의도가 있습니다. 그 외에도 Average Pooling을 비롯한 다양한 Pooling이 있습니다. 이 Pooling의 장점은, 무엇보다 계산이 단순해서 계산량, Computation Cost가 작다는 것 정도가 되겠습니다. 위에서 배운 Padding을 응용해보면, Padding을 적당히 준 이미지에 Pooling을 하면 이미지 사이즈는 줄지 않지만 빠른 계산은 가능한 구조도 가능하겠죠? 여기까지 기본적인 딥러닝 용어인, Batch, Epoch, 그리고 CNN의 기본 용어들을 살펴 보았습니다! 다음 포스팅에서는, Activation Function, MLP와 CNN의 차이점에 대해 이야기 해 보도록 하겠습니다. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2018/04/06/Basic-Deep-Learning-02/"},{"title":"Colab tutorial :)","text":"Cloud Jupyter Notebook powered by Google Colab Introduction 2020.01.14 - Update Colab이란? 😶 Jupyter Notebook를 구글 드라이브에서 사용할 수 있는 것인데, 여태 있었던 구글 문서나 구글 스프레드시트처럼, 실시간 협업이 가능한 버전이라고 생각하면 된다고 합니다. 원래 구글 내부에서 직원들이 사용하던 것이라고 하네요! 그래서 error가 생기면 STACK OVERFLOW 버튼이 …👍 그렇기 때문에, Jupyter Notebook나 다른 IDE에서 흔히 제공하는 Variable다음 .을 찍고, Tab키를 치면 하위요소(함수 or 변수)의 일부만 쳐도 나타나게 하는 친숙한 기능들도, 조금 반응이 느리긴 하지만, 0.5초 정도 뒤에 나타납니다. 😄 GPU, TPU 사용 설정을 하시려거든, 맨처음 Notebook을 생성하실 때, 선택하실 수도 있고, 수정 -> 노트 설정을 누르시면 GPU, TPU 사용 설정을 할 수 있습니다. 😄 시작하기 구글 아이디를 가지고 계신다면, 이 링크를 클릭하시거나 간단하게 Colab이라고 검색을 하시면 쉽게 Colab을 시작하실 수 있습니다. 그러면, 아래와 같은 첫 화면을 보실 수 있습니다. 그럼 새 PYTHON 3 노트 버튼을 누르시고, Jupyter notebook을 사용하듯 사용하시면 됩니다. 😃 Linux 명령어 사용하기 😲 Colab에서는 느낌표로 쓴 뒤에 명령어를 치면 terminal에서 작동하는 것으로 처리해줍니다. 로컬에서 Bash창을 켜서 설치하고 다시 Jupyter로 돌아와야했던 귀찮은 과정이 매우 심플해졌습니다! 하지만, 1$ cat sample.txt 같은 native Linux에서나 가능한 몇몇 문법들은 작동하지 않습니다. 그래서, 100% Linux native 명령어를 쓸 수 있는 것은 아니며, 어디까지나 그때 그때, 필요한 library, package 설치 편의를 위한 수준이라고 볼 수 있을 것 같습니다. 리눅스 명령어에서 익숙한 분들은 당연히 경로 이동을 할때 cd를 쓸 텐데, !cd는 먹히지 않고, os를 import하신 뒤에, os.chdir()로 이동해야 합니다. 빈 폴더를 하나 만들고 그 안에 짧은 txt파일을 만들어 확인해 봅시다. Cell. 1 123456import os!mkdir testos.chdir('test')!pwd# 명령어는 못찾는다고 나오지만 생성은 합니다 :)!'Sample text!' > sample.txt Output: 123/content/test /bin/bash: Sample text!: command not found sample.txt Cell. 2 1234# 목록에는 파일이 확인되지만, !ls# cat은 작동하지 않습니다. !cat sample.txt Output: 1sample.txt Colab.에서 쓸 수 있는 GPU 😉 먼저 수정 - 노트 설정 에 들어가셔서, 하드웨어 가속기 를 None에서 GPU로 설정합니다. 일단 필요한 Util을 다운을 받고, 1234!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi!pip install gputil!pip install psutil!pip install humanize Import 해줍시다. 1234import psutilimport humanizeimport osimport GPUtil as GPU GPU는 하나만 허락해 줍니다만… 갓글님의 그 하나는 에이스입니다. (2020.01.13 update) 현재. 무려 Tesla P100-PCIE-16GB. 😭 개인 실험용으로는 충분한 것 같습니다. 123456GPUs = GPU.getGPUs()gpu = GPUs[0]print(gpu.__dict__[\"name\"])print(\"The number of GPUs: {}\".format(len(GPUs)))print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil* 100, gpu.memoryTotal)) Output: 123Tesla P100-PCIE-16GB The number of GPUs: 1GPU RAM Free: 16280MB | Used: 0MB | Util 0% | Total 16280MB 참고사항 안타깝지만, Colab은 Docker기반의 Container로 실행되기 때문에 한 번에 12시간까지만 사용 할 수 있습니다. 그래서, 작업 하시던 Colab을 껏다가 잠시 후 다시 시작하면 새로운 docker container를 받기 때문에 새로운 인스턴스 위에서 시작할 때마다 설치가 필요한 패키지들을 매번 다시 설치해야 합니다. 그 다음 부터는 Jupyter notebook을 사용하듯 사용하면 됩니다. Local file Upload! 😀 Local 파일을 업로드하는 것은 아래와 같이 좌측의 화살표 버튼을 눌러 탭을 여신 뒤에, 파일 탭을 누르시면 됩니다. 😃 그 다음 업로드 버튼을 누르시고 올리실 파일을 선택하시면 업로드가 잘 됐다는 것을 파일 목록에서 확인하실 수 있습니다. 이미지를 업로드 하시면 알림창 하나를 보실 수 있으실텐데, 로컬에서 가져온 업로드한 데이터들은 instance가 바뀌면(= 새로 시작하면) 없어진다는 말이니, 참고하세요. 아래의 코드를 실행해보면, 업로드한 그림을 확인해 볼 수 있습니다. 12345678import ioimport matplotlib.pyplot as pltfrom PIL import Imageos.chdir('..')image = Image.open('./person-walking.png')plt.imshow(image) 사실 이렇게 이미지를 올리면, upload 하고자 하는 데이터가 많을 수록 업로드 속도도 느릴 뿐만 아니라 브라우저를 다시 시작할 때마다, 또 업로드를 해야합니다. 잠깐 확인하는 용도로는 상관이 없겠지만요 😃 일괄적으로 Colab을 이용하되, 같은 데이터를 매번 사용해야하는 경우에는 개인 Google Drive에 업로드 한 다음, 그 directory 안에서 작업을 하면 더 편합니다. Google Drive를 Mount 하는 방법을 알아보도록 하겠습니다. 😃 Mount your Google Drive Google Drive를 마운트하는 것도, 이전에 비해서 더 직관적으로 편하게 되었습니다. 아래와 같이, 드라이브 마운트 버튼을 눌리면 갑자기 코드가 하나 생기고, 그 셀을 실행하게되면 나오는 URL을 클릭하시고 연결하고 싶은 Google Drive를 가지고 있는 구글 아이디로 인증을 거칩니다. 인증을 마치면 아래와 같은 키를 하나 생성해줍니다. 그 키를 복사에서 Colab으로 돌아가, Enter your authorization code: 부분에 입력해줍니다. 그러면 아래와 같이 본인의 Google Drive가 Mount된 것을 확인하실 수 있습니다. Google Cloud Storage 개인 실험은 위와 같이 로컬데이터 업로드나 구글 드라이브를 사용하겠지만, 단체의 프로젝트라고 하면, Google Storage와 같은 데이터 저장소를 공유해야합니다. 이번에는 그 방법입니다. Google Cloud Storage(이하 GCS)와 함께 Colaboratory를 사용하려면 Google Cloud 프로젝트를 만들거나 기존 프로젝트를 사용해야 합니다. 저는 아래와 같이 colab-test이라는 이름의 프로젝트를 만들었고 프로젝트 ID는 colab-test-265006이네요. 만들기 버튼을 눌러서 프로젝트를 만들어줍니다. 아래의 코드를 실행해서 인증을 통해, GCS에 접근합니다. 12from google.colab import authauth.authenticate_user() 그러면 Google Drive를 Mount할 때와 똑같은 인증 과정을 거치게 됩니다. 그 뒤로는 Command-line Utility인 gsutil 혹은 Python API 를 통해 GCS에 접근할 수 있습니다. 먼저 GCS에 파일을 업로드 하는 방법부터 알아보겠습니다. 데이터를 넣어놔야, Colab에서 접근해서 그 데이터를 사용할 수 있을테니까요. Upload data into the GCS Bucket gsutil이나 PythonAPI를 사용해서 GCS에 Bucket을 만들고 데이터를 업로드 할 수 있습니다. 하지만, 이 포스팅에서는 브라우저를 사용해서 간단하게 Bucket을 생성하고, Drag & Drop으로 데이터를 업로드 하도록 하겠습니다. Creat a bucket 생성한 프로젝트를 선택하시고, 좌상단에 위치한 탐색 메뉴를 클릭하셔서 아래와 같이 많은 제품들 중 저장소 카테고리 아래에 있는 Storage를 선택합니다. 그 다음은 버켓을 만들어 봅니다. 버킷 생성 , 혹은 버킷 만들기를 눌러 버킷을 생성해봅니다. 버킷의 이름, 데이터 저장 위치, 기본 스토리지, 액세스 제어 방법을 선택하는 옵션이 나오는데 필자의 경우, 이름은 colab-sample, 데이터 저장 위치는 Multi-region유형으로 asia, 클래스는 Standard, 객체 접근 방식은 세분화된 엑세스 제어로 하였고, 고급설정은 암호화 Google 관리 키인 상태에서 나머지는 아무것도 건드리지 않았습니다. 그렇게 아래와 같이 Bucket이 생성되었습니다. Uplaod data 그럼 데이터를 Drag & Drop이나 버튼을 눌러서 Upload 합니다. Upload가 잘 된 것을 확인한 후, 창을 닫아도 좋습니다. Download using gsutil 먼저 gcloud를 사용해서 생성한 프로젝트를 설정해줍니다. 1!gcloud config set project colab-test-265006 Output: 1Updated property [core/project]. 그 다음, 해당 버킷에 접근해서 업로드했던 파일을 다운로드 해봅니다. !gsutil cp gs://다음에는 사용하는 bucket의 이름부터 다운로드하고자 하는(정확하게는 cp 복사) 파일에 대한 절대경로를 명시하시고, 그 다음 다운로드 받을 위치를 명시해주면 됩니다. 저는 똑같은 파일을 위의 Local Upload에서 사용해서, 이름을 person-walking2.png로 하였습니다. 1!gsutil cp gs://colab-sample/person-walking.png ./person-walking2.png Output: 123Copying gs://colab-sample/person-walking.png.../ [1 files][ 2.1 KiB/ 2.1 KiB] Operation completed over 1 objects/2.1 KiB. 다운로드가 잘 됐는지 이미지를 열어서 확인해봅니다. 12image = Image.open('./person-walking2.png')plt.imshow(image) Download using Python API 먼저 서비스 클라이언트를 만듭니다. 12 from googleapiclient.discovery import buildgcs_service = build('storage', 'v1') 파일을 다운로드 합니다. gcs_service.objects().get_media()함수에 매개변수로, bucket 부분에 접근하고자 하는 Bucket의 이름은, object에는 해당 파일의 절대경로를 적어 주시면 됩니다. 이번에는 같은 파일을 person-walking3.png로 받도록 하겠습니다. 12345678910111213from apiclient.http import MediaIoBaseDownloadwith open('./person-walking3.png', 'wb') as f: request = gcs_service.objects().get_media(bucket='colab-sample', object='person-walking.png') media = MediaIoBaseDownload(f, request) done = False while not done: # _ is a placeholder for a progress object that we ignore. # (Our file is small, so we skip reporting progress.) _, done = media.next_chunk()print('Download complete') Output: 1Download complete 다운이 잘 됐는지 확인해봅니다. 12image = Image.open('./person-walking3.png')plt.imshow(image) References Jaeyeon Baek님의 블로그, 최건호님의 Google Colaboratory 사용법 Colab:로컬 파일, 드라이브, 스프레드시트, Cloud Storage document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2019/06/12/Colab_tutorial/"},{"title":"Tensorpack tutorial","text":"Tensorflow Wrapper인 Tensorpack을 소개합니다. Tensorflow, Tensorpack ModelDesc와 Trainer를 중심으로 Peter Cha Tensorpack을 공부하면, 우선 모르는 것들 투성이다. 알고나면 너무 쓰기 편하지만, 처음 접할 때는 너무 많이 추상화 된 API에 '이게 tensorflow는 맞는지…'할 정도니까. 우선 이 튜토리얼을 보기 전, 필자의 tensorpack_tutorial.ipynb를 실행해 보길 바란다. 대략적인 dataflow는 데이터를 불러오는 부분으로 이해를 마쳤다고 생각을 하고, dataflow부분은 생략하고 설명을 진행하도록 하겠다. 이번에는 Model의 선언하게 될 때 상속받은 ModelDesc class와, 학습을 실행하는 Trainer들의 모태가 되는 TowerTrainer 에 대해 알아보고자 한다.tensorpack_tutorial.ipynb에서 설명에 해당하는 부분을 함께 찾아보면 이해에 도움이 더 될 것 같다. 이 Tutorial은 Tensorpack documentation을 참고해서 만들었다. 1. Class ModelDescBase Base class for a model description이다. ModelDesc는 ModelDescBase를 기반으로 만들어졌기 때문에, ModelDescBase를 먼저 설명한다. 1.1. build_graph(*args) 모든 symbolic graph (Model의 형태)를 Build한다. 이 함수가 뒤에서 설명할 TowerTrainer에서 tower function의 일부분이다. 그 다음 설명할 inputs()에서 정의된 input list에 맞는 tf.Tensor를 parameter로 받는다. 아무것도 리턴하지 않는다. 1.2. inputs() Model에서 input으로 받을 텐서들의 placeholder들을 정의하는 함수다. 후에 InputDesc로 변환될, tf.placeholder들을 return 한다. 1.3. get_inputs_desc 이름에서 알 수 있듯이, inputs()에서 정의된 모양대로 생긴 InputDesc를 list로 반환하는 함수다. 2. Class ModelDesc 주의사항: build_graph()를 꼭 cost를 return하도록 코딩해야 한다. 앞에서 설명한 ModelDescBase를 상속받은 터라, 위의 3가지는 함수는 내장하고 있다. 2.1. optimizer() tf.train.Optimizer를 여기에 선언해주고 Return하게끔 함수를 짜준다. 2.2. get_optimizer() optimizer()를 호출하면, 계속 새로 optimizer를 만들어서 생성하는데, 이 함수를 쓰면 이미 optimizer()를 통해 생긴 optimizer를 기록해 놓았다가 반환시켜준다. 3. Class TowerTrainer Tensorpack에서는, 우리가 흔히 말하는 Model을 계속 Tower라고 지칭한다.(왜 그런지 모르겠다.😶) 그래서 아래에서 나오는 TowerTrainer는 만든 모델을 학습을 시키는 Trainer고, 그 트레이너가 어떤 특징들을 가진 함수들을 들고 다니는지 이해하면 이해가 쉽다. 기본적으로 Tensorpack에 나오는 모든 Trainer들은 TowerTrainer의 subclass다. 이 개념이 그래서 궁극적으로는 모든 neural-network training을 가능하게 해준다. 3.1. get_predictor(input_names, output_names, device=0) Returns a callable predictor built under TowerContext(is_training=False). 이 함수가 호출되면, 가지고 있는 TowerContext(모델)가 training mode가 아닌 상태(is_training=False)로 돌려준다. 그러니까 Test data로 시험할 때만 부르는 함수. 그래서 이름도 predictor. Parameters: input_names (list), output_names (list), device (int) – build the predictor on device ‘/gpu:{device}’ or use -1 for ‘/cpu:0’. 파라미터로 들어가는 input, output이름은 모델 안에서 선언된 이름이 아니면 안 돌아가니까 조심. 3.2. inputs_desc Returns – list[InputDesc]: metainfo about the inputs to the tower. 말 그대로, 모델에 들어갈 Input의 크기와 같은 정보가 들어있는 list를 반환해준다. 3.3. tower_func Build Model. 이 친구가 실제 모델을 정의하고, Build할 수 있는 함수를 세팅하는 부분! ModelDesc interface로 정의된 model을 trainer로 돌려야 하는 상황이 자주 발생할 수 있는데, 이 때, ModelDesc에서 선언된 build_graph 함수가 이 역할을 대신해 줄 수 있다. 3.4. towers Returns – a TowerTensorHandles object, to access the tower handles by either indices or names. 모델 및 Train 전반에 걸쳐 관련된 변수들에 접근하고 싶을 때 사용한다! 그래서 이 함수는 Transfer learning을 할 때 유용할 거 같다. 이미 모델 그래프가 Set up이 끝난 뒤에만 이 함수는 호출될 수 있다. 각각의 layer와 attributes에 이 towers함수를 호출하면 접근할 수 있게 된다! 아래는 예시. 12# Access the conv1/output tensor in the first training towertrainer.towers.training()[0].get_tensor('conv1/output') 4. Class Trainer Base class for a trainer. 분명히 위에서 금방, \"기본적으로 Tensorpack에 나오는 모든 Trainer들은 TowerTrainer의 subclass다 \" 라고 했는데, 이 TowerTrainer가 상속을 받는 class가 있었으니, 이름하여 TowerTrainer보다 더 단순한 Trainer 다. 다른 TowerTrainer를 상속 받은 Trainer들을 사용할 때, 종종 TowerTrainer에서 본 적 없는 친구들이 나타나는데, 그 친구들이 Trainer의 것인 경우가 있다. 하지만, Trainer 고유 함수나 요소에 직접적으로 접근할 일이 별로 없어서 아래의 3가지 정도만 알고 있으면 될 것 같다. 나머지는 문서를 참고하자. 아래 1, 2번의 max_epoch과, steps_per_epoch은 TrainConfig에서 자주 만나는 키워드들인데, 이 친구들이 Trainer의 요소였다. 4.1. max_epoch Epoch은 몇 번 돌릴 것인지 4.2. steps_per_epoch 한 에폭당 steps은 총 몇 번인지. 4.3. register_callback(cb) Register callbacks to the trainer. It can only be called before Trainer.train(). Trainer가 모델을 돌릴 때마다(epoch이 진행 됨에 따라), 수행하게 될 부가적인 기능들을 Tensorpack에서는 callback이라고 부르고, 대표적인 callback으로 ModelSaver() 가 있다. 이 Callback을 명시적으로 전달하여 Trainer Object에 세팅할 수 있는 기능이다. 주로 모델을 튜닝할 때, 설정하면서 종종 쓰는 것을 코드 상에서 확인할 수 있다. 5. TowerContext TowerContext 는 Training과 Validation 혹은 Test시에 동작이 달라야 하는 BatchNorm이나, Dropout을 제어하기 위해서 만들어진 function이다. tensorpack_tutorial.ipynb에서는 이 친구를 찾아볼 수 없는데, SimpleTrainer 소스코드를 보니, 자체적으로 안에서 train/test time에 맞춰서 TrainTowerContext라는 것으로 조절하고 있기 때문이었다. 사용법은 간단하다. 1234567# trainingwith TowerContext('', is_training=True): # call any tensorpack layer# testwith TowerContext('name or empty', is_training=False): # build the graph again 그래서, 내가 세운 모델을 외부에서 사용하고 싶을 때, 즉, 나만의 Trainer를 새로 정의해서 train/test time때, 다르게 동작을 해야하는 상황이라면, TowerContext를 적절히 써서 분기시켜줘야 한다. 아래는 Tensorpack Github에서 제공하는 GANTrainer에서 실제로 TowerContext를 어떻게 설정해주는지 보여주는 예시다. 1234567891011class GANTrainer(TowerTrainer): def __init__(self, input, model): .. ... # Build the graph self.tower_func = TowerFuncWrapper(model.build_graph, inputs_desc) with TowerContext('', is_training=True): self.tower_func(*input.get_input_tensors()) opt = model.get_optimizer() ... Thank you 😃 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2018/08/18/Tensorpack-tutorial/"},{"title":"Keras tutorial (1)","text":"A Keras Usage with fashion MNIST Keras example using Colab Pytorch와 Tensorflow의 Wrapper인 Tensorpack만 써봤던 저는, 올해 처음으로 Keras를 사용해보게 되었습니다. 블록과 함께 하는 파이썬 딥러닝 케라스 라는 책으로 공부하면서 그 간결함에 놀랐고, 대충대충 이해하고 넘어갔던 개념들이 좋은 예시로 설명되어 있어, 그 내용들을 정리해보고자 포스팅을 하게 되었습니다. 이번 포스팅은 책의 Part 1, 케라스 시작하기와 Part 2. 딥러닝 개념잡기에 나오는 Keras 예제들을 Fashion MNIST 데이터로 재구성해 본 것입니다. 😬 책 내용 이외에 추가된 부분은, Colab의 Notebook으로 이번 포스팅이 구성이 되었다는 것이고, 여기에서 Colab Notebook으도 동일한 내용 확인하실 수 있습니다. 또, matplotlib의 plot 대신에, Tensorboardcolab을 사용하여서 plot 시각화를 하도록 방식을 바꿔보았습니다. 흔쾌히 블로그 포스팅을 허락해주신 저자 김태영님께 다시 한 번 감사를 표합니다. ☺️ Import packages 자, 시작해볼까요! 😎 1234567891011121314import osimport kerasimport tensorflow as tfimport keras.utils as utils import matplotlib.pyplot as pltfrom tqdm import tqdm_notebookfrom keras.datasets import fashion_mnistfrom keras.layers import Dense, Activationfrom keras.models import Sequential, load_model# remove error message from tensorflowtf.logging.set_verbosity(tf.logging.ERROR)%matplotlib inline Output: 1Using TensorFlow backend. keras.utils: 자주 사용되는 유용한 기능들 모음. 대표적으로 One-hot encoding을 해주는 to_categorical(), l1-l2 normalize를 가능하게 해주는 normalize() 등이 있다. keras.datasets: MNIST, Fashion MNIST, CIFAR10, CIFAR100, IMDB Movie reviews 긍정-부정 판별셋, Reuters 뉴스토픽 분류셋, Boston 부동산가격 dataset을 불러올 수 있다. keras.layers: Dense부터 CNN, Pooling, Padding, RNN 등등… 익숙한 딥러닝 layer들의 집합소. keras.models: keras 모델을 만드는 방법은 크게, Sequential과 Model - functional API를 사용하는 방법 2가지로 나뉜다. Pytorch랑 비슷하다. Sequential은 사용할 모델의 부품을 다 설정한 뒤 Input을 넣으면 한번에 레이어와 레이어 사이 설정을 세팅한 output에 맞게 맞춰주고, functional API인 Model을 사용하면 한땀한땀(?) 그 흐름을 구체적으로 설정할 수 있는 자유도를 가진다. 본 포스팅에서는 Sequential을 사용할 것이니, Model을 사용하는 예시는 여기에서 확인. Loading Data Fashion MNIST 1(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data() Output: 12345678Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz32768/29515 [=================================] - 0s 9us/stepDownloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz26427392/26421880 [==============================] - 5s 0us/stepDownloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz8192/5148 [===============================================] - 0s 0us/stepDownloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz4423680/4422102 [==============================] - 2s 1us/step Peeking the data 10개의 Fashion MNIST 데이터들을 시각화! 😏123456789101112class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']plt.figure(figsize=(8, 4))for i in range(10): plt.subplot(2, 5, i+1) plt.xticks([]) plt.yticks([]) plt.imshow(x_train[i], cmap=plt.cm.binary) plt.xlabel(class_names[y_train[i]]) plt.grid(False)plt.show() Preprocessing 50000 for Training, 10000 for Validation, 10000 for Test 123456789x_val = x_train[50000:]y_val = y_train[50000:]x_train = x_train[:50000]y_train = y_train[:50000]# preprocessing x_train = x_train.reshape(50000, 784).astype('float32') / 255.0x_val = x_val.reshape(10000, 784).astype('float32') / 255.0x_test = x_test.reshape(10000, 784).astype('float32') / 255.0 Label one-hot encoding (utils.to_categorical): keras.utils.to_categorical API 1234# label one-hot encoding.y_train = utils.to_categorical(y_train)y_val = utils.to_categorical(y_val)y_test = utils.to_categorical(y_test) Callbacks Tensorboard 띄우기 Early Stopping / 5번을 기다려도 성능이 나아지지 않을 경우 학습 중단 Model Checkpoint / val_loss가 가장 낮을 때만 저장 123456789101112131415161718from tensorboardcolab import *from keras.callbacks import EarlyStopping, ModelCheckpoint# 3. Tensorboard 세팅tbc=TensorBoardColab()# 4. Early Stopping early_stopping = EarlyStopping(patience=5) # 5. Model Checkpointpath = './model/'if not os.path.exists(path): os.mkdir(path) model_path = path + '{epoch:02d}-{val_loss:.4f}.h5'checkpoint = ModelCheckpoint(filepath = model_path, monitor = 'val_loss', verbose = 0, save_best_only = True) Output: 123Wait for 8 seconds...TensorBoard link:https://57207182.ngrok.io Create a model / Train / Test Sequential() - 모델 생성 add() - 모델 블럭끼우기 compile() - 모델 학습에 쓸 도구 세팅하기 fit() - 학습시키기 evaluate() - 평가하기 123456789101112131415161718192021222324252627282930313233343536373839# GPU 사용# with tf.device('/device:GPU:0'):# 1. 모델 구성model = Sequential()model.add(Dense(units=64, input_dim=28*28, activation='relu'))model.add(Dense(units=10, activation='softmax'))# 2. 모델 학습과정 설정하기 model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])# 3. 모델 학습시키기for epoch in tqdm_notebook(range(50)): hist = model.fit(x_train, y_train, epochs=1, batch_size=32, verbose=0, validation_data=(x_test, y_test), callbacks=[TensorBoardColabCallback(tbc), early_stopping, checkpoint]) # tensorboard lines tbc.save_value(\"fasion mnist\", \"train_acc\", epoch, hist.history['acc'][0]) tbc.save_value(\"fasion mnist\", \"val_acc\", epoch, hist.history['val_acc'][0]) tbc.save_value(\"fasion mnist\", \"train_loss\", epoch, hist.history['loss'][0]) tbc.save_value(\"fasion mnist\", \"val_loss\", epoch, hist.history['val_loss'][0]) tbc.flush_line(\"train_acc\") tbc.flush_line(\"val_acc\") tbc.flush_line(\"train_loss\") tbc.flush_line(\"val_loss\") if (epoch+1)%10 == 0: print('-----'*5) print(\"Epoch: {} | Loss: {:0.3f} | Acc: {:0.3f}\".format( epoch+1, hist.history['loss'][0], hist.history['acc'][0]))loss_and_metrics = model.evaluate(x_test, y_test, batch_size=32)tbc.close()print('-----'*10)print('\\nLoss and metrics: ' + str(loss_and_metrics)) Output: 12345678910111213141516HBox(children=(IntProgress(value=0, max=50), HTML(value='')))-------------------------Epoch: 10 | Loss: 0.393 | Acc: 0.864-------------------------Epoch: 20 | Loss: 0.342 | Acc: 0.879-------------------------Epoch: 30 | Loss: 0.312 | Acc: 0.890-------------------------Epoch: 40 | Loss: 0.288 | Acc: 0.898-------------------------Epoch: 50 | Loss: 0.270 | Acc: 0.90510000/10000 [==============================] - 1s 60us/step--------------------------------------------------Loss and metrics: [0.3537946595430374, 0.8761] Tensorboard Plot: Model Structure Visualization Model 구조 시각화하기: model.summary() & SVG() 😃 12345from IPython.display import SVGfrom keras.utils.vis_utils import model_to_dotprint(model.summary())SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg')) Output: 123456789101112_________________________________________________________________Layer (type) Output Shape Param # =================================================================dense_1 (Dense) (None, 64) 50240 _________________________________________________________________dense_2 (Dense) (None, 10) 650 =================================================================Total params: 50,890Trainable params: 50,890Non-trainable params: 0_________________________________________________________________None Save & Load Model Model 저장 & 불러오기 12 model.save('fashion_mnist_model.h5')model = load_model('fashion_mnist_model.h5') document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2019/06/12/keras_101/"},{"title":"Docker tutorial (1)","text":"Introduction to Docker. Docker 개념, 설치, 유용한 명령어 사용해보기. Docker? 😶 귀여운 고래 아이콘🐳으로 많은 사랑을 받고 있는 Docker에 대해 간단히 알아보도록 하겠습니다. Docker는 기존의 Virtual Machine들과 같이 Host OS 위에 Guest OS를 올리는 것이 아니라, 별도의 OS를 만들지 않고 단순히 프로세스만 격리시켜서 동작하는 방식입니다. 그래서 훨씬 더 빠르게 가상환경을 즐길 수 있게 해줍니다! 그래서 CPU나 메모리는 프로세스가 필요한 만큼만 사용하기 때문에 성능도 실제 Host에서 돌아가는 다른 Process들과 비교해서 큰 차이가 없답니다. 😉 Container 안에 다 있어요! Docker가 핫한 이유는 이게 다가 아니겠죠. 머신러닝을 위한 환경구축을 해보신 분들은 다들 공감하실 수 빡쳐 보신 적👿있으실 겁니다. 내가 그렇게 수많은 에러들과 StackOverflow를 헤매가며 설정한 그 환경 말이죠. (Anarconda + Tensorflow + Pytorch + R + R Studio + cuDNN + 온갖 Python Packages + 등등…) 이제 Docker를 쓰면, 그렇게 만든 환경을 Docker계의 Github인 Docker Hub에 Image화 한 뒤 올리면, 어디서나 받아서 쓸 수 있습니다. 또한 남들이 열심히 만들어서 공유해 준, 인성 거의 산타🎅 환경을 받아 편하게 쓸 수도 있습니다. 너무 좋죠? 그렇게 공유하는 (Github의 Private Repository처럼 Docker Hub에도 Private계정을 제공합니다.) 파일을 Image라고 하고, 그 Image를 받아서 생성하게 되는 하나하나의 Process 가상환경을 우리는 Container라고 부릅니다. Docker라는 이름에 걸맞게 항만에서 수많은 컨테이너들이 공유되는 환경이 Docker Hub이라고 생각하시면 되겠네요. 그럼 이제 Docker를 설치해 보실까요! Installation Linux Ubuntu 환경만 설명하자면, 아래와 같은 명령어로 간편하게 설치 하실 수 있습니다. 명령어 위에 주석처리된 부분은 2019년 5월 현재 Ubuntu 환경의 Prerequisites이니 참고하세요 😃 12345678# OS requirements# To install Docker CE, you need the 64-bit version of one of these Ubuntu versions:# Cosmic 18.10# Bionic 18.04 (LTS)# Xenial 16.04 (LTS)$ sudo apt-get update$ sudo apt-get install docker-ce docker-ce-cli containerd.io 다른 Linux 환경은 여기를 참조하세요. Mac & Windows Mac과 Windows 환경은 다음과 같은 기준에 따라 설치 방법이 두 가지로 나누어 집니다. Windows: Windows 10 Pro 이상 모델 (10 Home 안됨) → Docker for Windows Mac: OS가 Sierra 10.12 혹은 그 이상 → Docker for Mac Windows & Mac: 1번 조건을 만족하지 못하는 경우 → Docker ToolBox 참고 사항 기본적으로 Docker에 회원가입을 하시고 ID를 생성하셔야 설치 파일을 다운을 받을 수 있습니다. Windows에서는 [작업 관리자 > 성능] 에 들어가 \"가상화 : 사용\" 을 확인을 하시고 안되어있다면, 활성화를 해줘야 합니다. 개인적인 경험상, Windows 환경에서 1번의 환경이 충족되어 Docker for Windows를 설치하였는데도, Linux Container로 Switch를 못한다 던가 하는 에러가 발생되어 사용이 힘들 때 →\\to→ 그냥 2번으로 Docker ToolBox를 설치하고, Docker Quickstart Terminal을 사용, Docker를 실행하기도 했습니다. 되기만하면 장땡이니까요 설치 확인 docker version 이라고 Terminal에 쳤을 때, 아래와 같이 version 정보가 잘 나온다면 설치가 잘 된 것입니다. 😃 123456789101112131415161718Client: Docker Engine - Community Version: 18.09.2 API version: 1.39 Go version: go1.10.8 Git commit: 6247962 Built: Sun Feb 10 04:12:39 2019 OS/Arch: darwin/amd64 Experimental: falseServer: Docker Engine - Community Engine: Version: 18.09.2 API version: 1.39 (minimum version 1.12) Go version: go1.10.6 Git commit: 6247962 Built: Sun Feb 10 04:13:06 2019 OS/Arch: linux/amd64 Experimental: true Docker version을 확인해보니, Client-Server로 나뉘어져 있는 것을 확인할 수 있습니다. Docker가 실제로 어떻게 동작하는지 볼 수 있는 부분입니다. 😉 사용자가 Docker 명령을 내리면 Client에서 기본적으로 Docker Server를 바라보고 있기 때문에, 사용자는 바로 명령만 내린 것 같지만, 실제로는 Server가 Client로 부터 전송을 받아, 처리한 결과를 다시 Client에게 돌려주고 있는 것이죠. 자, 이제 설치를 잘 마쳤으면 본격적으로 Docker를 사용해 볼까요? Practice 아래와 같은 순서로 실습을 하면서 필요한 명령어들을 정리해 보겠습니다 😉 1231. Anaconda가 설치되어 있는 Ubuntu Image를 다운받기. 2. 그 이미지로 생성한 컨테이너에서 Jupyter notebook을 띄워놓기.3. Host Browser로 접근해서 사용. 1. 이미지 다운로드 하기 (pull, images) Docker Hub에 접속해서 jupyter-python3로 검색을 하니, 천만 다운로드에 빛나는 이미지가 나오네요. 아래의 명령어를 입력하면, Anaconda3를 품은 Ubuntu 18.04 Image를 다운로드 받게 됩니다. 다운로드에 약간의 시간이 소요됩니다. 1$ docker pull civisanalytics/civis-jupyter-python3 보시다시피, 기본적으로 Docker 명령어는 docker로 시작하고, Git을 쓰신 분들은 익숙한 단어이실 pull이라는 명령어로 원하는 이미지를 가져 올 수 있습니다. 가져온 이미지는 명령어 1$ docker images 를 통해 확인할 수 있습니다. 2. 컨테이너 목록 조회 & 삭제 (ps, rm, stop) 컨테이너 실습에 앞서서, 깔끔한 진행을 위해 현재 Docker 설치시에 Default로 가지고 있는 컨테이너들을 한 번 싹 비우고 시작하도록 하겠습니다😏. 아래 명령어를 입력하시면 현재 멈춰있는 컨테이너까지 포함한 목록들을 확인할 수 있습니다. 1$ docker ps -a docker ps는 현재 동작하고 있는 컨테이너를 모든 컨테이너들을 조회하는 명령어 입니다. -a 옵션은 멈춘 컨테이너까지 모두 조회합니다. 아직 컨테이너 생성은 하지도 않는데 목록에 멈춰있는 다른 컨테이너들이 보이실 것입니다. 다 지워보도록 하겠습니다.😈 1$ docker rm $(docker ps -a -q) docker rm는 멈춰있는 컨테이너를 삭제하는 명령이고, -q 옵션은, PORT, STATUS, NAME등의 정보는 제외하고 컨테이너 ID만 확인하게 해주는 옵션입니다. 따라서 바로 위에서 배웠던 docker ps -a -q의 결과로 나오는 ID에 해당하는 모든 컨테이너들을 삭제해라는 명령이 됩니다. 컨테이너가 아니라 이미지를 삭제하고 싶으면 docker rmi로 i만 추가해주세요! 혹시 컨테이너가 정지 상태가 아닌데 docker rm container_ID를 실행하시면 해당 컨테이너는 삭제되지 않습니다. 먼저 docker stop container_ID로 삭제하고자 하는 컨테이너의 동작을 멈춘 뒤에 실행해야합니다. 3. 컨테이너 생성 & 포트 설정 & 이름지어주기 (run -p, --name) 이제 우리가 사용할 우분투 컨테이너를 생성하고 접속해보도록 하겠습니다. 123$ docker run -it --name docker101 \\ -p 8888:8888 civisanalytics/civis-jupyter-python3 \\ /bin/bash run명령어로, 가지고 이미지를 실행하라고 하면서 -it 옵션을 주어서(-ti도 같습니다) 터미널(t)에 입력(i) 을 받을 수 있게 해줍니다. -i와 -t옵션을 함께 쓴 거죠. 제일 마지막에 터미널의 경로를 /bin/bash 로 전달해줍니다. --name 옵션을 줘서 원하는 이름을 부여할 수도 있습니다. 이름을 따로 주지 않으면 유명한 과학자의 이름에 수식어를 붙여서 랜덤생성합니다.(장영실도 포함되어있다고 하네요!) -p 옵션은 포워딩 해줄 port번호를 의미하는데요, 앞에 8888은 호스트 port, 뒤의 8888은 컨테이너 port를 의미합니다. 접속이 잘 되었다면 아래와 같이 sudo mode로 root권한을 가진 상태의 bash를 쓸 수 있게 됐습니다! 간단한 ls -a이나 pwd같은 명령어들로 가상환경 Ubuntu를 느껴보세요.😎. apt update로 우분투를 업데이트 해줍니다. 1$ [work] # apt update 사실 run명령어는 실행하라고 한 이미지가 로컬에 없을 경우, 가장 최신 버전으로 다운을 받기 때문에, 이미지를 다운받으면서 동시에 컨테이너를 만드는 명령어이기도 합니다. 하지만, 한 번 다운 받고 난 뒤에는 해당 이미지가 업데이트가 되어도 가지고 있는 이미지만 사용하기 때문에, 가장 최신 버전을 다운 받게 해주는 pull도 메리트가 있는 것이죠. 다시 본론으로 돌아와, 접속한 컨테이너 우분투 환경에서 python을 입력해보면, Acaconda3-python 3.7이 설치 되어있음을 확인할 수 있습니다. 이로써 Anaconda를 설치한 적도 없지만, 사용할 수 있는 환경을 Get하게 됐습니다.😊 12345[work] # pythonPython 3.7.1 | packaged by conda-forge | (default, Feb 18 2019, 01:42:00) [GCC 7.3.0] :: Anaconda, Inc. on linuxType \"help\", \"copyright\", \"credits\" or \"license\" for more information.>>> 이 상태에서 jupyter notebook을 바로 실행을 시킬 수도 있습니다. 하지만 다른 명령어들을 더 익히기 위해 exit로 일단 나오도록 합니다. 4. 컨테이너 시작, 명령어 실행시키기 (start, exec -d) Jupyter notebook을 실행시키도록 해보겠습니다! 먼저 docker ps -a을 입력해서, 방금 나왔던 컨테이너의 이름을 확인해봅니다. exit명령어로 환경을 나오면, 기본적으로 컨테이너는 Stopped(Exited)인 상태입니다. 123chayesol-ui-MacBook-Pro:Blog chayesol$ docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES57ca32782e7e civisanalytics/civis-jupyter-python3 \"/tini -- /bin/bash\" 9 minutes ago Exited (0) 6 minutes ago docker101 제가 생성할 때 이름으로 주었던 docker101이 이름으로 잘 보이네요! 이 컨테이너는 지금 Stopped 상태(Exited)이기 때문에 새로 동작을 시켜 줘야합니다. docker start명령어로 다시 작동시킨 뒤, Jupyter notebook을 실행합니다. 123$ docker start docker101$ docker exec docker101 jupyter notebook \\ --ip=0.0.0.0 --port=8888 --allow-root exec 명령어는 해당 컨테이너 이름(or ID) 뒤에 나열된 명령어들을 실행하게 해줍니다. ip는 0.0.0.0으로 로컬호스트를 지칭하고, 포트는 우리가 컨테이너 생성할 때 작성한 8888로 주었습니다. 실행 후, 나오는 token값을 복사한 뒤, 호스트의 브라우저 창에 localhost:8888을 입력하시면 token을 적게 되어 있습니다. 그 결과, 아래와 같이 우리는 컨테이너에 있는 Jupyter notebook을 도커를 통해 로컬호스트에서 사용할 수 있게 되었습니다!😆 exec -d? exec명령을 줄 때, -d옵션을 추가하면, detached mode, 즉 백그라운드에서 컨테이너가 실행되도록 할 수 있습니다. (-d옵션은 run에도 쓸 수 있습니다.) 우리 예시에서는 보안상 Jupyter notebook의 token을 출력받고 사용해야하기 때문에 -d옵션을 사용하지 않았습니다. 보안상 권장사항은 아니지만, exec -d 옵션을 사용해서 편하고 빠르게, ‘귀찮은 token입력 없이 jupyter notebook만 내가 띄우고 싶다!’ 하시면 아래와 같은 명령어를 사용하실 수 있습니다. 1234$ docker exec -d docker101 \\ jupyter notebook --NotebookApp.token='' \\ --ip=0.0.0.0 --port=8888 --allow-root$ 실행 후에는 아무 일도 없다는 듯이 그 다음 line을 출력하지만, localhost:8888로 접속하시면 똑같이 Jupyter notebook을 사용하실 수 있습니다. 5. 컨테이너 재접속 & 재시작 (attach, restart) Jupyter notebook은 성공적으로 띄웠으나, 컨테이너에 다시 접속해서 패키지를 더 설치하거나 환경을 세팅해줘야 할 경우, 재접속 하는 방법은 exec 를 사용하는 방법과 attach 를 사용하는 방법, 두 가지가 있습니다. 또 다른 Terminal을 띄우신 뒤에, 123$ docker exec -it container_name /bin/bashor$ docker attach container_name 둘 중 하나를 입력하시면 됩니다! 컨테이너를 재시작 해주고 싶은 경우는 docker restart 를 사용합니다. 1$ docker restart contatiner_name 이상, Docker의 개념과 설치, 그리고 간단한 기본 명령어들에 대해 알아보았습니다. 다음 Hi, Docker!:) (2)에서는 컨테이너와 로컬저장소 연결하기 와 내 컨테이너 이미지로 업로드하기 라는 두가지 주제를 다뤄보도록 하겠습니다. 감사합니다 😌 References 이 글은 아래 두 글을 기반으로 작성되었습니다😄. 더 자세한 정보는 ↓ 초보를 위한 도커 안내서 - 도커란 무엇인가? 초보를 위한 도커 안내서 - 설치하고 컨테이너 실행하기 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2019/04/24/docker-intro/"},{"title":"L1 & L2","text":"L1, L2 loss, regularization, and norm. Machine Learning을 공부하기 시작하면, 꼭 마주치는 L1, L2. L1, L2 loss라고도 하고 L1, L2 Regularization이라고도 하는데, 명확히 그 각각의 개념과 그 차이를 짚고 넘어가려, Loss로써 쓰일 때와 Regularization으로써 쓰일 때를 정리해 보았다. As an Error Function 모델의 Loss, 즉 Cost를 구하는 방법으로 사용하겠다 하면 L1, L2 loss function은 아래와 같은 식을 사용한다. L1 loss L1 loss부터 살펴보면, 식에서 보는 것과 같이 실제 값(yiy_iyi​)과, 예측값(f(xi)f(x_i)f(xi​))의 그 차이값에 절대값을 취해, 그 오차 합을 최소화하는 방향으로 loss를 구한다. Least Absolute Deviations라고 하고 줄여서, LAD라고 한다. L=∑i=1n∣yi−f(xi)∣L = \\sum\\limits_{i=1}^{n}|y_i - f(x_i)| L=i=1∑n​∣yi​−f(xi​)∣ L2 loss L2 loss는 MSE (Mean Sqaured Error)를 안다면 아주 익숙한 개념으로, target value인 실제값(yiy_iyi​)과 예측값(f(xi)f(x_i)f(xi​)) 사이의 오차를 제곱한 값들을 다 합해서 Loss로 산정한다. Least squares error라고 하고, 줄여서 LSE라고 한다. L=∑i=1n(yi−f(xi))2L = \\sum\\limits_{i=1}^n(y_i - f(x_i))^2 L=i=1∑n​(yi​−f(xi​))2 L1 loss와 L2 loss 비교 L1 loss와 L2 loss는 아래와 같은 차이점을 가지고 있다. 1. Robustness: L1>L2L1 > L2 L1>L2 여기서 말하는 Robustness는 outlier, 즉 이상치가 등장했을 때, loss function 얼마나 영향을 받는지를 뜻하는 용어다. L2 loss는 outlier의 정도가 심하면 심할 수록, 직관적으로 제곱을 하기 때문에 그 계산 값이 L1보다는 더 큰 수치로 작용을 할 수 밖에 없기 때문에 Roubustness에서 L1보다 더 그 성질이 작다고 말할 수 있다. 그렇기 때문에, outliers가 효과적으로 적당히 무시되길 원한다면, 비교적 이상치의 영향력을 작게 받는 L1 loss를, 반대로, 이상치의 등장에 주의 깊게 주목을 해야할 필요가 있는 경우라면 L2 loss를 취하는 선택을 할 수 있겠다. 2. Stability: L1","link":"/2018/09/24/l1l2/"},{"title":"Docker tutorial (3)","text":"Docker Container에 SSH 접속 현재 내 컨테이너를 이미지로 저장하기 Docker Tutorial, 세 번째⭐️입니다! 이번 포스팅에서는 이전 글에서 노출시킨 22 port 를 사용, 컨테이너 환경으로 외부에서 SSH 접속을 하는 방법에 대해 다루고, SSH Setting이 끝난 컨테이너 자체를 이미지로 만드는 방법을 알아보도록 하겠습니다. 최근들어 많은 회사나 단체, 개인들이 AWS나 Google Cloud Platform 등을 사용하면서 GPU 및 TPU Server를 사용하고 있죠. 그래서 당연히 AWS나 Google Cloud에서도 Docker를 사용할 수 있게 됐고, 자연스럽게 언제 어디서든 무거운 계산이나 학습은 Cloud나 원격 서버에서 처리하고 실제 사용자는 자유롭게 접속하고 해당 머신을 사용할 수 있게 되었습니다. Digital Nomad🐑가 가까워지고 있습니다. 혹은 Digital Slave… 이런 현실에 맞춰, SSH로 내 Deocker Container에 접속하는 법 정도는 아는 것이 좋겠죠? 물론 키와 패스워드 관리 및 보안상의 이유로 일각에서는 SSH로 컨테이너에 접속할 수 있게 하는 것을 우려하지만, 개인적인 학습이나 가벼운 실험 용도로만 사용한다고 했을 때는 효율적이라고 생각해 포스팅을 하기로 했습니다. SSH Server in Container 1. Container 생성 (--cap-add) 먼저 SSH을 사용할 수 있게 이미지로 부터 컨테이너를 생성할 때, 아래와 같은 옵션을 함께 줍니다. 이미지는 Hi, Docker! 😃 (2)에서 생성한 docker101이미지를 사용합니다. 1$ docker run -d --name ssh_container --cap-add=NET_ADMIN --cap-add=NET_RAW -p 8888:8888 -p 22022:22 docker101 --cap-add=NET_ADMIN에서 --cap-add 옵션은, 컨테이너에게 특정한 cgroups 을 사용하게 해주는 것으로, 여기서는 admin의 network를 사용하겠다는 의미로 NET_ADMIN 을, admin의 iptables를 그대로 사용하겠다는 의미 NET_RAW 를 변수로 줍니다. cgroups: control groups의 약자 - 프로세스들의 자원의 사용(CPU, 메모리, 디스크 입출력, 네트워크 등)을 제한하고 격리시키는 리눅스 커널 기능 (feat. wikipedia) 저는 -p 22022:22 로, 컨테이너가 SSH 접속을 허용할 22번 포트를 Host의 22022와 연결시켜 주겠습니다. 2. SSH, ufw 설치 이렇게 생성한 컨테이너에 이제 SSH 접속을 할 수 있도록 하려면, 손봐줄 것이 많기때문에…👊 백그라운드에서 돌아가고 있는 ssh_container에 exec로 접속합니다. 1$ docker exec -it ssh_container /bin/bash 그리고 SSH와, 리눅스에서 방화벽을 관리해주는 ufw를 설치합니다. 1$ apt-get install ssh ufw -y 아래와 같이 sshd의 위치가 잘 나온다면 설치가 잘 된 것입니다! 12[work] # which sshd/usr/sbin/sshd 3. 비밀번호 설정 (sshd_config, passwd root) SSH 접속을 할 때 물어볼 비밀번호를 설정하려면, 먼저 /etc/ssh/ 아래에 있는 sshd_config파일을 좀 수정해야합니다. 1$ vim /etc/ssh/sshd_config 를 보시면, 123456789# $OpenBSD: sshd_config,v 1.101 2017/03/14 07:19:07 djm Exp $# This is the sshd server system-wide configuration file. See# sshd_config(5) for more information.# 어쩌구저쩌구......#LoginGraceTime 2m#PermitRootLogin prohibit-password#StrictModes yes... 라고 뜰 텐데요, 여기서 아래와 같이 #PermitRootLogin prohibit-password 부분의 주석을 해제하고, prohibit-password를 yes로 바꿔줍니다. 12345...#LoginGraceTime 2mPermitRootLogin yes#StrictModes yes... 그렇게 저장을 하신 뒤에, 다시 bash로 돌아와 passwd root 을 입력해서 비밀번호를 설정해주면 끝입니다! 1234[work] # passwd rootEnter new UNIX password:Retype new UNIX password: passwd: password updated successfully 4. ufw 세팅 etc/ufw/아래에 있는 ufw.conf파일에서 ENABLED=no를, 1$ vim /etc/ufw/ufw.conf 1234# /etc/ufw/ufw.conf...ENABLED=no... ENABLED=yes로 👇 바꿔줍니다. 123...ENABLED=yes... 그 뒤에, bash로 돌아와서 ufw enable 로 활성화를 해주고, 12[work] # ufw enableFirewall is active and enabled on system startup 이제 실제로 22번 port를 방화벽에서 허락할 수 있게 ufw allow 를 사용합니다. 123[work] # ufw allow 22/tcpRule addedRule added (v6) 5. SSH server 시작 수고하셨습니다. 이제 귀찮은 일은 다 끝났습니다. service ssh start로 ssh를 시작하고, 12[work] # service ssh start * Starting OpenBSD Secure Shell server sshd [ OK ] service ssh status로 동작을 확인하면 끝! 12[work] # service ssh status * sshd is running SSH Client 자, 그럼 열심히 세팅한 컨테이너로 이제 접속을 해봐야겠죠?😎 당연하지만 기본 개념은 Port forwarding입니다. Host의 22022번 port가 ssh_container의 22번 port와 연결되어있기 때문에, 기본적으로 외부에서 Docker 컨테이너로 접속을 하려면 Docker가 실행되고 있는 Host 머신을 통해서 연결을 해야합니다. 먼저 도커가 실행되고 있는 제 PC는 맥북인 관계로 ifconfig를 사용해서 제 컴퓨터의 IP가 172.16.8.236라는 것을 알아냈습니다. (linux: ifconfig - window: ipconfig) 따라서, ssh 명령어도 아래와 같습니다. 1$ ssh -p 22022 root@172.16.8.236 그렇다면 이 명령을 내리는 의미는 아래의 그림과 같습니다. 아래와 같이 접속을 허용하겠냐는 물음에 답하면 비밀번호를 물어보고, 12345The authenticity of host '[172.16.8.236]:22022 ([172.16.8.236]:22022)' can't be established.ECDSA key fingerprint is SHA256:iMDl7X79mKNZEA5oadtMr2zQdJhJ4n2toAeJ58o9Tsg.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added '[172.16.8.236]:22022' (ECDSA) to the list of known hosts.root@172.16.8.236's password: 비밀번호를 입력하면 드디어 ssh_container 안으로 입성(?)하게 됩니다! 123456789101112131415161718Welcome to Ubuntu 18.04.2 LTS (GNU/Linux 4.9.125-linuxkit x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantageThis system has been minimized by removing packages and content that arenot required on a system that users do not log into.To restore this content, you can run the 'unminimize' command.The programs included with the Ubuntu system are free software;the exact distribution terms for each program are described in theindividual files in /usr/share/doc/*/copyright.Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted byapplicable law. [~] # ssh를 사용해서 컨테이너 안으로 들어왔습니다! work directory안에 반가운 Data폴더도 그대로 잘 있네요. 이제 곧 Digital Nomad🐑가 될 것같은 기분 은 희망사항 입니다. 1234[~] # cd work[work] # ls -a. .. .empty Data[work] # 이 예시는 내부망이라서, 외부에서 원격접속은 못하잖아요? 맞습니다. 잘 아시겠지만, 이번 포스팅에서 예시로 들었던 IP 172.16.8.236은 내부망(같은 공유기를 쓰는, 혹은 같은 사내망)입니다. 따라서, 때로는 192.xx.xxx.xx가 될 수도 있죠. 같은 내부망뿐만 아니라, 진정으로 인터넷을 통해 어디서나 접근을 가능하게 하고 싶다면 저 IP는 Host가 부여받은 Public IP인 external IP이어야 합니다. AWS나 GCP에서 Docker 컨테이너를 사용하고 계신다면, 사용하고 계신 Cloud Machine에 접근할 수 있는 IP를 알면 되는 것이고, 자체 사설 서버를 운영하신다고 해도 마찬가지 입니다. 해당 Host 머신이 가지고 있는 Static IP, 고정IP가 되겠죠. 다만 각각의 환경에 따라 ssh 접속을 허용하기 위한 SSH Server측의 방화벽 및 구체적인 세팅을 다 설명하기에는 이번 포스팅의 논지를 흐릴 것 같아 사실 제가 힘들어서😞, 내부망 접속이라는 예시를 통해 ssh container 접속의 개념만 살펴보았습니다. 외부망을 통해 Docker Container에 접속을 하시려면, 필요에 따라 가지고 계신 환경에 따른 더 많은 정보가 필요합니다. Save My Container as an Image.(commit) 더 게으르고 싶은 개발자가 좋은 개발자기 때문에, 우리는 이 귀찮은 작업들을, 매번 컨테이너를 생성할 때마다 해줄 수는 없습니다! 이렇게 힘들게(?) 세팅한, SSH 접속이 되는 ssh_container를 image로 만들어서 Docker Hub에도 올리고, 편하게 사용하고 싶습니다. 이것을 가능하게 해주는 명령어가 commit 입니다. 말이 나온 김에 지금 당장 사용하도록 하겠습니다. ssh_container로 ssh_machine_learning이라는 이름으로 Image를 만듭니다. 12$ docker commit ssh_container ssh_machine_learningsha256:2e134384b1c846ee76a069db2aad0b2664610c195b9d8c1b03d79b9e3de74a0e 이미지가 잘 생성이 됐는지 확인을 해보니, 잘 생성이 되었네요! 참 쉽죠?😇🤘 1234$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEssh_machine_learning latest 4d9c11b16e4c 4 seconds ago 4.75GBdocker101 latest 71522855bd07 4 days ago 4.72GB Test 그럼 정말로 의도대로 잘 생성된 이미지인지 확인하기 위해, 기존의 ssh_container와, docker101 이미지를 삭제합니다. 123456$ docker stop ssh_containerssh_container$ docker rm ssh_containerssh_container$ docker rmi docker101Untagged: docker101:latest 그리고 생성한 ssh_machine_learning 이미지로 컨테이너를 생성하고 확인합니다. happy_cori라는 이름의 컨테이너로 생성이 되어있습니다. 12345$ docker run -d --cap-add=NET_ADMIN --cap-add=NET_RAW -p 8888:8888 -p 22022:22 ssh_machine_learninge3f1a95dc804632ed1802c17c90b44046f13301a261d06ebd48dcd7cb8f57447$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES292970ce1cff ssh_machine_learning \"/tini -- /bin/sh -c…\" 3 seconds ago Up 2 seconds 0.0.0.0:8888->8888/tcp, 0.0.0.0:22022->22/tcp happy_cori happy_cori로 exec 접속을 해서 SSH Server를 시작시킵니다. 123$ docker exec -it happy_cori /bin/bash[work] # service ssh start * Starting OpenBSD Secure Shell server sshd [ OK ] 위에 나왔던 같은 방식으로, SSH 접속을 해봅니다.팝콘준비🍟 Dealing with an Error 아, 지금 저처럼 같은 PC로 접속을 하신다면 아마도… 123456789 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!어쩌구저쵸구.....Add correct host key in /Users/chayesol/.ssh/known_hosts to get rid of this message.... 이런 멋진 친구를 먼저 만나게 될 것입니다. 그도 그럴만한 것이, 접속을 하려는 PC 입장에서는 똑같은 IP로 접속을 하려고 하는데, 전혀 다른 Server인데??라고 하면서 '뭐냐이거'라고 정색을 하는 것이죠. 이럴 경우, PC가 가지고 있떤 ssh-key를 아예 초기화시켜주는, 1$ ssh-keygen -R YOUR.IP.ADDR.ESS 를 실행한 뒤에 다시 하면 된다는 분들이 계시고, 저같은 경우 이 방법이 안먹혀서 에러 메세지 하단쯤에 친절히 적혀 있는 /Users/chayesol/.ssh/known_hosts을 파일을 열고 해당 IP를 지워버렸습니다. 그리고나서 다시, 도즈언🏊 하니 잘 되네요! 123456789101112131415161718192021222324$ ssh -p 22022 root@172.16.8.236The authenticity of host '[172.16.8.236]:22022 ([172.16.8.236]:22022)' can't be established.ECDSA key fingerprint is SHA256:XFFyjB4g0y4cJzd08AV2nDfxGxzcm8qBoS5n7VRc9fo.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added '[172.16.8.236]:22022' (ECDSA) to the list of known hosts.root@172.16.8.236's password: Welcome to Ubuntu 18.04.2 LTS (GNU/Linux 4.9.125-linuxkit x86_64) * Documentation: https://help.ubuntu.com * Management: https://landscape.canonical.com * Support: https://ubuntu.com/advantageThis system has been minimized by removing packages and content that arenot required on a system that users do not log into.To restore this content, you can run the 'unminimize' command.The programs included with the Ubuntu system are free software;the exact distribution terms for each program are described in theindividual files in /usr/share/doc/*/copyright.Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted byapplicable law.[~] # 이상으로 3번에 걸친 Docker Tutorial을 모두 마치도록 하겠습니다. 읽어주셔서 감사합니다. 🙇 References ruo91 - GitHub Gist docker의 ubuntu container에 ssh로 접속하기 HOW TO CREATE A DOCKER IMAGE FROM A CONTAINER document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2019/05/25/docker-103/"},{"title":"Pseudo Labelling","text":"준지도학습인 Pseudo Labeling에 대해 알아봅니다. Semi-Supervised Learning, Pseudo Labeling Pseudo-Label: The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks Peter Cha 이 포스팅은 SHUBHAM JAIN의 글을 레퍼런스로 사용하였습니다. 이번 포스팅에서 살펴 볼, Pseudo Label은 Semi-supervised Learning의 여러 방법 중 한 가지 입니다. 먼저, Semi Supervised Learning이 무엇인지 살펴보기로 하겠습니다. 1. Semi-Supervised Learning란? 우리는 보통 labelled data (supervised learning)와 unlabelled data(unsupervised learning) 양쪽 모두를 사용하는 방식의 학습을 Semi-Supervised Learning(이하 SSL)라고 정의합니다. 그러면, 어떠한 상황에 SSL이 필요할까요? 보통 다음과 같은 두 가지 상황이 있습니다. 만들고자 하는 Model에 쓸, Training data가 절대적으로 부족할 때. Large dataset이 될 수록 새로 생성되는 data들에 대한 Human annotation이 힘들고, 비쌀 때. 그래서, 보통은 '데이터가 부족할 때 쓰는 방법’으로만 알고 있지만, SSL을 아래와 같은 이유로 사용할 수도 있습니다. Labeled data만으로는 도달 할 수 있는 성능에 한계가 있을 때, Unlabelled data를 사용하여 전반적인 성능을 더 높이기 위해. 논문에서 말하고 있는 Semi Supervised Learning의 목적과, 이 글 후반부에 나올 필자의 Pseudo Label실험도 이 세 번째 이유에 대한 것입니다. Image from Dataiku hadoop summit. 2. Unlabeled data는 어떻게 도움이 될까? 그렇다면 Unlabeled data를 쓴다는 것은 어떤 이점이 있을까요? Labelled data 는 비싸고 얻기 힘들지만 unlabelled는 그 양이 풍부하고 값이 싸기 때문에 데이터 획득이 용이하다. Unlabeled data는 Model의 Decision boundary를 더 정확하게 잡아주는 역할을 해줌으로써, 모델의 Robustness를 향상시킨다. 두 번째 장점을 처음 읽으면 조금 추상적으로 다가올 수 있습니다. 그림으로 조금 더 설명하자면 아래와 같습니다. 단순히 2가지의 Class를 구별해야하는 모델의 경우, Labeled data만 가지고 그 Decision Boundary를 결정하게 되면 선형으로 그 Decision Boundary가 그어진다고 생각해 봅시다. 가지고 있는 Labeled Data에서 경계라고 판단할 만한 정보가 저것밖에 없기 때문에 가지고 있는 Data로 학습시킬 수 있는 모델의 한계라고도 생각할 수 있습니다. 다른 Unlabeled Data를 사용하면서 학습을 하면, 경계를 그을 때, 더 많은 Case들을 고려하면서 정교하게 경계를 긋기 시작합니다. 이는 자연스럽게, 나중에 모델이 Test set을 만났을 때, 혹은 처음보는 다른 Data를 만났을 때도, ‘당황하지 않고 대응할 수 있는’ 힘을 가지게 해준다고 이해할 수도 있습니다. 그래서 두 번째 장점에서 말하고 있는 모델의 Robustness(견고함)는 이를 뜻합니다. 우리가 잘 알고 있는 Overfitting도 이 Robustness의 정도가 낮아서 발생하는 것이라고 볼 수 있습니다. Images from A Tutorial on Graph based Semi-Supervised Learning Algorithms for NLP. 3. Pseudo Labeling을 소개합니다 😃 Pseudo Labelling의 개념은 아주 간단합니다. Labeled Data처럼 일일히 label을 하기보다, 이미 가지고 있는 Labeled data에 기반하여 대략적인 Label을 주는 것을 Pseudo Labelling이라고 합니다. 그래서 Pseudo Labeling의 순서는 다음과 같습니다. Labeled Data로 Model을 먼저 학습시킨다. 그렇게 학습된 모델을 사용하여, Unlabeled Data를 예측하고 그 결과를 Label로 사용하는 Pseudo-labeled data를 만든다. Pseudo-labeled data와 Labeled data를 모두 사용하여 다시 그 모델을 학습시킨다. Images from Data, what now? 3.1. Pseudo Labeled data Pseudo Label은 아래와 같은 식으로, 각각의 sample에 대해, 예측된 확률이 가장 높은 것으로 정합니다. 예측된 확률이 가장 높은 것을 Label로 선정한다고 했을 때, 제대로 학습을 마치지 못한 모델로 이 작업을 하였을 경우에는 상식적으로 더 학습을 저해하는 데이터를 만들 뿐입니다. 그래서 Pseudo Label은 학습을 Labeled data로 일정 수준까지 마친 뒤의, fine-tuning phase에 시행합니다. 3.2. Loss Function for Pseudo Labelling 논문에서 Pseudo Label로 학습을 할 때는, 다음과 같은 Loss function을 사용합니다. 적절한 수치의 α(t)\\alpha(t)α(t)가 네트워크 성능에 있어서 매우 중요한 요소입니다. α(t)\\alpha(t)α(t)가 너무 높으면 labeled data의 training을 방해할 것이고, 반대로 너무 그 수치가 작으면 unlabeled data의 유익을 취할 수 없게 되겠죠. 그래서 아래와 같이 점진적으로 그 비율을 늘려주는 식으로 α(t)\\alpha(t)α(t)를 조절하여서 local minima에 빠지는 문제를 점차 피할 수 있도록하여, Process를 최적화시킵니다. 4. Pseudo-Label로 성능향상이 왜 가능한거죠? 논문에서는 이 Pseudo Label이 왜 잘 동작하는지에 대해 아래와 같이 설명합니다. 4.1. Low-Density Separation between Classes. Cluster Assumption (Chapelle et al., 2005)에 의하면, Model의 전반적인 성능을 높이기 위해서는 Model의 Decision boundary는 Low-densidy regions에 위치해야한다고 말하고 있습니다. 즉, 앞에서 살펴본 Decision Boundary를 결정할 때, 그 경계를 구분하는 지점의 데이터가 몰려있는 밀도가 낮으면 낮을수록, 더 미세한 차이점도 구별한다고 생각할 수 있기 때문에, 전체적인 성능을 높일 수 있다고 말하고 있습니다. 그런 점에서, Pseudo Label이 아닌 다른 SSL들인, Semi-Supervised Embedding (Weston et al., 2008)이나, Manifold Tangent Classifier (Rifai et al., 2011b)도 같은 목적을 달성한 방식이라고 소개하고 있습니다. Pseudo Label도 마찬가지로 Low-Density Separation 효과를 가져오는 방법이라는 것이죠. 4.2. Entropy Regularization Entropy Regularization (Grandvalet, Yoshua Bengio, 2006)은 Baysian에서 말하는 Maximum posteriori estimation(or Maximum a posteriori, MAP) 관점에서 Unlabelled data의 이점을 얻는 수단입니다. 이 방식은 Unlabeled data가 가지는 class별 확률에 대한 Entropy를 최소화시킴으로써, 위에서 언급한 Class들 간의 Low-Density separation을 추구합니다. 아래에 나오는 MAP식들과 함께 더 자세히 설명하도록 하겠습니다. 위와 같은 MAP식에서 ∑m=1nlogP(ym∣xm;θ)\\sum\\limits_{m = 1}^{n}\\text{log}P(y^{m}|x^{m};\\theta)m=1∑n​logP(ym∣xm;θ)를 첫번째 항, −λH(y∣x′;θ)-\\lambda H(y|x^{'};\\theta)−λH(y∣x′;θ)를 두 번째 항이라고 지칭할 때, 첫 번째 항인 labeled data의 log-likelihood을 최대화시키면서, 두 번째 항인 unlabeled data의 entropy를 최소화시키기 때문에, 우리는 좀 더 좋은 성능을 얻을 수 있게 됩니다. 두번째 항에서 최소화 시킨다는 Entropy는 Class간의 그 경계치가 Overlap이 되는 정도를 뜻하는데요, Class Overlap이 작아질수록, data들의 밀집된 부분이 더 낮은 decision boundary를 가지게 됩니다. 이것이 위에서 설명한, class별 확률에 대한 Entropy를 최소화시킴으로써, 위에서 언급한 Class들 간의 Low-Density separation을 추구합니다. 의 의미입니다. 따라서, 위에서 설명 하였던 Pseudo-Label의 Loss function에서 나오는 α\\alphaα는 Entropy Regularization의 λ\\lambdaλ와 같은 역할을 하고 있다는 것을 관찰하실 수 있습니다. 결론적으로 이 논문에서 말하고 있는 바는, 우리가 취한 Loss function은 Entropy Regularzation과 동일하다! 그래서, Pseudo-Label을 Entropy Regularization으로 Training하는 것이 효과가 있다. 로 정리할 수 있습니다. 4.3. 논문 실험 결과 논문의 저자는 MNIST test data로 t-SNE (Van der Maaten et al., 2008) 2-D embedding results를 첨부하여 그 성능을 보여주고 있습니다. train data는 600개 밖에 쓰지 않았고, 60000개의 unlabeled data를 사용했다고 하네요. Pseudo-Label (이하 PL)을 쓰지 않은 DropNN 모델과, 거기에 PL을 쓴 +PL 모델의 Conditional Entropy를 비교해 보면, Train에서는 +PL이 확실히 그 Entropy가 더 높게 나타나지만, Unlabeled data나, Test set에서 나오는 Entropy는 월등히 낮음을 볼 수 있었습니다. 시각적으로도 그 구분하는 경계 즉, Decision boundary가 어떤 모델이 더 섬세하게 작용하고 있는지(=더 확실히 구별하고 있는지) 확인할 수 있습니다. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2018/08/22/pseudo-label/"},{"title":"MobileNet V1","text":"2017년 4월 17일. 대놓고 청량감을 주는 이름으로, 현재 많은 모바일, 자율주행 등의 Local 장비에서의 Object Detection으로 사랑받고 있는 Google의 MobileNet.📱 어느새 2019년 6월 12일을 기점으로 Version 3까지 나와버렸습니다. 😎 그 기념으로(?) MobileNet은 과연 어떠한 진화를 거듭해 왔는지 함께 살펴 보실까요? MobileNet Version 1부터 시작합니다! 논문 설명부터는 편의상 평어체를 사용하겠습니다.😶 2017.04.17 MobileNet은 논문에서, 자신들의 3×33 \\times 33×3 Depthwise Separable Convolutions 을 쓰면, Standard Convolution 계산량의 8배에서 9배 가량 작은 계산을 할 수 있게 된다고 한다. 이것을 가능하게 하는 MobileNet의 핵심 아이디어는 아래와 같다. ‘굳이 모든 Input Feature Map 들에 대해 모든 Filter 가 학습해야하나?’ 😯 1. Standard Convolution 전형적인 Conv layer의 계산량은 아래와 같다. DKD_KDK​: Dimension of the Kernel MMM: Input Channel Depth. NNN: Output channel Depth. Stride가 1이고, Padding이 있다는 전제하에, 이 Conv layer가 GGG를 생산하기 위한 계산량은, 아래와 같이 Input 필터 사이즈인 MMM만큼, 또 Output 필터 사이즈인 NNN만큼 계산하게 된다. Gk,l,n=∑i,j,mKi,j,m,n⋅Fk+i−1,l+j−1,mG_{k,l,n} = \\sum_{i, j, m}K_{i,j,m,n}\\cdot F_{k+i-1, l+j-1,m} Gk,l,n​=i,j,m∑​Ki,j,m,n​⋅Fk+i−1,l+j−1,m​ FFF: Input Feature map GGG: Output Feature map KKK: Kernel 그래서 총 Cost는, Kernel Size(DKD_KDK​)와 Input channel 수(MMM), Output channel 수(NNN), 그리고 Output feature map size(DFD_FDF​)에 의해 결정된다. DK⋅DK⋅M⋅N⋅DF⋅DFD_K \\cdot D_K \\cdot M \\cdot N \\cdot D_F \\cdot D_F DK​⋅DK​⋅M⋅N⋅DF​⋅DF​ 2. Depthwise Separable Convolutions 이번엔 MobileNet이 사용하는 Depthwise Separable Conv layer의 경우를 살펴보자. 이름에 Separable은, MobileNet이 쓰는 conv layer가 depthwise convolution 과 pointwise convolution 로 나누어져 쌍으로 동작하는 것을 말한다. 2.1. Depthwise Convolution 위에 보이는 바와 같이, '하나의 Feature map(=input channel)에 대해, 하나의 filter’를 사용한다. 기존의 MMM만큼 필터를 사용했던 Standard Conv의 필터들보다 훨씬 더 가볍다. 그렇기 때문에 Input channel의 사이즈인 M개만큼의 Kernel(=filter)가 생길 뿐이고, 이는 학습해야할 parameter를 현저하게 줄여주는 역할을 한다. 그래서 이 depthwise conv를 통과하고 생성되는 feature map은 아래와 같이 n이 없어진 계산량을 가지게 된다. K^\\hat{K}K^: Depthwise Convolution Kernel of size DK×DK×MD_K \\times D_K \\times MDK​×DK​×M G^\\hat{G}G^: mthm_{th}mth​ channel of the filtered output feature map 2.2. Pointwise Convolution 이렇게 depthwise conv layer를 통과한 Depth 1짜리 output들은, 결과적으로 새로운 feature가 되기위해서 합쳐져야 한다. 그 역할을 1×11 \\times 11×1 pointwise convolution 이 해준다. 원하는 NNN만큼의 Depth를 가지는 Feature map 을 만들기 위해, 이렇게 Depthwise Conv를 통과한 MMM개의 output들을 NNN개의 1×11 \\times 11×1 convolution들이 차원을 맞춰준다. 이 때 비로소 Depthwise separable convolution이라는 칭호(?)를 얻으며, Cost는 아래와 같이 된다. +++를 기준으로, 앞항은 Depthwise Conv, 뒷 항은 Pointwise Conv. DK⋅DK⋅M⋅DF⋅DF+M⋅N⋅DF⋅DFD_K \\cdot D_K \\cdot M \\cdot D_F \\cdot D_F + M\\cdot N \\cdot D_F \\cdot D_F DK​⋅DK​⋅M⋅DF​⋅DF​+M⋅N⋅DF​⋅DF​ 2.3. Depthwise Separable Convolutions 이 과정을 정리하면 아래와 같다. Standard Convolution에 비해 얼마나 계산량이 줄어드는지, 식으로 간단히 계산해보면 아래와 같다. DK⋅DK⋅M⋅DF⋅DF+M⋅N⋅DF⋅DFDK⋅DK⋅M⋅N⋅DF⋅DF\\frac{D_K \\cdot D_K \\cdot M \\cdot D_F \\cdot D_F + M\\cdot N \\cdot D_F \\cdot D_F}{D_K \\cdot D_K \\cdot M \\cdot N \\cdot D_F \\cdot D_F} DK​⋅DK​⋅M⋅N⋅DF​⋅DF​DK​⋅DK​⋅M⋅DF​⋅DF​+M⋅N⋅DF​⋅DF​​ =1N+1DK2=\\frac{1}{N} + \\frac{1}{D^2_K} =N1​+DK2​1​ 이는 Output으로 낼 feature map의 depth인 NNN과, 사용할 Kernel 사이즈에 따라 기존 Convolution에 비해 계산량이 얼마나 주는지 정해진다는 말이 된다. 모바일넷은 3×33 \\times 33×3 depthwise conv를 쓰기 때문에, 이는 약, 8배에서 9배 가량 Standard Convolutions 보다 적은 계산량으로, 비슷한 정확도를 내는 모델을 만들 수 있다는 말이 된다. 3. Network Structure and Training 좌측은 일반 Conv. 우측은 Depthwise Separable Conv. Depthwise Separable Conv는 위와같이 Depthwise Conv와 Pointwise Conv뒤에 BN-ReLU를 적용한다. 이렇게 Depthwise conv, Pointwise conv를 각각 하나의 Layer로 센다면, MobileNet은 아래와 같이 총 28 layers로 구성된 모델이 된다. 첫번째 layer는 Standard Conv. Channel이 3으로 계산량이 많은 편이 아니라서 그런듯. Down sampling은 depthwise conv의 첫 번째 layer에서 stride를 2를 주는 방식으로 진행. 마지막에 Average pooling을 사용 1×11 \\times 11×1으로 축소. Optimizer는 RMSprop with asynchronous gradient descent. Table 4에서는, ImageNet 데이터에 대해서, 같은 Mobilenet Structure에 Standard Conv를 쓴 모델과 Depthwise Separable Conv를 사용했을 때, 계산량과 학습 parameters는 현격히 줄어든 것에 비해 정확도는 1% 정도밖에 낮아지지 않았음을 보여주고 있다. MobileNet에는 두 가지 Hyper-parameter가 있다. Width Multiplier와, Resolution Multiplier다. 4. Width Multiplier: Thinner Models Width Multiplier라 불리는 α\\alphaα의 역할은 각각의 layer를 균일하게 얇게 만들어 줘서 전체 네트워크를 좀 더 가볍게 만들어 주는 것이다. 이는, Input Channels 수인 MMM과 Output Channels 수인 NNN에 width multiplier인 α\\alphaα를 단순히 곱해주는 것으로, α\\alphaα를 사용한 모델의 Depthwise Separable Conv는 아래와 같은 계산량을 가진다. DK⋅DK⋅αM⋅DF⋅DF+αM⋅αN⋅DF⋅DFD_K \\cdot D_K \\cdot \\alpha M \\cdot D_F \\cdot D_F + \\alpha M\\cdot \\alpha N \\cdot D_F \\cdot D_F DK​⋅DK​⋅αM⋅DF​⋅DF​+αM⋅αN⋅DF​⋅DF​ α∈(0,1]\\alpha \\in (0, 1]α∈(0,1]의 값으로서, 보통 1, 0.75, 0.5, 0.25로 세팅이 된다. α=1\\alpha = 1α=1이면 baseline MobileNet이 되는 것이고, α { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2019/08/11/mobilenet/"},{"title":"Basic Deep learning 01","text":"Deep Learning 개념 및 용어들을 알아봅니다. Optimizer, Loss function, Back propagation Peter Cha Intro Deep Learning을 사용해서 우리가 하고자 하는 일련의 과정은 결국, 우리가 만든 AI(model)가 특정 데이터를 얼마나 잘, 데이터를 구별(classification), 혹은 감지(detection)할 수 있게 할 것인가? 하는 것입니다. 물론, GAN과 같은 Unsupervised Learning에서는 말이 달라 질 수 있지만, 설명을 위해 편의상 그렇다고 생각해봅시다. AI를 학습시키고자 하는 데이터만 주면, model 스스로 ‘아, A는 이렇게 생겼구나, 이렇게 생기면 B라고 하구나’하고 그 데이터가 가지고 있는 특징(feature)을 스스로 깨우치기 원하는 거죠. 그렇게 잘 학습이 잘 되면, 한 번도 본 적은 없지만 여태껏 봐왔던 특징을 가지고 있는 새로운 이미지를 봤을 때, ‘아, 이건 A야.’혹은, ‘B야’하고 맞출 수 있게 되는 것이구요. 더 쉽게 이야기 해보죠. 우리는 강아지와 고양이가 어떻게 생겼는지 우리 model에게 알려주고, 처음보는 강아지나 고양이를 봐도 그 것이 강아지인지, 고양이인지 잘 구별할 수 있었으면 좋겠습니다. 아래 그림처럼 강아지와, 고양이 그림을 엄청나게 많이 주고 우리는 우리가 만든 Model에게 ‘강아지는 이렇게 생긴거야’, ‘고양이는 이렇게 생겼단다.’하고 알려줍니다. 우리는 이 과정을 학습, 혹은 training - learning이라고 부릅니다. 그렇게 잘 학습된 모델은, 훈련할 때 본 적은 없지만, 처음 본 강아지 사진(test)을 봐도 ‘얘는 강아지네요. 고양이는 아니에요’라고 말할 수 있게 됩니다. (Image from KDnuggets) The purpose of deep learning is all about the question, “How can we let our AI classify or detect the certain image or something?” Of course, there are exceptions like GAN, unspervised learning area, let’s simplify the concept for the explanation. What we want to do is to create the model knowing the characteristics or feature of a particular class of data, so it can answer which class the data in. Let’s talk more easily. We want to tell our model how puppies or cats look like and wish it can distinguish whether it is a puppy or a cat when it sees another puppy or another cat for the first time. As shown in the picture above, we give a lot of puppies and cats pictures to our model, and we teach the model, ‘puppy looks like this’, and ‘cat looks like this.’ This is called, learning or training. Then, as the process progresses, the model can distinguish the cat from the dog. That’s all what we are going to talk about. In this post, Deep Learning을 이해하고, 직접 Deep Learning을 구현하고자 했을 때 필요한 기본 개념들을 정리해 보았습니다. 학습이란 무엇을 의미하는지, Optimizer는 어떤 역할을 하는 것인지, Loss function은 무엇인지, 그리고 마지막으로 Back propagation은 어떻게 진행되는지, 간단한 예시를 통해 알아보도록 하겠습니다. 😃 In this post, I will talk about the basic concepts of deep learning. Let’s start to learn what Learning means, Optimizer does, Loss function is, and How the Back propagation works via a simple example. Learning 특정한 값을 예측을 하고 싶다고 하면, 우리는 먼저 실제로 그러한 예측을 할 수 있는 Model이 필요합니다. 그리고, 그 모델이 예측한 값(prediction)과 실제 값(grounth truth or answer)과의 값의 차이를 loss라고 말합니다. 예를 들어, 간단한 선형 모델인 y = wx을 우리가 모델로 가지고 있다고 합니다. y는 실제 정답이고 x는 input값 입니다. 이 때, w를 우리는 weight라고 부릅니다. 보통 맨 처음엔 이 w를 랜덤하게 고릅니다. w가 매우 적절하게 잘 정해져서 실제 정답인 y와 wx가 똑같은 값이 되었다면 losslossloss는 0이 되겠죠! 그래서 Input인 x를 주면 정답인 y를 잘 맞추려면, 당연히 우리는 이 w를 잘 맞출 필요가 있습니다. 근데 위에서 말한 것 처럼 w를 랜덤하게 시작해서는 곤란하죠. 한 번 만에 잘 맞춘다는 건 힘듭니다. 그래서 우리는 학습을 진행을 함에 따라, 우리는 무엇이 됐을지 모르는 이 w값을 반복적으로 update를 시켜서, losslossloss를 최소화 할 수 있게끔 만듭니다. 그래서 학습이라는 것은 loss를 최소화 시키는 w 찾기! 라고 할 수 있습니다. When we predict some values, firstly we need a model that can actually do predict, and we call the difference with a prediction and an actual value, ground truch, loss. For example, if we have a linear model y = wx, y is the answer, and x is input. Then, we call the w weight. If the weight is set very properly, then the loss will be 0! We often choose the inital value of w randomly. As the training proceeds, we repeatedly update this w so that we can find minimizes the loss. Therefore, Learning is finding w that minimizes the loss! Optimizer Weight update 자,우리는 losslossloss를 최소화시키는 www를 알고 싶습니다. 그럴 때, www가 값에따라, 그림과 같이 losslossloss와 www의 값으로 그래프를 그렸다고 했을 때, U자로 형성됐다고 생각하고, losslossloss를 최소화하는 www의 값으로 www를 update하고 싶습니다. 시작점은 편의상, 랜덤하게 정해졌다고 합시다. w값을 update시키기 위해서, 우리는 다음과 같은 공식을 씁니다. 여기서 α\\alphaα가 뜻하는 것은 learning rate라고 하는 것인데요, 보통 0.001같은 아주 작은 값이고, 그래서 다음 학습할 때 쓸 w는 지금 w와 얼마만큼 떨어져있는지정도를 의미합니다. w=w−α⋅∂loss∂ww = w - \\alpha \\cdot \\frac {\\partial loss}{\\partial w}w=w−α⋅∂w∂loss​ 이 w값은 미분을 하면 구할 수 있는데요, 미분의 의미는 결국 아주 작은 구간에서의 순간변화율이라고 우리가 알고 있는 만큼, 이는 미분은 곧, 기울기의 정도를 표현한다고 할 수 있죠. 이 www값을 구하기 위한 미분 방법은 아래에 나오는 Back Propagation을 소개하면서 다시 이야기 하도록 하겠습니다. 작은 예시로, Back Propagation이 ‘내가 지금 알고 있는 지식을 가지고 대학교 졸업 후의 나로 돌아 갈 수 있다면, 훨씬 더 좋은 선택과 결정을 하면서 살 수 있을 것이다.’ 같은 겁니다. 여기서, 근데 이 learning rate을 너무 크게 설정해줘서, 필요이상으로 군 입대 하루 전으로 돌아간다면 끔찍하겠죠? 그 과거로 ‘적절히’ 돌아가야 지금 가지고 있는 정보를 십분 활용할 수 있기 때문에, 이 learning rate라는 수치가 중요합니다. Let me think we want to know the value of weigh which minimizes the loss. If we draw a graph consists of loss and w, let us consider it looks like a bowl like the image above. Then, we want to update the weight to the point which becomes the value minimizing the loss. Of course, the starting point is randomly selected. To update the w value, we use follwing equation. alpha means learning rate which is usually very small number like 0.001, so it means that How far the next step w is from where now w is. w=w−α⋅∂loss∂ww = w - \\alpha \\cdot \\frac {\\partial loss}{\\partial w}w=w−α⋅∂w∂loss​ As we know, the meaning of derivative is Instantaneous rate of change, inclination. Let me introduce the way how to calculate the derivative of w, later on the Back propagation part. SGD 미분을 이용하여, 만약 미분 결과값이 -인 경우, w는 좀더 양수쪽으로 가게 되고, 반대로는 음수로 가는 방식으로 우리는 w를 update할 수 있습니다. 이런 update 방법을 Stochastic Gradient Descent 최적화 - 한국어로는 경사하강법 -, 또는 줄여서 SGD라고 부릅니다. SGD 이외에 최적화 기법으로 Adam, Adamx 등등 다양한 기법들이 있습니다만, 클래식한 이런 경사하강법의 방식의 다른 방식이라고 이해해도 크게 틀리지 않습니다. By using derivative, We can update the w in this way: if the drivative value(=gradient) is minus, then w will be move toward the posivie side, and visa versa. This kind of update approach is called Stochastic Gradient Descent Optimization, or SGD for short. There are other more various different Optimizers like Adam, Adamax and so on, but you might think that those are different to classical stochastic gradient descent. Loss function 자, 그러면 위에서 설명한 loss라는 것을 계산하기 위해서는 어떤 방법을 사용할까요? Loss function을 설명하기 위해서, 쉬운 loss function 하나를 예시로 들어봅시다. MSE는 모델의 loss를 산출하는 방법 중 하나입니다. MSE는 Mean Squared Error의 준 말로, 문자 그대로 아래에 보이는 수식 - 예측한 값에서 실제 값을 빼고 그 차이를 제곱하여 평균을 내는 방식 - 으로 모델의 loss를 계산합니다. Here, what should we do to get the loss mentioned before? To explain loss function, let’s take an easy loss function as an example. MSE is an one of ways to measure the loss of a model. The Acronym for the Mean Square Error which is following equation. y hat is a prediction of our model, and y is a real value. So, it means simply the sum of differences between forecasts and actual values. Binary class에 대한 loss를 구해줘야 할 때는, MSE 보다는 BCEloss를 더 잘 씁니다. 이렇게 더 다양한 loss function들이 있습니다. There are various other loss functions like BCEloss for binary loss, and so on. Back Propagation 역전파라고도 하는, Back Progagation은 loss를 weight로 미분한 값을 계산하는 방법입니다. 예를 들어, 우리의 모델이 선형회기식인, Linear model이라고 하고, 우리의 loss function이 MSE라고 합시다. 그러면 우리 모델이 loss를 구할 때 거쳐가게 될 공식은, loss=(y^−y)2loss = (\\hat y - y)^2loss=(y^​−y)2 이기 때문에, 즉 (x∗w−y)2(x*w - y)^2(x∗w−y)2이 될테고, 이 식은 아래의 그림과 같이 도식화 할 수 있습니다. Back propagation is the way to calculate the derivate value of loss by w. For example, our model is a linear model, and we use MSE as a loss function. Then, the gates of our model will look like following. x = 1, y = 2, 그리고 w = 1이라고 합시다. 그러면 loss를 구하는 forward path는 명백합니다. Let’s assume that x = 1, y = 2, and w = 1. Then, the forward path is obvious. Derivate Computation Chain Rule에 의해서, 우리는 차근 차근 w의 미분값을 계산해 나갈 수 있습니다. loss functionloss\\text{ } functionloss function 공식에서 사용하는 operator 하나를, 하나의 gate라고 생각할 때, 각 gate마다 input으로 들어오게 되는 그 값이 최종 losslossloss 가 산출되는데 얼마만큼이나 기여를 하나. 하는 정도가 곧 우리가 미분을 하는 이유입니다. 그래서 결국은 그렇게 w가 loss functionloss\\text{ } functionloss function에서 input으로 들어가게 될 때의 미분값을 구하면, 그 것은 즉, w가 loss를 구하는데 얼마나 영향을 미치는가(= 기울기)라는 의미가 됩니다. 😄 Back Propagation은 가장 우측의 gate와 함께 loss로부터 시작합니다. By using Chain Rule, we can calculate the derivative value of w, step by step. Let us consider the each operator in the loss function is a gate, then, we are going to calculate how much this input of each gate contributes to the loss. That’s the reason why we do the derivative calculation. So, at last, we can get the derivative of w as an input of a gate, it means the amount of contribution of w to the final loss value. 😄 The back propagation starts from the loss with rightmost local gate. x2x^2x2 gate $loss$인 1은 s인 -1을 제곱해서 나온 값이니까요, loss=s2loss = s^2loss=s2 으로 생각할 수 있습니다. loss를 제곱 gate로 미분한 다는 의미의 ∂loss∂s\\frac{\\partial loss} {\\partial s}∂s∂loss​라는 식은 곧, ∂s2∂s\\frac{\\partial s^2} {\\partial s}∂s∂s2​라는 식과 같다고 생각할 수 있습니다. ∂loss∂s\\frac{\\partial loss} {\\partial s}∂s∂loss​ = ∂s2∂s\\frac{\\partial s^2} {\\partial s}∂s∂s2​ s2s^2s2을 sss로 미분한거죠! 그러면 ∂s2∂s=2s\\frac{\\partial s^2} {\\partial s} = 2s∂s∂s2​=2s 이기 때문에, 우리가 알고 있는 s = -1를 대입하면, x2x^2x2gate의 local gradient는 -2가 됩니다. losslossloss is 1, and s is -1, so, the local derative of square gate is -2. ∂loss∂s\\frac{\\partial loss} {\\partial s}∂s∂loss​ = ∂s2∂s=2s\\frac{\\partial s^2} {\\partial s} = 2s∂s∂s2​=2s Again, s is -1. Therefore, the local gradient of - gate is -2. −-− gate x2x^2x2gate에서 -2가 - gate에 losslossloss로 들어왔습니다. 그리고 그 - gate의 계산결과는 s인, -1 이었구요. 자, 이제 Chain Rule을 사용해서 - gate의 local gradient를 구해볼까요? 우리는, Chain Rule에 의해서, - gate의 local gradient를 아래와 같은 식으로 표현할 수 있습니다. ∂loss∂y^=∂loss∂s∂s∂y^⇒−2⋅∂y^−∂y∂y^=−2⋅1=−2\\frac{\\partial loss} {\\partial \\hat y} = \\frac{\\partial loss}{\\partial s}\\frac {\\partial s}{ \\partial \\hat y} \\Rightarrow -2\\cdot\\frac {\\partial \\hat y - \\partial y}{\\partial \\hat y} = -2\\cdot1 = -2∂y^​∂loss​=∂s∂loss​∂y^​∂s​⇒−2⋅∂y^​∂y^​−∂y​=−2⋅1=−2 ∂loss∂s\\frac{\\partial loss}{\\partial s}∂s∂loss​은 -2라는 것을 x2x^2x2 gate에서 이미 알고 있기때문에, 나머지, s를, y^\\hat yy^​로 미분한 결과만 계산해서 곱하면 끝납니다. s는 y^−y\\hat y - yy^​−y 라는 식의 결과나 마찬가지었으니, 치환해서 생각하면 편하구요. 그래서 결과는 -gate에서도 여전히 local gradient는 -2가 되군요! -2 is passed to the - gate as loss. In the - gate, yyy is a constant value and y^\\hat yy^​ is 1, so the derivative is -2. We already know that ∂loss∂s\\frac{\\partial loss}{\\partial s}∂s∂loss​ = -2, so, the thing we need to do is to calculate the ∂y^−∂y∂y^\\frac {\\partial \\hat y - \\partial y}{\\partial \\hat y}∂y^​∂y^​−∂y​. ∂loss∂y^=∂loss∂s∂s∂y^⇒−2⋅∂y^−∂y∂y^=−2⋅1=−2\\frac{\\partial loss} {\\partial \\hat y} = \\frac{\\partial loss}{\\partial s}\\frac {\\partial s}{ \\partial \\hat y} \\Rightarrow -2\\cdot\\frac {\\partial \\hat y - \\partial y}{\\partial \\hat y} = -2\\cdot1 = -2∂y^​∂loss​=∂s∂loss​∂y^​∂s​⇒−2⋅∂y^​∂y^​−∂y​=−2⋅1=−2 ∗*∗ gate 자 이제 마지막으로, w가 input으로 들어간 * gate의 local gradient를 계산하고 w의 gradient를 계산하는 과정을 끝냅시다. -gate에서 -2가 losslossloss로 넘어왔고, 또 다시, 우리는 위와 같은 방법으로 Chain Rule을 쓰면, ∂loss∂w=∂loss∂y^∂y^∂w\\frac {\\partial loss}{\\partial w} = \\frac{\\partial loss}{\\partial \\hat y}\\frac{\\partial \\hat y}{\\partial w}∂w∂loss​=∂y^​∂loss​∂w∂y^​​ 로, 표현할 수 있고, 또 우린 이미 ∂loss∂y^\\frac{\\partial loss}{\\partial \\hat y}∂y^​∂loss​ = -2 라는 것을 알고 있습니다. 그래서 여기서 y^\\hat yy^​를 의미하는 식인 wxwxwx를 www로 미분한 값만 알면되는데, 그 값은 xxx이므로 그냥 x였던, 1를 넣어주면, losslossloss에 대한 w의 미분값이 -2라는 결과를 얻습니다. 이런 식으로 우리는 w의 미분값을 계산해 나가면서, losslossloss가 최소가 되는 w를 찾습니다. Let’s finish this process with calculating the local gradient of the * gate. As we know, -2 is given from the - gate, so, as the same way, using the Chain Rule again, we can write the equation like following. ∂loss∂w=∂loss∂y^∂y^∂w\\frac {\\partial loss}{\\partial w} = \\frac{\\partial loss}{\\partial \\hat y}\\frac{\\partial \\hat y}{\\partial w}∂w∂loss​=∂y^​∂loss​∂w∂y^​​ We already know the value of ∂loss∂y^\\frac{\\partial loss}{\\partial \\hat y}∂y^​∂loss​ is -2, so, we just put the value of derivative of wxwxwx with www, 1. Then, at last, we can know the derivative value of w is -2. In this way, we can find the weight point where the loss becomes minimum. Update weight 그렇게 계산이 끝난 W의 loss를 어떻게 update해줄까요? 아래 보시는 것처럼 단순히 w=w−learning rate∗w.lossw = w - learning{\\:} rate * w.lossw=w−learningrate∗w.loss 로 Update를 하게 됩니다. 아래 예시에서는 learning rate로 0.01을 줬네요. 이것으로 Back Propagation에 대한 설명을 마치도록 하겠습니다. Then, how to update calculated weight’s loss? Simply we can calculate simply like this. w=w−learning rate∗w.lossw = w - learning{\\:} rate * w.lossw=w−learningrate∗w.loss . In the example above, it used 0.01 as a learning rate. document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2018/02/06/Basic-Deep learning-01/"},{"title":"Docker tutorial (2)","text":"로컬 저장소 컨테이너로 마운트하기 내 컨테이너 환경 이미지로 Upload하기 안녕하세요 😃 Docker Tutorial, 두 번째✌️입니다! 컨테이너를 만들고 작업을 열심히 했는데, 그 컨테이너가 실수로 지워진다면, 그 컨테이너 안에 있던 모든 자료들이 삭제되었다는 것을 의미합니다 동공지진👀 그런 일을 미연에 방지하기 위해서, 저장소를 컨테이너 외부에 두는 것이 안전하죠! 그래서, 첫 번째로 생성한 Docker 컨테이너에 내 로컬 폴더 연동하기를 함께 알아보겠습니다. 또 내가 쓰던 개발환경을 떠나, 새로운 곳으로 갔을때이직 일일히 환경설정을 그 때 환경으로 다 할 필요 없이 우리는 도커를 쓸 수 있습니다. 두 번째로 내가 작업하고 있는 컨테이너의 설정을 Image로 Docker Hub or Registry에 업로드해서 다른 곳에서도 쓸 수 있도록 하는 방법을 알아보겠습니다! Local Directory Mount (run -v) 내가 가지고 있는 Host OS있는 폴더를 생성한 컨테이너와 공유하려면 run 명령어를 실행할 때 -v 옵션을 사용하면 됩니다! 포트 설정을 할 때와 같이 :을 기준으로 좌측에는 마운트하고자 하는 폴더경로를, 우측에는 마운트시킬 컨테이너 경로를 적어주면 됩니다.123$ docker run -it \\ -v /path/to/folder:/root/work \\ image_ID_or_name /bin/bash Example 제 컴퓨터 바탕화면에 Data라는 폴더에는 CNN 학습을 위한 이미지들이 담겨있는 train이라는 폴더가 있습니다. 이 폴더를 이전 포스팅에서 다운 받았던 이미지를 사용한 컨테이너와 연동시켜보도록 하겠습니다. 먼저 가지고 있는 images의 ID를 확인해보니, a8928a6d6eaa이네요. 12REPOSITORY TAG IMAGE ID CREATED SIZEcivisanalytics/civis-jupyter-python3 latest a8928a6d6eaa 8 days ago 3.37GB 해당 이미지로 컨테이너를 생성하면서 마운트를 해보면, 123$ docker run -it -p 8888:8888 \\ -v /Users/chayesol/Desktop/Data:/root/work \\ a8928a6d6eaa /bin/bash 실제로 1.jpg, 2.jpg가 들어있는 train 폴더가 /root/work/train로 잘 마운트 된 것을 확인할 수 있었습니다. 1234567[work] # ls -a. .. train[work] # cd train [train] # ls -a. .. 1.jpg 2.jpg[train] # pwd/root/work/train 이 폴더는 실제로 공유되고 있는 폴더이기 때문에, 이 폴더 안에 파일을 지우면 당연히 컨테이너 상에서도 지워집니다. (Vise Versa) Docker는 로컬 뿐만아니라 AWS S3 Bucket과 연동시킬 수도 있습니다. 시간나면…쿨럭…😷다시 포스팅하겠습니다… 내 컨테이너 환경 Upload하기! Layer? 컨테이너는 이미지 기반으로 생성되기 때문에, 내 컨테이너 환경을 업로드한다는 것은, 내 컨테이너 환경의 이미지를 만든다고 생각할 수 있습니다. Docker는 이미지를 만들 때, 최종 이미지를 만들기 위한 명령어들을 한땀한땀 실행합니다. 이태리장인 즉, 최종 이미지를 만들기 시작한 맨처음 기반이 되어주는 이미지로 컨테이너를 만든 다음, 그 컨테이너에서 그 다음 부품(명령어)을 가져다가 설치해서 다시 이미지를 만들고, 또 그 다음 부품, 그 다음, 그 다음… 식이라는 거죠. 이렇게 한 겹, 한 겹 싸이는 구조 때문에, 그 부품이 되는 명령어 하나하나를 layer라고 부릅니다. 업로드 할 이미지 만들기(Dockerfile) Docker는 이미지 파일을 Dockerfile 이라는 파일을 보고 생성합니다. Dockerfile을 작성하기 위해서는 Dockerfile 명령어들을 사용해서 내 환경에 맞게 잘 적어줘야합니다. 실습을 위해, 여태까지는 Tensorflow만 사용했지만, Pytorch도 사용하고 싶은 사용자가 있다고 가정합시다. 그렇다면 다음과 같은 시나리오로 이미지를 만들 수 있습니다. 1231. Jupyter notebook과 Tensorflow가 설치되어 있는 이미지를 Base Image로 사용2. Pytorch를 추가로 설치 (layer 추가)3. 컨테이너 생성시, Jupyter notebook 자동 실행하도록 세팅. 이 시나리오에 해당하는 Dockerfile을 먼저 보고, 하나씩 설명해보도록 하겠습니다. Dockerfile 우리가 만들 Image의 Dockerfile은 다음과 같습니다. 원하시는 경로에서 다음과 같은 Dockerfile을 만들어 주세요. (파일 이름을 \"Dockerfile\"로) 123456789101112131415161718# Base ImageFROM civisanalytics/civis-jupyter-python3LABEL maintainer=\"petercha90@gmail.com\"# Pytorch 설치RUN apt-get -y -qq update && \\ conda install -y pytorch-cpu torchvision-cpu -c pytorch# Data 폴더 복사COPY ./Data /root/work/Data/# 명령어를 실행할 디렉토리 설정WORKDIR /root/work# Jupyter notebook 가동EXPOSE 8888 22CMD jupyter notebook --NotebookApp.token='' \\ --ip=0.0.0.0 --port=8888 --allow-root FROM: 1FROM civisanalytics/civis-jupyter-python3:1.11.0 '이미지의 Base를 어디로부터 가져오겠느냐’는 말입니다. 저는 Jupyter notebook과 Tensorflow가 설치되어 있는 이미지를 Base Image로 사용하려 이전 포스팅에서 사용한 이미지를 Base로 삼고 있습니다. :앞에는 이미지 이름이, 뒤에는 버전정보라고 볼 수 있는 Tag 를 적어줍니다. LABEL: 1LABEL maintainer=\"petercha90@gmail.com\" 보통 MAINTANER라는 명령어를 쓰지만, 곧 'will be deprecated’라고 하여, 자체적으로 추천하는 LABEL을 사용해 봤습니다. LABEL의 key값으로 maintainer를 주고, value로 제 e-mail을 써서, 이 이미지의 관리자가 누구인지 밝히고 있습니다. - 추가 정보기 때문에 Build하는데 영향을 주지는 않습니다. RUN: 12RUN apt-get -y -qq update && \\ conda install -y pytorch-cpu torchvision-cpu -c pytorch 가장 많이, 자주 쓰는 명령어입니다. RUN 다음 적히는 명령어를 그대로 실행하게 해줍니다. 먼저 ubuntu 다운을 위해 apt-get update를 해줍니다. -y는 update 도중 생기는 yes/no를 묻는 질문에 막혀서 멈추지 않도록 미리 모두 yes를 주기 위함이고, -qq는 설치 내역 등의 log 출력하지 않도록하는 quiet 옵션입니다. 그 다음, Pytorch를 설치하는 명령어를 실행하도록 하고 있습니다. COPY: 1COPY ./Data /root/work/Data/ COPY 다음에 나오는 첫번째 경로에 있는 파일을 두번째로 적힌 컨테이너 상의 경로로 복사합니다. 저는 바탕화면에 있는 Data 폴더(현재 Dockerfile이 있는 경로가 바탕화면이라서 ./Data)를 /root/work/Data/로 복사하게 했습니다. 두 번째 경로에 해당하는 디렉토리가 없다면 자동 생성합니다. 실제로 /root/work/에는 Data라는 이름의 폴더가 없지만 이번 이미지 Build과정을 통해 생성하게 됩니다. WORKDIR: 1WORKDIR /root/work RUN, COPY, CMD 등의 명령어들을 실행할 경로를 지정해줍니다. 접속했을 때, 맨 처음 위치하게 되는 경로이기도 합니다. 그 다음 명령어 상에서 경로 이동이 생기더라도, 그 다음 명령어에서 자동으로 여기 적은 경로로 위치가 초기화됩니다. EXPOSE: 1EXPOSE 8888 22 컨테이너 실행시, 요청을 기다리는 port를 미리 열어줍니다. 여러개를 설정 할 수도 있습니다. 저는 그 다음 포스팅에서 나올 개념인 ssh접속을 위해서 22번 port도 열어주기로 했습니다. CMD: 12CMD jupyter notebook --NotebookApp.token='' \\ --ip=0.0.0.0 --port=8888 --allow-root 컨테이너가 실제로 실행되었을 때, CMD에 적힌 명령어들을 실행합니다. 위에서 말했듯이, 컨테이너 생성시 Jupyter notebook을 자동 실행하도록 세팅하기 위해 미리 CMD로 해당 명령어를 적어놓았습니다. 그 외에도 자주 쓰는 명령어로 ADD, VOLUME, ENV 등이 있습니다. 더 많은 정보는 공식문서를 참고해주세요. Build! 이미지를 생성하는 명령어 build와 이름을 설정해주는 -t(tag)옵션을 사용한 아래 명령어로 Dockerfile을 실행 시키면, 1$ docker build -t docker101 . 시간이 꽤 걸립니다…⏳ 123456...Removing intermediate container a39920cdab45 ---> 547c429ef2eeSuccessfully built 547c429ef2eeSuccessfully tagged docker101:latest$ 라는 메세지가 나오면 성공적으로 Image를 생성한 것입니다! docker images로 생성한 이미지를 확인해봅니다. 와우… 3.37GB Base-image에 Pytorch를 설치해서 4.71GB SIZE가 돼버린 뚠뚠이🐷 docker101 Image가 보이네요! 1234$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEdocker101 latest 547c429ef2ee 13 minutes ago 4.71GBcivisanalytics/civis-jupyter-python3 latest a8928a6d6eaa 10 days ago 3.37GB 그렇다면…! 떨리는 맘으로 Jupyter notebook을 자동 실행되도록 설정한 것까지 잘 되는지 확인해보겠습니다! 팝콘각🍟 1$ docker run --name first_one -d -p 8888:8888 docker101 역시 localhost:8888로 접속을하니 바로 Jupyter notebook으로 접속이 되네요! 의도한 CMD 세팅이 잘 되어 있다는 것을 확인할 수 있습니다. 😎 맨처음 COPY로 복사했던 Data 폴더가 먼저 환영해 주네요 😃 그 안에 학습용 데이터들도 잘 복사가 되어있는 것을 확인할 수 있습니다. 새 Python notebook을 만들어 Pytorch까지 잘 설치되어있는지 확인해 보면…맙소사… Pytorch와 Tensorflow, 둘 다 잘 작동하는군요! 성공적입니다!🎂🎉🎊🎅🎁 Upload 잘만든 이미지를 공유하는 방법으로 아래와 같은 두 가지 방법이 있습니다. Docker Registry 를 설치하여 업로드한다. (like Git) Docker Hub 에 업로드 한다. (like Github) Docker Registry Docker Registry는 git처럼 설치형으로, 만든 이미지를 Registry 서버로 push하고, 다른 서버에서 pull을 받아서 사용합니다. Docker Hub Docker Hub는 도커에서 제공하는 기본 이미지 저장소로, 회원가입만하면, 대용량의 이미지도 무료로 업로드하고 다운로드 받을 수 있습니다. 마치 Github처럼 웹상에서 다운받을 수도 있고, docker 명령어로 이미지를 업로드 하고, 다운 받을 수 있는 공간입니다. 이번 포스팅에서는 Docker Hub를 사용하여 업로는 하는 방법만 다뤄보도록 하겠습니다! Docker Hub Uploading Docker Hub에 만든 이미지를 올리는 것은 3가지 단계만 거치면 끝입니다. 1. Login 2. Renaming 3 Push. Login (login) 먼저 Docker Hub 홈페이지에서 회원가입을 하시고 아래 명령어를 입력하면 Docker Hub 계정에 로그인 할 수 있습니다. 12345$ docker loginLogin with your Docker ID to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com to create one.Username: petercha90Password:Login Succeeded Renaming (tag) Docker Hub에 올릴 이미지는, [User_ID]/[Image_name]:[tag] 라는 규칙으로 이름을 구성합니다. tag 명령어를 사용하여 금방 만들었던 docker101 이미지의 이름을 규칙에 맞게 바꿔주면서, 버전 정보(1.0)도 입력해보겠습니다. 1$ docker tag docker101 petercha90/peter-tensorflow-pytorch:1.0 docker images로 확인해보니, docker101과 용량도 같은데 이름이 petercha90/peter-tensorflow-pytorch이고, TAG가 1.0인 이미지를 확인할 수 있었습니다. 1234 $ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEdocker101 latest 34bbb1c093da 20 minutes ago 4.71GBpetercha90/peter-tensorflow-pytorch 1.0 34bbb1c093da 20 minutes ago 4.71GB Push (push) 이제 push명령어로 Docker Hub에 업로드만 하면 됩니다! 1$ docker push petercha90/peter-tensorflow-pytorch:1.0 또 시간이 좀 걸립니다… … ⏳ 1234567891011121314151617181920The push refers to repository [docker.io/petercha90/peter-tensorflow-pytorch]fdd4a938bd06: Pushed be825b2f1c27: Pushed a3442dbff3f9: Mounted from civisanalytics/civis-jupyter-python3 734b701f4670: Mounted from civisanalytics/civis-jupyter-python3 5d22569a8a20: Mounted from civisanalytics/civis-jupyter-python3 99645d10b61c: Mounted from civisanalytics/civis-jupyter-python3 43c65e275431: Mounted from civisanalytics/civis-jupyter-python3 5efecd288488: Mounted from civisanalytics/civis-jupyter-python3 ef945229e9cd: Mounted from civisanalytics/civis-jupyter-python3 dc2c8974046d: Mounted from civisanalytics/civis-jupyter-python3 2a83ea7d2735: Mounted from civisanalytics/civis-jupyter-python3 70191a7dc716: Mounted from civisanalytics/civis-jupyter-python3 c3a15ba40d5e: Mounted from civisanalytics/civis-jupyter-python3 603a1f4a3e0c: Mounted from civisanalytics/civis-jupyter-python3 b57c79f4a9f3: Mounted from civisanalytics/civis-jupyter-python3 d60e01b37e74: Mounted from civisanalytics/civis-jupyter-python3 e45cfbc98a50: Mounted from civisanalytics/civis-jupyter-python3 762d8e1a6054: Mounted from civisanalytics/civis-jupyter-python3 1.0: digest: sha256:ffc16d44d34f063a0856999b0f04fc71f322e4d1d47e026ffed6ad2ab89f6a81 size: 4094 🎉Ta-da!!🎉 Upload가 완료되었습니다! 이전 포스트에서 이미지를 Pull하면서 시작한 것 기억나시나요? 그 것처럼 이제 어디서든 인터넷만 잘 된다면, Anaconda3-Tensorflow-Pytorch이 설치되어 있고 + Jupyter notebook 자동실행이 되는! 보안은어쩔 이미지를 다운받아 사용할 수 있습니다! 😎 여기까지 이전 포스팅과 함께 Docker에 대해 아주 러프하게 함께 알아 보았습니다. 더 구체적이고 설정 방법들과 Docker를 이용한 다양한 서비스 개발에 대해서는 Reference를 참고하시면 되겠습니다! References 초보를 위한 도커 안내서 - 설치하고 컨테이너 실행하기 초보를 위한 도커 안내서 - 이미지 만들고 배포하기 document.querySelectorAll('.github-emoji') .forEach(el => { if (!el.dataset.src) { return; } const img = document.createElement('img'); img.style = 'display:none !important;'; img.src = el.dataset.src; img.addEventListener('error', () => { img.remove(); el.style.color = 'inherit'; el.style.backgroundImage = 'none'; el.style.background = 'none'; }); img.addEventListener('load', () => { img.remove(); }); document.body.appendChild(img); });","link":"/2019/05/09/docker-102/"}],"tags":[{"name":"blog","slug":"blog","link":"/tags/blog/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"tutorial","slug":"tutorial","link":"/tags/tutorial/"},{"name":"useful-info","slug":"useful-info","link":"/tags/useful-info/"},{"name":"machine-learning","slug":"machine-learning","link":"/tags/machine-learning/"},{"name":"ml","slug":"ml","link":"/tags/ml/"},{"name":"logarithm","slug":"logarithm","link":"/tags/logarithm/"},{"name":"log","slug":"log","link":"/tags/log/"},{"name":"machine_learning","slug":"machine-learning","link":"/tags/machine-learning/"},{"name":"pandas","slug":"pandas","link":"/tags/pandas/"},{"name":"dataframe","slug":"dataframe","link":"/tags/dataframe/"},{"name":"deep_learning","slug":"deep-learning","link":"/tags/deep-learning/"},{"name":"tensorflow","slug":"tensorflow","link":"/tags/tensorflow/"},{"name":"basic","slug":"basic","link":"/tags/basic/"},{"name":"CNN","slug":"CNN","link":"/tags/CNN/"},{"name":"Batch","slug":"Batch","link":"/tags/Batch/"},{"name":"Epoch","slug":"Epoch","link":"/tags/Epoch/"},{"name":"Filter","slug":"Filter","link":"/tags/Filter/"},{"name":"Kernel","slug":"Kernel","link":"/tags/Kernel/"},{"name":"Padding","slug":"Padding","link":"/tags/Padding/"},{"name":"Pooling","slug":"Pooling","link":"/tags/Pooling/"},{"name":"Colab","slug":"Colab","link":"/tags/Colab/"},{"name":"tensorpack","slug":"tensorpack","link":"/tags/tensorpack/"},{"name":"keras","slug":"keras","link":"/tags/keras/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"regularization","slug":"regularization","link":"/tags/regularization/"},{"name":"loss","slug":"loss","link":"/tags/loss/"},{"name":"L1","slug":"L1","link":"/tags/L1/"},{"name":"L2","slug":"L2","link":"/tags/L2/"},{"name":"Lasso","slug":"Lasso","link":"/tags/Lasso/"},{"name":"Ridge","slug":"Ridge","link":"/tags/Ridge/"},{"name":"convex_optimisation","slug":"convex-optimisation","link":"/tags/convex-optimisation/"},{"name":"ssh","slug":"ssh","link":"/tags/ssh/"},{"name":"container ssh","slug":"container-ssh","link":"/tags/container-ssh/"},{"name":"pseudo-label","slug":"pseudo-label","link":"/tags/pseudo-label/"},{"name":"mobilenet","slug":"mobilenet","link":"/tags/mobilenet/"},{"name":"Optimizer","slug":"Optimizer","link":"/tags/Optimizer/"},{"name":"Loss function","slug":"Loss-function","link":"/tags/Loss-function/"},{"name":"Back propagation","slug":"Back-propagation","link":"/tags/Back-propagation/"}],"categories":[{"name":"Tutorial","slug":"Tutorial","link":"/categories/Tutorial/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/categories/Machine-Learning/"},{"name":"Deep Learning paper","slug":"Deep-Learning-paper","link":"/categories/Deep-Learning-paper/"}]}